{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AllNO2_QH.csv',\n",
       " 'AllPM_QH.csv',\n",
       " 'Env_QH.csv',\n",
       " 'GradientTemp_15minDataSet.csv',\n",
       " 'micro_sud3.pkl',\n",
       " 'micro_sud3_normalized.pkl',\n",
       " 'Patm_15minDataSet.csv',\n",
       " 'pickles']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>PM_ref</th>\n",
       "      <th>PM_6182</th>\n",
       "      <th>PM_6179</th>\n",
       "      <th>PM_617B</th>\n",
       "      <th>PM25_6182</th>\n",
       "      <th>PM25_6179</th>\n",
       "      <th>PM25_617B</th>\n",
       "      <th>NO2_ref</th>\n",
       "      <th>NO2_61FD</th>\n",
       "      <th>NO2_61F0</th>\n",
       "      <th>NO2_61EF</th>\n",
       "      <th>temp</th>\n",
       "      <th>rh</th>\n",
       "      <th>tgrad</th>\n",
       "      <th>pressure</th>\n",
       "      <th>pluvio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>2017-09-28 14:00:00</td>\n",
       "      <td>16.2</td>\n",
       "      <td>-1.178505</td>\n",
       "      <td>-1.137844</td>\n",
       "      <td>-1.134624</td>\n",
       "      <td>-1.183081</td>\n",
       "      <td>-1.128074</td>\n",
       "      <td>-1.148204</td>\n",
       "      <td>10.1</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.986031</td>\n",
       "      <td>-1.114144</td>\n",
       "      <td>-0.922393</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>2017-09-28 14:15:00</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-1.108262</td>\n",
       "      <td>-1.085060</td>\n",
       "      <td>-1.121956</td>\n",
       "      <td>-1.101652</td>\n",
       "      <td>-1.071229</td>\n",
       "      <td>-1.128278</td>\n",
       "      <td>9.9</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>2.057032</td>\n",
       "      <td>-1.123212</td>\n",
       "      <td>-0.977185</td>\n",
       "      <td>0.335134</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>2017-09-28 14:30:00</td>\n",
       "      <td>10.3</td>\n",
       "      <td>-1.178505</td>\n",
       "      <td>-1.169515</td>\n",
       "      <td>-1.257077</td>\n",
       "      <td>-1.176817</td>\n",
       "      <td>-1.167865</td>\n",
       "      <td>-1.252817</td>\n",
       "      <td>16.1</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>2.080699</td>\n",
       "      <td>-1.232038</td>\n",
       "      <td>-1.086769</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2017-09-28 14:45:00</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-1.137530</td>\n",
       "      <td>-1.000606</td>\n",
       "      <td>-1.206407</td>\n",
       "      <td>-1.139235</td>\n",
       "      <td>-1.008700</td>\n",
       "      <td>-1.222928</td>\n",
       "      <td>10.9</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>2.009698</td>\n",
       "      <td>-1.259245</td>\n",
       "      <td>-0.812809</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2017-09-28 15:00:00</td>\n",
       "      <td>10.7</td>\n",
       "      <td>-1.166798</td>\n",
       "      <td>-1.164236</td>\n",
       "      <td>-1.138846</td>\n",
       "      <td>-1.164290</td>\n",
       "      <td>-1.167865</td>\n",
       "      <td>-1.148204</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.867697</td>\n",
       "      <td>-1.141350</td>\n",
       "      <td>-0.922393</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>2017-09-28 15:15:00</td>\n",
       "      <td>10.7</td>\n",
       "      <td>-1.166798</td>\n",
       "      <td>-1.201185</td>\n",
       "      <td>-1.037506</td>\n",
       "      <td>-1.158026</td>\n",
       "      <td>-1.196288</td>\n",
       "      <td>-1.038610</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.749363</td>\n",
       "      <td>-1.050662</td>\n",
       "      <td>-0.867601</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>2017-09-28 15:30:00</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-1.078994</td>\n",
       "      <td>-1.169515</td>\n",
       "      <td>-1.007948</td>\n",
       "      <td>-1.070333</td>\n",
       "      <td>-1.162181</td>\n",
       "      <td>-0.998757</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.654695</td>\n",
       "      <td>-0.959974</td>\n",
       "      <td>-0.812809</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>2017-09-28 15:45:00</td>\n",
       "      <td>10.2</td>\n",
       "      <td>-1.032164</td>\n",
       "      <td>-1.132566</td>\n",
       "      <td>-0.995280</td>\n",
       "      <td>-1.032750</td>\n",
       "      <td>-1.122389</td>\n",
       "      <td>-0.988794</td>\n",
       "      <td>19.1</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.583695</td>\n",
       "      <td>-0.887423</td>\n",
       "      <td>-0.758017</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23</td>\n",
       "      <td>2017-09-28 16:00:00</td>\n",
       "      <td>9.8</td>\n",
       "      <td>-1.043872</td>\n",
       "      <td>-1.095617</td>\n",
       "      <td>-0.910830</td>\n",
       "      <td>-1.039014</td>\n",
       "      <td>-1.093967</td>\n",
       "      <td>-0.919052</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.536361</td>\n",
       "      <td>-0.851148</td>\n",
       "      <td>-0.758017</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>2017-09-28 16:15:00</td>\n",
       "      <td>8.9</td>\n",
       "      <td>-1.043872</td>\n",
       "      <td>-1.042833</td>\n",
       "      <td>-1.181072</td>\n",
       "      <td>-1.039014</td>\n",
       "      <td>-1.037122</td>\n",
       "      <td>-1.173112</td>\n",
       "      <td>19.4</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.489027</td>\n",
       "      <td>-0.851148</td>\n",
       "      <td>-0.703225</td>\n",
       "      <td>0.335134</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>2017-09-28 16:30:00</td>\n",
       "      <td>9.8</td>\n",
       "      <td>-0.915092</td>\n",
       "      <td>-0.889760</td>\n",
       "      <td>-1.185294</td>\n",
       "      <td>-0.913739</td>\n",
       "      <td>-0.889325</td>\n",
       "      <td>-1.178094</td>\n",
       "      <td>20.6</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.441693</td>\n",
       "      <td>-0.805804</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>2017-09-28 16:45:00</td>\n",
       "      <td>10.1</td>\n",
       "      <td>-0.920945</td>\n",
       "      <td>-0.784192</td>\n",
       "      <td>-1.029061</td>\n",
       "      <td>-0.926266</td>\n",
       "      <td>-0.787005</td>\n",
       "      <td>-1.013702</td>\n",
       "      <td>17.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.576624</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.370693</td>\n",
       "      <td>-0.715116</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>0.315942</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27</td>\n",
       "      <td>2017-09-28 17:00:00</td>\n",
       "      <td>10.7</td>\n",
       "      <td>-0.862409</td>\n",
       "      <td>-0.842254</td>\n",
       "      <td>-0.969945</td>\n",
       "      <td>-0.869892</td>\n",
       "      <td>-0.843850</td>\n",
       "      <td>-0.963886</td>\n",
       "      <td>16.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.299692</td>\n",
       "      <td>-0.642565</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>0.335134</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28</td>\n",
       "      <td>2017-09-28 17:15:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.932653</td>\n",
       "      <td>-0.778913</td>\n",
       "      <td>-0.915052</td>\n",
       "      <td>-0.932530</td>\n",
       "      <td>-0.775636</td>\n",
       "      <td>-0.914070</td>\n",
       "      <td>16.4</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.228692</td>\n",
       "      <td>-0.506533</td>\n",
       "      <td>-0.593641</td>\n",
       "      <td>0.335134</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>2017-09-28 17:30:00</td>\n",
       "      <td>12.3</td>\n",
       "      <td>-0.903385</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.817934</td>\n",
       "      <td>-0.901211</td>\n",
       "      <td>-0.900694</td>\n",
       "      <td>-0.814439</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.134024</td>\n",
       "      <td>-0.397707</td>\n",
       "      <td>-0.538849</td>\n",
       "      <td>0.354326</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>2017-09-28 17:45:00</td>\n",
       "      <td>13.7</td>\n",
       "      <td>-0.967775</td>\n",
       "      <td>-0.805305</td>\n",
       "      <td>-0.843269</td>\n",
       "      <td>-0.963849</td>\n",
       "      <td>-0.804058</td>\n",
       "      <td>-0.844329</td>\n",
       "      <td>28.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.525787</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.063024</td>\n",
       "      <td>-0.288882</td>\n",
       "      <td>-0.429265</td>\n",
       "      <td>0.354326</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31</td>\n",
       "      <td>2017-09-28 18:00:00</td>\n",
       "      <td>14.3</td>\n",
       "      <td>-0.698507</td>\n",
       "      <td>-0.615283</td>\n",
       "      <td>-0.910830</td>\n",
       "      <td>-0.688243</td>\n",
       "      <td>-0.616470</td>\n",
       "      <td>-0.904107</td>\n",
       "      <td>27.1</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.992023</td>\n",
       "      <td>-0.189125</td>\n",
       "      <td>-0.319681</td>\n",
       "      <td>0.373518</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>2017-09-28 18:15:00</td>\n",
       "      <td>14.7</td>\n",
       "      <td>-0.885824</td>\n",
       "      <td>-0.662789</td>\n",
       "      <td>-0.847492</td>\n",
       "      <td>-0.876156</td>\n",
       "      <td>-0.667631</td>\n",
       "      <td>-0.839347</td>\n",
       "      <td>21.9</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.921022</td>\n",
       "      <td>-0.044024</td>\n",
       "      <td>-0.264889</td>\n",
       "      <td>0.392710</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38</td>\n",
       "      <td>2017-09-28 19:45:00</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.751190</td>\n",
       "      <td>-0.625840</td>\n",
       "      <td>-0.796821</td>\n",
       "      <td>-0.744617</td>\n",
       "      <td>-0.616470</td>\n",
       "      <td>-0.794513</td>\n",
       "      <td>32.4</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.363744</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.613353</td>\n",
       "      <td>0.382210</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.431095</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39</td>\n",
       "      <td>2017-09-28 20:00:00</td>\n",
       "      <td>20.1</td>\n",
       "      <td>-0.827287</td>\n",
       "      <td>-0.757800</td>\n",
       "      <td>-0.813711</td>\n",
       "      <td>-0.826046</td>\n",
       "      <td>-0.741529</td>\n",
       "      <td>-0.819421</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.427290</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.566019</td>\n",
       "      <td>0.427554</td>\n",
       "      <td>-0.155305</td>\n",
       "      <td>0.431095</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40</td>\n",
       "      <td>2017-09-28 20:15:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.721922</td>\n",
       "      <td>-0.794749</td>\n",
       "      <td>-0.906607</td>\n",
       "      <td>-0.725825</td>\n",
       "      <td>-0.787005</td>\n",
       "      <td>-0.889163</td>\n",
       "      <td>27.5</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.576624</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.518686</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.431095</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>41</td>\n",
       "      <td>2017-09-28 20:30:00</td>\n",
       "      <td>19.2</td>\n",
       "      <td>-0.598996</td>\n",
       "      <td>-0.757800</td>\n",
       "      <td>-0.915052</td>\n",
       "      <td>-0.600550</td>\n",
       "      <td>-0.752898</td>\n",
       "      <td>-0.914070</td>\n",
       "      <td>25.6</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.495019</td>\n",
       "      <td>0.554518</td>\n",
       "      <td>0.063863</td>\n",
       "      <td>0.431095</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>42</td>\n",
       "      <td>2017-09-28 20:45:00</td>\n",
       "      <td>17.9</td>\n",
       "      <td>-0.803873</td>\n",
       "      <td>-0.889760</td>\n",
       "      <td>-0.898162</td>\n",
       "      <td>-0.807254</td>\n",
       "      <td>-0.895010</td>\n",
       "      <td>-0.899126</td>\n",
       "      <td>27.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.495019</td>\n",
       "      <td>0.581724</td>\n",
       "      <td>0.173447</td>\n",
       "      <td>0.431095</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43</td>\n",
       "      <td>2017-09-28 21:00:00</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.757044</td>\n",
       "      <td>-0.842254</td>\n",
       "      <td>-0.733483</td>\n",
       "      <td>-0.750880</td>\n",
       "      <td>-0.838165</td>\n",
       "      <td>-0.729752</td>\n",
       "      <td>26.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.471352</td>\n",
       "      <td>0.581724</td>\n",
       "      <td>0.118655</td>\n",
       "      <td>0.431095</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44</td>\n",
       "      <td>2017-09-28 21:15:00</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.798019</td>\n",
       "      <td>-0.858089</td>\n",
       "      <td>-0.695481</td>\n",
       "      <td>-0.788463</td>\n",
       "      <td>-0.860903</td>\n",
       "      <td>-0.684918</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.471352</td>\n",
       "      <td>0.636137</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>0.411902</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45</td>\n",
       "      <td>2017-09-28 21:30:00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.973628</td>\n",
       "      <td>-0.873925</td>\n",
       "      <td>-0.670145</td>\n",
       "      <td>-0.963849</td>\n",
       "      <td>-0.872272</td>\n",
       "      <td>-0.655029</td>\n",
       "      <td>25.7</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.447685</td>\n",
       "      <td>0.672412</td>\n",
       "      <td>0.173447</td>\n",
       "      <td>0.392710</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46</td>\n",
       "      <td>2017-09-28 21:45:00</td>\n",
       "      <td>18.5</td>\n",
       "      <td>-0.838995</td>\n",
       "      <td>-0.810584</td>\n",
       "      <td>-0.657478</td>\n",
       "      <td>-0.832310</td>\n",
       "      <td>-0.809743</td>\n",
       "      <td>-0.660010</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.447685</td>\n",
       "      <td>0.672412</td>\n",
       "      <td>0.118655</td>\n",
       "      <td>0.392710</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>47</td>\n",
       "      <td>2017-09-28 22:00:00</td>\n",
       "      <td>18.6</td>\n",
       "      <td>-0.838995</td>\n",
       "      <td>-0.831697</td>\n",
       "      <td>-0.678591</td>\n",
       "      <td>-0.838573</td>\n",
       "      <td>-0.821112</td>\n",
       "      <td>-0.674955</td>\n",
       "      <td>21.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.471352</td>\n",
       "      <td>0.672412</td>\n",
       "      <td>0.228239</td>\n",
       "      <td>0.392710</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>48</td>\n",
       "      <td>2017-09-28 22:15:00</td>\n",
       "      <td>18.8</td>\n",
       "      <td>-0.622410</td>\n",
       "      <td>-0.678624</td>\n",
       "      <td>-0.535024</td>\n",
       "      <td>-0.619341</td>\n",
       "      <td>-0.684684</td>\n",
       "      <td>-0.540453</td>\n",
       "      <td>20.8</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.471352</td>\n",
       "      <td>0.672412</td>\n",
       "      <td>0.611783</td>\n",
       "      <td>0.373518</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>49</td>\n",
       "      <td>2017-09-28 22:30:00</td>\n",
       "      <td>18.5</td>\n",
       "      <td>-0.657532</td>\n",
       "      <td>-0.652232</td>\n",
       "      <td>-0.678591</td>\n",
       "      <td>-0.663188</td>\n",
       "      <td>-0.656262</td>\n",
       "      <td>-0.674955</td>\n",
       "      <td>22.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>0.447685</td>\n",
       "      <td>0.717756</td>\n",
       "      <td>0.830951</td>\n",
       "      <td>0.373518</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>94</td>\n",
       "      <td>2017-09-29 09:45:00</td>\n",
       "      <td>26.6</td>\n",
       "      <td>-0.236071</td>\n",
       "      <td>-0.018825</td>\n",
       "      <td>-0.146552</td>\n",
       "      <td>-0.243514</td>\n",
       "      <td>-0.025284</td>\n",
       "      <td>-0.156872</td>\n",
       "      <td>40.8</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.106382</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.252359</td>\n",
       "      <td>-0.370501</td>\n",
       "      <td>-1.031977</td>\n",
       "      <td>-0.183054</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>95</td>\n",
       "      <td>2017-09-29 10:00:00</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-0.236071</td>\n",
       "      <td>-0.134949</td>\n",
       "      <td>-0.311230</td>\n",
       "      <td>-0.243514</td>\n",
       "      <td>-0.150342</td>\n",
       "      <td>-0.311301</td>\n",
       "      <td>29.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.459064</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.276025</td>\n",
       "      <td>-0.352363</td>\n",
       "      <td>-1.251145</td>\n",
       "      <td>-0.202246</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>96</td>\n",
       "      <td>2017-09-29 10:15:00</td>\n",
       "      <td>29.9</td>\n",
       "      <td>-0.165827</td>\n",
       "      <td>0.033959</td>\n",
       "      <td>-0.370346</td>\n",
       "      <td>-0.168349</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>-0.381043</td>\n",
       "      <td>32.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.331971</td>\n",
       "      <td>-0.266085</td>\n",
       "      <td>1.323359</td>\n",
       "      <td>-0.352363</td>\n",
       "      <td>-1.031977</td>\n",
       "      <td>-0.221438</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>97</td>\n",
       "      <td>2017-09-29 10:30:00</td>\n",
       "      <td>30.1</td>\n",
       "      <td>-0.066315</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>-0.218335</td>\n",
       "      <td>-0.086920</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>-0.206688</td>\n",
       "      <td>27.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.363744</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.394360</td>\n",
       "      <td>-0.307019</td>\n",
       "      <td>-1.141561</td>\n",
       "      <td>-0.240630</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>98</td>\n",
       "      <td>2017-09-29 10:45:00</td>\n",
       "      <td>29.7</td>\n",
       "      <td>-0.253631</td>\n",
       "      <td>-0.024103</td>\n",
       "      <td>0.056130</td>\n",
       "      <td>-0.262306</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>20.5</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.427290</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.465360</td>\n",
       "      <td>-0.343295</td>\n",
       "      <td>-1.086769</td>\n",
       "      <td>-0.240630</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>99</td>\n",
       "      <td>2017-09-29 11:00:00</td>\n",
       "      <td>27.5</td>\n",
       "      <td>-0.095583</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>-0.176109</td>\n",
       "      <td>-0.099448</td>\n",
       "      <td>0.133882</td>\n",
       "      <td>-0.161853</td>\n",
       "      <td>12.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.536361</td>\n",
       "      <td>-0.433983</td>\n",
       "      <td>-0.977185</td>\n",
       "      <td>-0.221438</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>100</td>\n",
       "      <td>2017-09-29 11:15:00</td>\n",
       "      <td>26.4</td>\n",
       "      <td>-0.095583</td>\n",
       "      <td>0.213425</td>\n",
       "      <td>-0.150774</td>\n",
       "      <td>-0.086920</td>\n",
       "      <td>0.207780</td>\n",
       "      <td>-0.161853</td>\n",
       "      <td>16.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.489027</td>\n",
       "      <td>-0.397707</td>\n",
       "      <td>-1.086769</td>\n",
       "      <td>-0.221438</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>101</td>\n",
       "      <td>2017-09-29 11:30:00</td>\n",
       "      <td>26.3</td>\n",
       "      <td>-0.037047</td>\n",
       "      <td>0.313714</td>\n",
       "      <td>-0.015653</td>\n",
       "      <td>-0.024282</td>\n",
       "      <td>0.298732</td>\n",
       "      <td>-0.017388</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.192169</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.465360</td>\n",
       "      <td>-0.370501</td>\n",
       "      <td>-0.977185</td>\n",
       "      <td>-0.202246</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>102</td>\n",
       "      <td>2017-09-29 11:45:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.312168</td>\n",
       "      <td>0.055073</td>\n",
       "      <td>-0.129661</td>\n",
       "      <td>-0.299888</td>\n",
       "      <td>0.054299</td>\n",
       "      <td>-0.131964</td>\n",
       "      <td>17.8</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.465360</td>\n",
       "      <td>-0.352363</td>\n",
       "      <td>-1.031977</td>\n",
       "      <td>-0.221438</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>103</td>\n",
       "      <td>2017-09-29 12:00:00</td>\n",
       "      <td>28.8</td>\n",
       "      <td>-0.277046</td>\n",
       "      <td>-0.177176</td>\n",
       "      <td>-0.345011</td>\n",
       "      <td>-0.268570</td>\n",
       "      <td>-0.167396</td>\n",
       "      <td>-0.341190</td>\n",
       "      <td>30.7</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.459064</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.441693</td>\n",
       "      <td>-0.370501</td>\n",
       "      <td>-1.031977</td>\n",
       "      <td>-0.202246</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>106</td>\n",
       "      <td>2017-09-29 12:45:00</td>\n",
       "      <td>24.9</td>\n",
       "      <td>-0.312168</td>\n",
       "      <td>-0.076887</td>\n",
       "      <td>-0.450574</td>\n",
       "      <td>-0.299888</td>\n",
       "      <td>-0.087813</td>\n",
       "      <td>-0.440821</td>\n",
       "      <td>21.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.441693</td>\n",
       "      <td>-0.361432</td>\n",
       "      <td>-1.031977</td>\n",
       "      <td>-0.279014</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>107</td>\n",
       "      <td>2017-09-29 13:00:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-0.470216</td>\n",
       "      <td>-0.156063</td>\n",
       "      <td>-0.302785</td>\n",
       "      <td>-0.462747</td>\n",
       "      <td>-0.161711</td>\n",
       "      <td>-0.301337</td>\n",
       "      <td>14.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.441693</td>\n",
       "      <td>-0.397707</td>\n",
       "      <td>-0.922393</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>108</td>\n",
       "      <td>2017-09-29 13:15:00</td>\n",
       "      <td>22.5</td>\n",
       "      <td>-0.645825</td>\n",
       "      <td>-0.467488</td>\n",
       "      <td>-0.391458</td>\n",
       "      <td>-0.644396</td>\n",
       "      <td>-0.468674</td>\n",
       "      <td>-0.381043</td>\n",
       "      <td>16.1</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.560028</td>\n",
       "      <td>-0.433983</td>\n",
       "      <td>-1.141561</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>109</td>\n",
       "      <td>2017-09-29 13:30:00</td>\n",
       "      <td>20.9</td>\n",
       "      <td>-0.874116</td>\n",
       "      <td>-0.810584</td>\n",
       "      <td>-0.712371</td>\n",
       "      <td>-0.863628</td>\n",
       "      <td>-0.792689</td>\n",
       "      <td>-0.689900</td>\n",
       "      <td>8.9</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.867697</td>\n",
       "      <td>-0.706047</td>\n",
       "      <td>-1.086769</td>\n",
       "      <td>-0.317398</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>110</td>\n",
       "      <td>2017-09-29 13:45:00</td>\n",
       "      <td>18.9</td>\n",
       "      <td>-0.897531</td>\n",
       "      <td>-0.794749</td>\n",
       "      <td>-0.953055</td>\n",
       "      <td>-0.888684</td>\n",
       "      <td>-0.792689</td>\n",
       "      <td>-0.948941</td>\n",
       "      <td>11.6</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.915031</td>\n",
       "      <td>-0.987180</td>\n",
       "      <td>-0.977185</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>0.647429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>111</td>\n",
       "      <td>2017-09-29 14:00:00</td>\n",
       "      <td>17.4</td>\n",
       "      <td>-0.604849</td>\n",
       "      <td>-0.768357</td>\n",
       "      <td>-0.893940</td>\n",
       "      <td>-0.606814</td>\n",
       "      <td>-0.764267</td>\n",
       "      <td>-0.889163</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.796696</td>\n",
       "      <td>-0.932767</td>\n",
       "      <td>-0.922393</td>\n",
       "      <td>-0.317398</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>112</td>\n",
       "      <td>2017-09-29 14:15:00</td>\n",
       "      <td>16.4</td>\n",
       "      <td>-0.680947</td>\n",
       "      <td>-0.678624</td>\n",
       "      <td>-0.991058</td>\n",
       "      <td>-0.688243</td>\n",
       "      <td>-0.696053</td>\n",
       "      <td>-0.963886</td>\n",
       "      <td>17.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.557560</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.725696</td>\n",
       "      <td>-0.896492</td>\n",
       "      <td>-0.922393</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>113</td>\n",
       "      <td>2017-09-29 14:30:00</td>\n",
       "      <td>18.4</td>\n",
       "      <td>-0.879970</td>\n",
       "      <td>-0.546664</td>\n",
       "      <td>-0.995280</td>\n",
       "      <td>-0.876156</td>\n",
       "      <td>-0.553941</td>\n",
       "      <td>-0.983812</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.702029</td>\n",
       "      <td>-0.950905</td>\n",
       "      <td>-0.867601</td>\n",
       "      <td>-0.317398</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>114</td>\n",
       "      <td>2017-09-29 14:45:00</td>\n",
       "      <td>19.3</td>\n",
       "      <td>-0.774605</td>\n",
       "      <td>-0.573056</td>\n",
       "      <td>-0.868604</td>\n",
       "      <td>-0.763408</td>\n",
       "      <td>-0.582363</td>\n",
       "      <td>-0.864255</td>\n",
       "      <td>21.3</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.654695</td>\n",
       "      <td>-0.932767</td>\n",
       "      <td>-0.703225</td>\n",
       "      <td>-0.317398</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>115</td>\n",
       "      <td>2017-09-29 15:00:00</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-0.704361</td>\n",
       "      <td>-0.536107</td>\n",
       "      <td>-0.716593</td>\n",
       "      <td>-0.694506</td>\n",
       "      <td>-0.553941</td>\n",
       "      <td>-0.729752</td>\n",
       "      <td>18.6</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.621107</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.607362</td>\n",
       "      <td>-0.914630</td>\n",
       "      <td>-0.703225</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>116</td>\n",
       "      <td>2017-09-29 15:15:00</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-0.581435</td>\n",
       "      <td>-0.778913</td>\n",
       "      <td>-0.771486</td>\n",
       "      <td>-0.575495</td>\n",
       "      <td>-0.781320</td>\n",
       "      <td>-0.774587</td>\n",
       "      <td>25.7</td>\n",
       "      <td>-0.392423</td>\n",
       "      <td>-0.589334</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.536361</td>\n",
       "      <td>-0.896492</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>117</td>\n",
       "      <td>2017-09-29 15:30:00</td>\n",
       "      <td>18.9</td>\n",
       "      <td>-0.405826</td>\n",
       "      <td>-0.710294</td>\n",
       "      <td>-0.868604</td>\n",
       "      <td>-0.400109</td>\n",
       "      <td>-0.701737</td>\n",
       "      <td>-0.864255</td>\n",
       "      <td>45.4</td>\n",
       "      <td>-0.331906</td>\n",
       "      <td>0.344797</td>\n",
       "      <td>-0.209981</td>\n",
       "      <td>1.489027</td>\n",
       "      <td>-0.914630</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>-0.298206</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>118</td>\n",
       "      <td>2017-09-29 15:45:00</td>\n",
       "      <td>20.1</td>\n",
       "      <td>-0.575581</td>\n",
       "      <td>-0.414704</td>\n",
       "      <td>-0.817934</td>\n",
       "      <td>-0.575495</td>\n",
       "      <td>-0.411829</td>\n",
       "      <td>-0.814439</td>\n",
       "      <td>49.9</td>\n",
       "      <td>-0.350061</td>\n",
       "      <td>-0.071431</td>\n",
       "      <td>-0.419097</td>\n",
       "      <td>1.394360</td>\n",
       "      <td>-0.842079</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>-0.279014</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>119</td>\n",
       "      <td>2017-09-29 16:00:00</td>\n",
       "      <td>22.6</td>\n",
       "      <td>-0.692654</td>\n",
       "      <td>-0.483323</td>\n",
       "      <td>-0.649033</td>\n",
       "      <td>-0.688243</td>\n",
       "      <td>-0.497096</td>\n",
       "      <td>-0.635103</td>\n",
       "      <td>58.8</td>\n",
       "      <td>-0.271390</td>\n",
       "      <td>0.668883</td>\n",
       "      <td>-0.317089</td>\n",
       "      <td>1.299692</td>\n",
       "      <td>-0.751391</td>\n",
       "      <td>-0.648433</td>\n",
       "      <td>-0.279014</td>\n",
       "      <td>0.647429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>120</td>\n",
       "      <td>2017-09-29 16:15:00</td>\n",
       "      <td>23.9</td>\n",
       "      <td>-0.446801</td>\n",
       "      <td>-0.610005</td>\n",
       "      <td>-0.767264</td>\n",
       "      <td>-0.462747</td>\n",
       "      <td>-0.627839</td>\n",
       "      <td>-0.749679</td>\n",
       "      <td>50.4</td>\n",
       "      <td>0.097758</td>\n",
       "      <td>-0.007885</td>\n",
       "      <td>0.203151</td>\n",
       "      <td>1.252359</td>\n",
       "      <td>-0.742322</td>\n",
       "      <td>-0.703225</td>\n",
       "      <td>-0.240630</td>\n",
       "      <td>0.647429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>121</td>\n",
       "      <td>2017-09-29 16:30:00</td>\n",
       "      <td>25.7</td>\n",
       "      <td>-0.429240</td>\n",
       "      <td>-0.076887</td>\n",
       "      <td>-0.450574</td>\n",
       "      <td>-0.431428</td>\n",
       "      <td>-0.087813</td>\n",
       "      <td>-0.440821</td>\n",
       "      <td>77.2</td>\n",
       "      <td>0.466906</td>\n",
       "      <td>0.668883</td>\n",
       "      <td>0.565278</td>\n",
       "      <td>1.157691</td>\n",
       "      <td>-0.461189</td>\n",
       "      <td>-0.812809</td>\n",
       "      <td>-0.240630</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>122</td>\n",
       "      <td>2017-09-29 16:45:00</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.173684</td>\n",
       "      <td>0.355941</td>\n",
       "      <td>0.187028</td>\n",
       "      <td>0.176159</td>\n",
       "      <td>0.327154</td>\n",
       "      <td>0.186856</td>\n",
       "      <td>86.4</td>\n",
       "      <td>1.017602</td>\n",
       "      <td>1.269396</td>\n",
       "      <td>1.600657</td>\n",
       "      <td>1.063024</td>\n",
       "      <td>-0.325157</td>\n",
       "      <td>-0.812809</td>\n",
       "      <td>-0.183054</td>\n",
       "      <td>-0.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>123</td>\n",
       "      <td>2017-09-29 17:00:00</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.144416</td>\n",
       "      <td>0.398169</td>\n",
       "      <td>0.524831</td>\n",
       "      <td>0.144840</td>\n",
       "      <td>0.389684</td>\n",
       "      <td>0.505677</td>\n",
       "      <td>88.7</td>\n",
       "      <td>1.084170</td>\n",
       "      <td>0.506840</td>\n",
       "      <td>2.013788</td>\n",
       "      <td>0.944689</td>\n",
       "      <td>-0.216331</td>\n",
       "      <td>-0.758017</td>\n",
       "      <td>-0.144669</td>\n",
       "      <td>0.647429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>124</td>\n",
       "      <td>2017-09-29 17:15:00</td>\n",
       "      <td>31.2</td>\n",
       "      <td>-0.259485</td>\n",
       "      <td>0.334828</td>\n",
       "      <td>0.305259</td>\n",
       "      <td>-0.262306</td>\n",
       "      <td>0.349892</td>\n",
       "      <td>0.296451</td>\n",
       "      <td>87.3</td>\n",
       "      <td>1.326234</td>\n",
       "      <td>1.765057</td>\n",
       "      <td>1.804672</td>\n",
       "      <td>0.850022</td>\n",
       "      <td>-0.053093</td>\n",
       "      <td>-0.593641</td>\n",
       "      <td>-0.106285</td>\n",
       "      <td>3.041287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>125</td>\n",
       "      <td>2017-09-29 17:30:00</td>\n",
       "      <td>30.7</td>\n",
       "      <td>-0.054608</td>\n",
       "      <td>0.387612</td>\n",
       "      <td>0.305259</td>\n",
       "      <td>-0.049337</td>\n",
       "      <td>0.401053</td>\n",
       "      <td>0.281506</td>\n",
       "      <td>124.1</td>\n",
       "      <td>3.044891</td>\n",
       "      <td>2.861231</td>\n",
       "      <td>3.344989</td>\n",
       "      <td>0.708021</td>\n",
       "      <td>0.110146</td>\n",
       "      <td>-0.538849</td>\n",
       "      <td>-0.106285</td>\n",
       "      <td>7.031051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                 date  PM_ref   PM_6182   PM_6179   PM_617B  \\\n",
       "0      15  2017-09-28 14:00:00    16.2 -1.178505 -1.137844 -1.134624   \n",
       "1      16  2017-09-28 14:15:00     9.6 -1.108262 -1.085060 -1.121956   \n",
       "2      17  2017-09-28 14:30:00    10.3 -1.178505 -1.169515 -1.257077   \n",
       "3      18  2017-09-28 14:45:00     9.4 -1.137530 -1.000606 -1.206407   \n",
       "4      19  2017-09-28 15:00:00    10.7 -1.166798 -1.164236 -1.138846   \n",
       "5      20  2017-09-28 15:15:00    10.7 -1.166798 -1.201185 -1.037506   \n",
       "6      21  2017-09-28 15:30:00     9.6 -1.078994 -1.169515 -1.007948   \n",
       "7      22  2017-09-28 15:45:00    10.2 -1.032164 -1.132566 -0.995280   \n",
       "8      23  2017-09-28 16:00:00     9.8 -1.043872 -1.095617 -0.910830   \n",
       "9      24  2017-09-28 16:15:00     8.9 -1.043872 -1.042833 -1.181072   \n",
       "10     25  2017-09-28 16:30:00     9.8 -0.915092 -0.889760 -1.185294   \n",
       "11     26  2017-09-28 16:45:00    10.1 -0.920945 -0.784192 -1.029061   \n",
       "12     27  2017-09-28 17:00:00    10.7 -0.862409 -0.842254 -0.969945   \n",
       "13     28  2017-09-28 17:15:00    12.0 -0.932653 -0.778913 -0.915052   \n",
       "14     29  2017-09-28 17:30:00    12.3 -0.903385 -0.905595 -0.817934   \n",
       "15     30  2017-09-28 17:45:00    13.7 -0.967775 -0.805305 -0.843269   \n",
       "16     31  2017-09-28 18:00:00    14.3 -0.698507 -0.615283 -0.910830   \n",
       "17     32  2017-09-28 18:15:00    14.7 -0.885824 -0.662789 -0.847492   \n",
       "18     38  2017-09-28 19:45:00     9.4 -0.751190 -0.625840 -0.796821   \n",
       "19     39  2017-09-28 20:00:00    20.1 -0.827287 -0.757800 -0.813711   \n",
       "20     40  2017-09-28 20:15:00    20.0 -0.721922 -0.794749 -0.906607   \n",
       "21     41  2017-09-28 20:30:00    19.2 -0.598996 -0.757800 -0.915052   \n",
       "22     42  2017-09-28 20:45:00    17.9 -0.803873 -0.889760 -0.898162   \n",
       "23     43  2017-09-28 21:00:00    16.0 -0.757044 -0.842254 -0.733483   \n",
       "24     44  2017-09-28 21:15:00    16.0 -0.798019 -0.858089 -0.695481   \n",
       "25     45  2017-09-28 21:30:00    17.0 -0.973628 -0.873925 -0.670145   \n",
       "26     46  2017-09-28 21:45:00    18.5 -0.838995 -0.810584 -0.657478   \n",
       "27     47  2017-09-28 22:00:00    18.6 -0.838995 -0.831697 -0.678591   \n",
       "28     48  2017-09-28 22:15:00    18.8 -0.622410 -0.678624 -0.535024   \n",
       "29     49  2017-09-28 22:30:00    18.5 -0.657532 -0.652232 -0.678591   \n",
       "..    ...                  ...     ...       ...       ...       ...   \n",
       "70     94  2017-09-29 09:45:00    26.6 -0.236071 -0.018825 -0.146552   \n",
       "71     95  2017-09-29 10:00:00    29.4 -0.236071 -0.134949 -0.311230   \n",
       "72     96  2017-09-29 10:15:00    29.9 -0.165827  0.033959 -0.370346   \n",
       "73     97  2017-09-29 10:30:00    30.1 -0.066315  0.023403 -0.218335   \n",
       "74     98  2017-09-29 10:45:00    29.7 -0.253631 -0.024103  0.056130   \n",
       "75     99  2017-09-29 11:00:00    27.5 -0.095583  0.134249 -0.176109   \n",
       "76    100  2017-09-29 11:15:00    26.4 -0.095583  0.213425 -0.150774   \n",
       "77    101  2017-09-29 11:30:00    26.3 -0.037047  0.313714 -0.015653   \n",
       "78    102  2017-09-29 11:45:00    28.0 -0.312168  0.055073 -0.129661   \n",
       "79    103  2017-09-29 12:00:00    28.8 -0.277046 -0.177176 -0.345011   \n",
       "80    106  2017-09-29 12:45:00    24.9 -0.312168 -0.076887 -0.450574   \n",
       "81    107  2017-09-29 13:00:00    23.0 -0.470216 -0.156063 -0.302785   \n",
       "82    108  2017-09-29 13:15:00    22.5 -0.645825 -0.467488 -0.391458   \n",
       "83    109  2017-09-29 13:30:00    20.9 -0.874116 -0.810584 -0.712371   \n",
       "84    110  2017-09-29 13:45:00    18.9 -0.897531 -0.794749 -0.953055   \n",
       "85    111  2017-09-29 14:00:00    17.4 -0.604849 -0.768357 -0.893940   \n",
       "86    112  2017-09-29 14:15:00    16.4 -0.680947 -0.678624 -0.991058   \n",
       "87    113  2017-09-29 14:30:00    18.4 -0.879970 -0.546664 -0.995280   \n",
       "88    114  2017-09-29 14:45:00    19.3 -0.774605 -0.573056 -0.868604   \n",
       "89    115  2017-09-29 15:00:00    19.9 -0.704361 -0.536107 -0.716593   \n",
       "90    116  2017-09-29 15:15:00    19.9 -0.581435 -0.778913 -0.771486   \n",
       "91    117  2017-09-29 15:30:00    18.9 -0.405826 -0.710294 -0.868604   \n",
       "92    118  2017-09-29 15:45:00    20.1 -0.575581 -0.414704 -0.817934   \n",
       "93    119  2017-09-29 16:00:00    22.6 -0.692654 -0.483323 -0.649033   \n",
       "94    120  2017-09-29 16:15:00    23.9 -0.446801 -0.610005 -0.767264   \n",
       "95    121  2017-09-29 16:30:00    25.7 -0.429240 -0.076887 -0.450574   \n",
       "96    122  2017-09-29 16:45:00    26.4  0.173684  0.355941  0.187028   \n",
       "97    123  2017-09-29 17:00:00    29.3  0.144416  0.398169  0.524831   \n",
       "98    124  2017-09-29 17:15:00    31.2 -0.259485  0.334828  0.305259   \n",
       "99    125  2017-09-29 17:30:00    30.7 -0.054608  0.387612  0.305259   \n",
       "\n",
       "    PM25_6182  PM25_6179  PM25_617B  NO2_ref  NO2_61FD  NO2_61F0  NO2_61EF  \\\n",
       "0   -1.183081  -1.128074  -1.148204     10.1 -0.392423 -0.621107 -0.419097   \n",
       "1   -1.101652  -1.071229  -1.128278      9.9 -0.392423 -0.621107 -0.419097   \n",
       "2   -1.176817  -1.167865  -1.252817     16.1 -0.392423 -0.621107 -0.419097   \n",
       "3   -1.139235  -1.008700  -1.222928     10.9 -0.392423 -0.621107 -0.419097   \n",
       "4   -1.164290  -1.167865  -1.148204     16.0 -0.392423 -0.621107 -0.419097   \n",
       "5   -1.158026  -1.196288  -1.038610      9.7 -0.392423 -0.621107 -0.419097   \n",
       "6   -1.070333  -1.162181  -0.998757     10.0 -0.392423 -0.621107 -0.419097   \n",
       "7   -1.032750  -1.122389  -0.988794     19.1 -0.392423 -0.621107 -0.419097   \n",
       "8   -1.039014  -1.093967  -0.919052     15.0 -0.392423 -0.621107 -0.419097   \n",
       "9   -1.039014  -1.037122  -1.173112     19.4 -0.392423 -0.621107 -0.419097   \n",
       "10  -0.913739  -0.889325  -1.178094     20.6 -0.392423 -0.621107 -0.419097   \n",
       "11  -0.926266  -0.787005  -1.013702     17.3 -0.392423 -0.576624 -0.419097   \n",
       "12  -0.869892  -0.843850  -0.963886     16.3 -0.392423 -0.621107 -0.419097   \n",
       "13  -0.932530  -0.775636  -0.914070     16.4 -0.392423 -0.621107 -0.419097   \n",
       "14  -0.901211  -0.900694  -0.814439     22.0 -0.392423 -0.621107 -0.419097   \n",
       "15  -0.963849  -0.804058  -0.844329     28.2 -0.392423 -0.525787 -0.419097   \n",
       "16  -0.688243  -0.616470  -0.904107     27.1 -0.392423 -0.589334 -0.419097   \n",
       "17  -0.876156  -0.667631  -0.839347     21.9 -0.392423 -0.589334 -0.419097   \n",
       "18  -0.744617  -0.616470  -0.794513     32.4 -0.392423 -0.363744 -0.419097   \n",
       "19  -0.826046  -0.741529  -0.819421     37.0 -0.392423 -0.427290 -0.419097   \n",
       "20  -0.725825  -0.787005  -0.889163     27.5 -0.392423 -0.576624 -0.419097   \n",
       "21  -0.600550  -0.752898  -0.914070     25.6 -0.392423 -0.589334 -0.419097   \n",
       "22  -0.807254  -0.895010  -0.899126     27.3 -0.392423 -0.621107 -0.419097   \n",
       "23  -0.750880  -0.838165  -0.729752     26.3 -0.392423 -0.589334 -0.419097   \n",
       "24  -0.788463  -0.860903  -0.684918     27.0 -0.392423 -0.621107 -0.419097   \n",
       "25  -0.963849  -0.872272  -0.655029     25.7 -0.392423 -0.621107 -0.419097   \n",
       "26  -0.832310  -0.809743  -0.660010     23.2 -0.392423 -0.621107 -0.419097   \n",
       "27  -0.838573  -0.821112  -0.674955     21.2 -0.392423 -0.621107 -0.419097   \n",
       "28  -0.619341  -0.684684  -0.540453     20.8 -0.392423 -0.621107 -0.419097   \n",
       "29  -0.663188  -0.656262  -0.674955     22.3 -0.392423 -0.621107 -0.419097   \n",
       "..        ...        ...        ...      ...       ...       ...       ...   \n",
       "70  -0.243514  -0.025284  -0.156872     40.8 -0.392423 -0.106382 -0.419097   \n",
       "71  -0.243514  -0.150342  -0.311301     29.2 -0.392423 -0.459064 -0.419097   \n",
       "72  -0.168349   0.014508  -0.381043     32.3 -0.392423 -0.331971 -0.266085   \n",
       "73  -0.086920   0.020192  -0.206688     27.2 -0.392423 -0.363744 -0.419097   \n",
       "74  -0.262306  -0.002546   0.062317     20.5 -0.392423 -0.427290 -0.419097   \n",
       "75  -0.099448   0.133882  -0.161853     12.3 -0.392423 -0.621107 -0.419097   \n",
       "76  -0.086920   0.207780  -0.161853     16.2 -0.392423 -0.621107 -0.419097   \n",
       "77  -0.024282   0.298732  -0.017388     32.0 -0.392423 -0.192169 -0.419097   \n",
       "78  -0.299888   0.054299  -0.131964     17.8 -0.392423 -0.621107 -0.419097   \n",
       "79  -0.268570  -0.167396  -0.341190     30.7 -0.392423 -0.459064 -0.419097   \n",
       "80  -0.299888  -0.087813  -0.440821     21.3 -0.392423 -0.589334 -0.419097   \n",
       "81  -0.462747  -0.161711  -0.301337     14.3 -0.392423 -0.589334 -0.419097   \n",
       "82  -0.644396  -0.468674  -0.381043     16.1 -0.392423 -0.621107 -0.419097   \n",
       "83  -0.863628  -0.792689  -0.689900      8.9 -0.392423 -0.621107 -0.419097   \n",
       "84  -0.888684  -0.792689  -0.948941     11.6 -0.392423 -0.621107 -0.419097   \n",
       "85  -0.606814  -0.764267  -0.889163     13.5 -0.392423 -0.621107 -0.419097   \n",
       "86  -0.688243  -0.696053  -0.963886     17.2 -0.392423 -0.557560 -0.419097   \n",
       "87  -0.876156  -0.553941  -0.983812     12.2 -0.392423 -0.621107 -0.419097   \n",
       "88  -0.763408  -0.582363  -0.864255     21.3 -0.392423 -0.621107 -0.419097   \n",
       "89  -0.694506  -0.553941  -0.729752     18.6 -0.392423 -0.621107 -0.419097   \n",
       "90  -0.575495  -0.781320  -0.774587     25.7 -0.392423 -0.589334 -0.419097   \n",
       "91  -0.400109  -0.701737  -0.864255     45.4 -0.331906  0.344797 -0.209981   \n",
       "92  -0.575495  -0.411829  -0.814439     49.9 -0.350061 -0.071431 -0.419097   \n",
       "93  -0.688243  -0.497096  -0.635103     58.8 -0.271390  0.668883 -0.317089   \n",
       "94  -0.462747  -0.627839  -0.749679     50.4  0.097758 -0.007885  0.203151   \n",
       "95  -0.431428  -0.087813  -0.440821     77.2  0.466906  0.668883  0.565278   \n",
       "96   0.176159   0.327154   0.186856     86.4  1.017602  1.269396  1.600657   \n",
       "97   0.144840   0.389684   0.505677     88.7  1.084170  0.506840  2.013788   \n",
       "98  -0.262306   0.349892   0.296451     87.3  1.326234  1.765057  1.804672   \n",
       "99  -0.049337   0.401053   0.281506    124.1  3.044891  2.861231  3.344989   \n",
       "\n",
       "        temp        rh     tgrad  pressure    pluvio  \n",
       "0   1.986031 -1.114144 -0.922393  0.315942 -0.150524  \n",
       "1   2.057032 -1.123212 -0.977185  0.335134 -0.150524  \n",
       "2   2.080699 -1.232038 -1.086769  0.315942 -0.150524  \n",
       "3   2.009698 -1.259245 -0.812809  0.315942 -0.150524  \n",
       "4   1.867697 -1.141350 -0.922393  0.315942 -0.150524  \n",
       "5   1.749363 -1.050662 -0.867601  0.315942 -0.150524  \n",
       "6   1.654695 -0.959974 -0.812809  0.315942 -0.150524  \n",
       "7   1.583695 -0.887423 -0.758017  0.315942 -0.150524  \n",
       "8   1.536361 -0.851148 -0.758017  0.315942 -0.150524  \n",
       "9   1.489027 -0.851148 -0.703225  0.335134 -0.150524  \n",
       "10  1.441693 -0.805804 -0.648433  0.315942 -0.150524  \n",
       "11  1.370693 -0.715116 -0.648433  0.315942 -0.150524  \n",
       "12  1.299692 -0.642565 -0.648433  0.335134 -0.150524  \n",
       "13  1.228692 -0.506533 -0.593641  0.335134 -0.150524  \n",
       "14  1.134024 -0.397707 -0.538849  0.354326 -0.150524  \n",
       "15  1.063024 -0.288882 -0.429265  0.354326 -0.150524  \n",
       "16  0.992023 -0.189125 -0.319681  0.373518 -0.150524  \n",
       "17  0.921022 -0.044024 -0.264889  0.392710 -0.150524  \n",
       "18  0.613353  0.382210  0.009071  0.431095 -0.150524  \n",
       "19  0.566019  0.427554 -0.155305  0.431095 -0.150524  \n",
       "20  0.518686  0.509174  0.009071  0.431095 -0.150524  \n",
       "21  0.495019  0.554518  0.063863  0.431095 -0.150524  \n",
       "22  0.495019  0.581724  0.173447  0.431095 -0.150524  \n",
       "23  0.471352  0.581724  0.118655  0.431095 -0.150524  \n",
       "24  0.471352  0.636137  0.009071  0.411902 -0.150524  \n",
       "25  0.447685  0.672412  0.173447  0.392710 -0.150524  \n",
       "26  0.447685  0.672412  0.118655  0.392710 -0.150524  \n",
       "27  0.471352  0.672412  0.228239  0.392710 -0.150524  \n",
       "28  0.471352  0.672412  0.611783  0.373518 -0.150524  \n",
       "29  0.447685  0.717756  0.830951  0.373518 -0.150524  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "70  1.252359 -0.370501 -1.031977 -0.183054 -0.150524  \n",
       "71  1.276025 -0.352363 -1.251145 -0.202246 -0.150524  \n",
       "72  1.323359 -0.352363 -1.031977 -0.221438 -0.150524  \n",
       "73  1.394360 -0.307019 -1.141561 -0.240630 -0.150524  \n",
       "74  1.465360 -0.343295 -1.086769 -0.240630 -0.150524  \n",
       "75  1.536361 -0.433983 -0.977185 -0.221438 -0.150524  \n",
       "76  1.489027 -0.397707 -1.086769 -0.221438 -0.150524  \n",
       "77  1.465360 -0.370501 -0.977185 -0.202246 -0.150524  \n",
       "78  1.465360 -0.352363 -1.031977 -0.221438 -0.150524  \n",
       "79  1.441693 -0.370501 -1.031977 -0.202246 -0.150524  \n",
       "80  1.441693 -0.361432 -1.031977 -0.279014 -0.150524  \n",
       "81  1.441693 -0.397707 -0.922393 -0.298206 -0.150524  \n",
       "82  1.560028 -0.433983 -1.141561 -0.298206 -0.150524  \n",
       "83  1.867697 -0.706047 -1.086769 -0.317398 -0.150524  \n",
       "84  1.915031 -0.987180 -0.977185 -0.298206  0.647429  \n",
       "85  1.796696 -0.932767 -0.922393 -0.317398 -0.150524  \n",
       "86  1.725696 -0.896492 -0.922393 -0.298206 -0.150524  \n",
       "87  1.702029 -0.950905 -0.867601 -0.317398 -0.150524  \n",
       "88  1.654695 -0.932767 -0.703225 -0.317398 -0.150524  \n",
       "89  1.607362 -0.914630 -0.703225 -0.298206 -0.150524  \n",
       "90  1.536361 -0.896492 -0.648433 -0.298206 -0.150524  \n",
       "91  1.489027 -0.914630 -0.648433 -0.298206 -0.150524  \n",
       "92  1.394360 -0.842079 -0.648433 -0.279014 -0.150524  \n",
       "93  1.299692 -0.751391 -0.648433 -0.279014  0.647429  \n",
       "94  1.252359 -0.742322 -0.703225 -0.240630  0.647429  \n",
       "95  1.157691 -0.461189 -0.812809 -0.240630 -0.150524  \n",
       "96  1.063024 -0.325157 -0.812809 -0.183054 -0.150524  \n",
       "97  0.944689 -0.216331 -0.758017 -0.144669  0.647429  \n",
       "98  0.850022 -0.053093 -0.593641 -0.106285  3.041287  \n",
       "99  0.708021  0.110146 -0.538849 -0.106285  7.031051  \n",
       "\n",
       "[100 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data/micro_sud3_normalized.pkl')\n",
    "df = df.reset_index()\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['date', 'PM_ref', 'PM_6182', 'PM_6179','PM_617B', 'PM25_6182', 'PM25_617B', 'PM25_6179',\\\n",
    "         'temp', 'rh', 'tgrad', 'pressure', 'pluvio']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premier modèle: simple DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def baseline_model(dense_size, input_dim, loss='mean_squared_error', optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_size, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "def split_dataframe(dataframe, percent):\n",
    "    nb_rows = int(np.floor(percent * len(dataframe)))\n",
    "    return dataframe[:nb_rows], dataframe[nb_rows:]\n",
    "\n",
    "def dataframe_to_xy(df):\n",
    "    return (np.array(df[['PM_6182', 'PM_6179', 'PM_617B', 'PM25_6182', 'PM25_6179',\\\n",
    "                         'PM25_617B', 'temp', 'rh',\\\n",
    "                         'tgrad', 'pressure', 'pluvio']]),\\\n",
    "            np.array(df['PM_ref']))\n",
    "\n",
    "df_train, df_test = split_dataframe(df, 0.5) \n",
    "df_valid, df_test = split_dataframe(df_test, 0.5)\n",
    "\n",
    "X_train, y_train = dataframe_to_xy(df_train)\n",
    "X_valid, y_valid = dataframe_to_xy(df_valid)\n",
    "X_test, y_test = dataframe_to_xy(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngyAINsL/ByFewNQk9OwFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d25e00780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d25dcc3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d23cdcd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_train, '+r')\n",
    "plt.show()\n",
    "plt.plot(y_valid, '+r')\n",
    "plt.show()\n",
    "plt.plot(y_test, '+r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                384       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 417\n",
      "Trainable params: 417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1126 samples, validate on 563 samples\n",
      "Epoch 1/5000\n",
      "1126/1126 [==============================] - 0s 212us/step - loss: 564.0047 - val_loss: 469.7932\n",
      "Epoch 2/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 491.8054 - val_loss: 409.0221\n",
      "Epoch 3/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 428.7359 - val_loss: 358.6133\n",
      "Epoch 4/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 376.8241 - val_loss: 317.6727\n",
      "Epoch 5/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 334.8560 - val_loss: 284.8173\n",
      "Epoch 6/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 301.2061 - val_loss: 258.7212\n",
      "Epoch 7/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 274.3550 - val_loss: 238.6518\n",
      "Epoch 8/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 253.5230 - val_loss: 222.6765\n",
      "Epoch 9/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 236.7312 - val_loss: 209.3538\n",
      "Epoch 10/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 222.6338 - val_loss: 198.7658\n",
      "Epoch 11/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 211.2956 - val_loss: 189.7958\n",
      "Epoch 12/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 201.6296 - val_loss: 182.3466\n",
      "Epoch 13/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 193.5104 - val_loss: 176.1800\n",
      "Epoch 14/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 186.7263 - val_loss: 171.1787\n",
      "Epoch 15/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 181.1088 - val_loss: 166.4345\n",
      "Epoch 16/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 175.8243 - val_loss: 162.3977\n",
      "Epoch 17/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 171.2504 - val_loss: 158.8467\n",
      "Epoch 18/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 167.1539 - val_loss: 155.5175\n",
      "Epoch 19/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 163.3231 - val_loss: 152.5542\n",
      "Epoch 20/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 159.9074 - val_loss: 149.7077\n",
      "Epoch 21/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 156.6793 - val_loss: 147.0633\n",
      "Epoch 22/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 153.6782 - val_loss: 144.4745\n",
      "Epoch 23/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 150.7755 - val_loss: 142.0274\n",
      "Epoch 24/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 148.0945 - val_loss: 139.6989\n",
      "Epoch 25/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 145.5333 - val_loss: 137.5121\n",
      "Epoch 26/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 143.1240 - val_loss: 135.3818\n",
      "Epoch 27/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 140.7436 - val_loss: 133.2819\n",
      "Epoch 28/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 138.4487 - val_loss: 131.2938\n",
      "Epoch 29/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 136.2705 - val_loss: 129.3859\n",
      "Epoch 30/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 134.2210 - val_loss: 127.5046\n",
      "Epoch 31/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 132.2626 - val_loss: 125.7272\n",
      "Epoch 32/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 130.4081 - val_loss: 124.0316\n",
      "Epoch 33/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 128.6136 - val_loss: 122.4342\n",
      "Epoch 34/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 126.9158 - val_loss: 120.8898\n",
      "Epoch 35/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 125.2680 - val_loss: 119.3611\n",
      "Epoch 36/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 123.7104 - val_loss: 117.8692\n",
      "Epoch 37/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 122.1594 - val_loss: 116.3994\n",
      "Epoch 38/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 120.6594 - val_loss: 115.0083\n",
      "Epoch 39/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 119.2480 - val_loss: 113.7211\n",
      "Epoch 40/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 117.9076 - val_loss: 112.4397\n",
      "Epoch 41/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 116.6400 - val_loss: 111.2234\n",
      "Epoch 42/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 115.3726 - val_loss: 110.0988\n",
      "Epoch 43/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 114.2055 - val_loss: 108.9540\n",
      "Epoch 44/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 113.0619 - val_loss: 107.8525\n",
      "Epoch 45/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 111.9161 - val_loss: 106.7752\n",
      "Epoch 46/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 110.7985 - val_loss: 105.7623\n",
      "Epoch 47/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 109.8048 - val_loss: 104.7685\n",
      "Epoch 48/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 108.8190 - val_loss: 103.8122\n",
      "Epoch 49/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 107.8787 - val_loss: 102.9239\n",
      "Epoch 50/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 106.9817 - val_loss: 102.0597\n",
      "Epoch 51/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 106.1281 - val_loss: 101.2100\n",
      "Epoch 52/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 105.2739 - val_loss: 100.3600\n",
      "Epoch 53/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 104.4313 - val_loss: 99.5239\n",
      "Epoch 54/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 103.6500 - val_loss: 98.7286\n",
      "Epoch 55/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 102.8781 - val_loss: 97.9915\n",
      "Epoch 56/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 102.1381 - val_loss: 97.2851\n",
      "Epoch 57/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 101.4185 - val_loss: 96.5544\n",
      "Epoch 58/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 100.7105 - val_loss: 95.8696\n",
      "Epoch 59/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 100.0271 - val_loss: 95.1967\n",
      "Epoch 60/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 99.3679 - val_loss: 94.5018\n",
      "Epoch 61/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 98.7118 - val_loss: 93.8279\n",
      "Epoch 62/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 98.0785 - val_loss: 93.1873\n",
      "Epoch 63/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 97.4580 - val_loss: 92.5601\n",
      "Epoch 64/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 96.8589 - val_loss: 91.9724\n",
      "Epoch 65/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 96.2676 - val_loss: 91.3522\n",
      "Epoch 66/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 95.6798 - val_loss: 90.7944\n",
      "Epoch 67/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 95.1183 - val_loss: 90.2265\n",
      "Epoch 68/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 94.5430 - val_loss: 89.6740\n",
      "Epoch 69/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 94.0131 - val_loss: 89.1193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 93.4802 - val_loss: 88.5746\n",
      "Epoch 71/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 92.9731 - val_loss: 88.0721\n",
      "Epoch 72/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 92.4391 - val_loss: 87.5820\n",
      "Epoch 73/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 91.9453 - val_loss: 87.1070\n",
      "Epoch 74/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 91.4664 - val_loss: 86.5965\n",
      "Epoch 75/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 90.9779 - val_loss: 86.1177\n",
      "Epoch 76/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 90.5149 - val_loss: 85.6505\n",
      "Epoch 77/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 90.0623 - val_loss: 85.1874\n",
      "Epoch 78/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 89.6125 - val_loss: 84.7290\n",
      "Epoch 79/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 89.1680 - val_loss: 84.2876\n",
      "Epoch 80/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 88.7364 - val_loss: 83.8399\n",
      "Epoch 81/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 88.3209 - val_loss: 83.4459\n",
      "Epoch 82/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 87.9108 - val_loss: 83.0384\n",
      "Epoch 83/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 87.5108 - val_loss: 82.6150\n",
      "Epoch 84/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 87.0963 - val_loss: 82.1984\n",
      "Epoch 85/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 86.6911 - val_loss: 81.7716\n",
      "Epoch 86/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 86.2910 - val_loss: 81.3695\n",
      "Epoch 87/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 85.9097 - val_loss: 80.9570\n",
      "Epoch 88/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 85.5241 - val_loss: 80.5747\n",
      "Epoch 89/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 85.1507 - val_loss: 80.1730\n",
      "Epoch 90/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 84.7568 - val_loss: 79.7786\n",
      "Epoch 91/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 84.3765 - val_loss: 79.4097\n",
      "Epoch 92/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 84.0150 - val_loss: 78.9978\n",
      "Epoch 93/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 83.6466 - val_loss: 78.6485\n",
      "Epoch 94/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 83.2945 - val_loss: 78.2776\n",
      "Epoch 95/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 82.9334 - val_loss: 77.9149\n",
      "Epoch 96/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 82.5606 - val_loss: 77.5576\n",
      "Epoch 97/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 82.2367 - val_loss: 77.1766\n",
      "Epoch 98/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 81.8965 - val_loss: 76.8665\n",
      "Epoch 99/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 81.5835 - val_loss: 76.5206\n",
      "Epoch 100/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 81.2545 - val_loss: 76.1967\n",
      "Epoch 101/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 80.9100 - val_loss: 75.8777\n",
      "Epoch 102/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 80.6062 - val_loss: 75.5655\n",
      "Epoch 103/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 80.2859 - val_loss: 75.2260\n",
      "Epoch 104/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 79.9852 - val_loss: 74.9241\n",
      "Epoch 105/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 79.6647 - val_loss: 74.6346\n",
      "Epoch 106/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 79.3891 - val_loss: 74.3126\n",
      "Epoch 107/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 79.0801 - val_loss: 74.0154\n",
      "Epoch 108/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 78.7904 - val_loss: 73.7291\n",
      "Epoch 109/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 78.4981 - val_loss: 73.4347\n",
      "Epoch 110/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 78.2128 - val_loss: 73.1026\n",
      "Epoch 111/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 77.9181 - val_loss: 72.8165\n",
      "Epoch 112/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 77.6350 - val_loss: 72.5451\n",
      "Epoch 113/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 77.3665 - val_loss: 72.2621\n",
      "Epoch 114/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 77.0916 - val_loss: 71.9821\n",
      "Epoch 115/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 76.8323 - val_loss: 71.6880\n",
      "Epoch 116/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 76.5615 - val_loss: 71.4346\n",
      "Epoch 117/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 76.3036 - val_loss: 71.1756\n",
      "Epoch 118/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 76.0397 - val_loss: 70.9117\n",
      "Epoch 119/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 75.7959 - val_loss: 70.6616\n",
      "Epoch 120/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 75.5276 - val_loss: 70.3771\n",
      "Epoch 121/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 75.2580 - val_loss: 70.1268\n",
      "Epoch 122/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 75.0150 - val_loss: 69.8619\n",
      "Epoch 123/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 74.7670 - val_loss: 69.6326\n",
      "Epoch 124/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 74.5275 - val_loss: 69.3862\n",
      "Epoch 125/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 74.2931 - val_loss: 69.1337\n",
      "Epoch 126/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 74.0601 - val_loss: 68.8785\n",
      "Epoch 127/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 73.8013 - val_loss: 68.6506\n",
      "Epoch 128/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 73.5678 - val_loss: 68.4396\n",
      "Epoch 129/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 73.3367 - val_loss: 68.2137\n",
      "Epoch 130/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 73.1045 - val_loss: 68.0034\n",
      "Epoch 131/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 72.8788 - val_loss: 67.8008\n",
      "Epoch 132/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 72.6624 - val_loss: 67.5630\n",
      "Epoch 133/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 72.4372 - val_loss: 67.3396\n",
      "Epoch 134/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 72.2041 - val_loss: 67.0986\n",
      "Epoch 135/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 71.9809 - val_loss: 66.8728\n",
      "Epoch 136/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 71.7465 - val_loss: 66.6594\n",
      "Epoch 137/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 71.5287 - val_loss: 66.4578\n",
      "Epoch 138/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 71.3215 - val_loss: 66.2499\n",
      "Epoch 139/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 71.1092 - val_loss: 66.0334\n",
      "Epoch 140/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 70.9096 - val_loss: 65.8391\n",
      "Epoch 141/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 70.6960 - val_loss: 65.6502\n",
      "Epoch 142/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 70.4792 - val_loss: 65.4616\n",
      "Epoch 143/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 70.2844 - val_loss: 65.2695\n",
      "Epoch 144/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 70.0802 - val_loss: 65.0694\n",
      "Epoch 145/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 69.8816 - val_loss: 64.8749\n",
      "Epoch 146/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 69.6723 - val_loss: 64.6721\n",
      "Epoch 147/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 69.4747 - val_loss: 64.4795\n",
      "Epoch 148/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 69.2837 - val_loss: 64.2744\n",
      "Epoch 149/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 69.0937 - val_loss: 64.1058\n",
      "Epoch 150/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 68.8939 - val_loss: 63.9266\n",
      "Epoch 151/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 68.7167 - val_loss: 63.7650\n",
      "Epoch 152/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 68.5455 - val_loss: 63.6228\n",
      "Epoch 153/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 68.3710 - val_loss: 63.4395\n",
      "Epoch 154/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 68.1886 - val_loss: 63.2510\n",
      "Epoch 155/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 68.0112 - val_loss: 63.0683\n",
      "Epoch 156/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 67.8374 - val_loss: 62.9141\n",
      "Epoch 157/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 67.6659 - val_loss: 62.7501\n",
      "Epoch 158/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 67.5099 - val_loss: 62.6081\n",
      "Epoch 159/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 67.3410 - val_loss: 62.4515\n",
      "Epoch 160/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 67.1866 - val_loss: 62.2765\n",
      "Epoch 161/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 67.0387 - val_loss: 62.1089\n",
      "Epoch 162/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 66.8799 - val_loss: 61.9635\n",
      "Epoch 163/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 66.7373 - val_loss: 61.8137\n",
      "Epoch 164/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 66.5816 - val_loss: 61.6908\n",
      "Epoch 165/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 66.4392 - val_loss: 61.5485\n",
      "Epoch 166/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 66.2978 - val_loss: 61.4149\n",
      "Epoch 167/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 66.1387 - val_loss: 61.2855\n",
      "Epoch 168/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 66.0012 - val_loss: 61.1724\n",
      "Epoch 169/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 65.8690 - val_loss: 61.0249\n",
      "Epoch 170/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.7350 - val_loss: 60.8778\n",
      "Epoch 171/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 65.6006 - val_loss: 60.7551\n",
      "Epoch 172/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 65.4605 - val_loss: 60.6271\n",
      "Epoch 173/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.3260 - val_loss: 60.4836\n",
      "Epoch 174/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 65.1906 - val_loss: 60.3690\n",
      "Epoch 175/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 65.0593 - val_loss: 60.2424\n",
      "Epoch 176/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 64.9418 - val_loss: 60.1248\n",
      "Epoch 177/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.8100 - val_loss: 60.0189\n",
      "Epoch 178/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.6847 - val_loss: 59.8828\n",
      "Epoch 179/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 64.5676 - val_loss: 59.7671\n",
      "Epoch 180/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.4373 - val_loss: 59.6454\n",
      "Epoch 181/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 64.3125 - val_loss: 59.5274\n",
      "Epoch 182/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 64.1962 - val_loss: 59.4193\n",
      "Epoch 183/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.0854 - val_loss: 59.2989\n",
      "Epoch 184/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 63.9730 - val_loss: 59.2031\n",
      "Epoch 185/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 63.8540 - val_loss: 59.1043\n",
      "Epoch 186/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 63.7493 - val_loss: 58.9667\n",
      "Epoch 187/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.6403 - val_loss: 58.8526\n",
      "Epoch 188/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.5235 - val_loss: 58.7571\n",
      "Epoch 189/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 63.4190 - val_loss: 58.6694\n",
      "Epoch 190/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.3021 - val_loss: 58.5530\n",
      "Epoch 191/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.1870 - val_loss: 58.4505\n",
      "Epoch 192/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 63.0888 - val_loss: 58.3606\n",
      "Epoch 193/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 62.9835 - val_loss: 58.2612\n",
      "Epoch 194/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.8722 - val_loss: 58.1675\n",
      "Epoch 195/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 62.7671 - val_loss: 58.0666\n",
      "Epoch 196/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.6668 - val_loss: 57.9554\n",
      "Epoch 197/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.5610 - val_loss: 57.8611\n",
      "Epoch 198/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.4649 - val_loss: 57.7709\n",
      "Epoch 199/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.3711 - val_loss: 57.6855\n",
      "Epoch 200/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.2832 - val_loss: 57.6092\n",
      "Epoch 201/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.1789 - val_loss: 57.5131\n",
      "Epoch 202/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.0788 - val_loss: 57.4397\n",
      "Epoch 203/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 61.9895 - val_loss: 57.3571\n",
      "Epoch 204/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 61.8987 - val_loss: 57.2766\n",
      "Epoch 205/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.8105 - val_loss: 57.2063\n",
      "Epoch 206/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 61.7278 - val_loss: 57.1215\n",
      "Epoch 207/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.6338 - val_loss: 57.0385\n",
      "Epoch 208/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.5528 - val_loss: 56.9673\n",
      "Epoch 209/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 61.4670 - val_loss: 56.8988\n",
      "Epoch 210/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.3755 - val_loss: 56.8165\n",
      "Epoch 211/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 61.2974 - val_loss: 56.7287\n",
      "Epoch 212/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 61.2076 - val_loss: 56.6543\n",
      "Epoch 213/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 61.1355 - val_loss: 56.5796\n",
      "Epoch 214/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 61.0468 - val_loss: 56.5198\n",
      "Epoch 215/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.9701 - val_loss: 56.4432\n",
      "Epoch 216/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 60.8923 - val_loss: 56.3747\n",
      "Epoch 217/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.8180 - val_loss: 56.3116\n",
      "Epoch 218/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.7386 - val_loss: 56.2308\n",
      "Epoch 219/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.6731 - val_loss: 56.1587\n",
      "Epoch 220/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.6016 - val_loss: 56.0987\n",
      "Epoch 221/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.5335 - val_loss: 56.0318\n",
      "Epoch 222/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.4601 - val_loss: 55.9749\n",
      "Epoch 223/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 60.3910 - val_loss: 55.9122\n",
      "Epoch 224/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.3245 - val_loss: 55.8457\n",
      "Epoch 225/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.2701 - val_loss: 55.7870\n",
      "Epoch 226/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.1999 - val_loss: 55.7280\n",
      "Epoch 227/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 60.1478 - val_loss: 55.6616\n",
      "Epoch 228/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 60.0818 - val_loss: 55.6006\n",
      "Epoch 229/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 60.0229 - val_loss: 55.5458\n",
      "Epoch 230/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.9547 - val_loss: 55.4862\n",
      "Epoch 231/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.8995 - val_loss: 55.4372\n",
      "Epoch 232/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.8441 - val_loss: 55.3717\n",
      "Epoch 233/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.8019 - val_loss: 55.3189\n",
      "Epoch 234/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.7398 - val_loss: 55.2791\n",
      "Epoch 235/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.6904 - val_loss: 55.2305\n",
      "Epoch 236/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.6370 - val_loss: 55.1689\n",
      "Epoch 237/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.5918 - val_loss: 55.1292\n",
      "Epoch 238/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.5362 - val_loss: 55.0621\n",
      "Epoch 239/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.4787 - val_loss: 55.0184\n",
      "Epoch 240/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.4368 - val_loss: 54.9703\n",
      "Epoch 241/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.3847 - val_loss: 54.9323\n",
      "Epoch 242/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.3397 - val_loss: 54.8968\n",
      "Epoch 243/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.3000 - val_loss: 54.8502\n",
      "Epoch 244/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.2464 - val_loss: 54.8143\n",
      "Epoch 245/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 59.2081 - val_loss: 54.7690\n",
      "Epoch 246/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 59.1565 - val_loss: 54.7251\n",
      "Epoch 247/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.1123 - val_loss: 54.6971\n",
      "Epoch 248/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.0599 - val_loss: 54.6594\n",
      "Epoch 249/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.0224 - val_loss: 54.6220\n",
      "Epoch 250/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.9724 - val_loss: 54.5747\n",
      "Epoch 251/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.9317 - val_loss: 54.5289\n",
      "Epoch 252/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.8895 - val_loss: 54.4796\n",
      "Epoch 253/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.8445 - val_loss: 54.4291\n",
      "Epoch 254/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.8024 - val_loss: 54.3981\n",
      "Epoch 255/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.7647 - val_loss: 54.3484\n",
      "Epoch 256/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.7116 - val_loss: 54.3008\n",
      "Epoch 257/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.6754 - val_loss: 54.2580\n",
      "Epoch 258/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.6253 - val_loss: 54.2121\n",
      "Epoch 259/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 58.5905 - val_loss: 54.1705\n",
      "Epoch 260/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 58.5549 - val_loss: 54.1389\n",
      "Epoch 261/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 58.5192 - val_loss: 54.0992\n",
      "Epoch 262/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 58.4847 - val_loss: 54.0547\n",
      "Epoch 263/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.4397 - val_loss: 54.0196\n",
      "Epoch 264/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.3973 - val_loss: 53.9750\n",
      "Epoch 265/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.3608 - val_loss: 53.9456\n",
      "Epoch 266/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 58.3314 - val_loss: 53.9127\n",
      "Epoch 267/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 58.2848 - val_loss: 53.8768\n",
      "Epoch 268/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.2466 - val_loss: 53.8406\n",
      "Epoch 269/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.2137 - val_loss: 53.8184\n",
      "Epoch 270/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.1854 - val_loss: 53.7782\n",
      "Epoch 271/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.1484 - val_loss: 53.7593\n",
      "Epoch 272/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.1140 - val_loss: 53.7381\n",
      "Epoch 273/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 58.0826 - val_loss: 53.7125\n",
      "Epoch 274/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 58.0429 - val_loss: 53.6751\n",
      "Epoch 275/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.0176 - val_loss: 53.6435\n",
      "Epoch 276/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 57.9814 - val_loss: 53.6092\n",
      "Epoch 277/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 57.9447 - val_loss: 53.5887\n",
      "Epoch 278/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.9115 - val_loss: 53.5390\n",
      "Epoch 279/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.8799 - val_loss: 53.4985\n",
      "Epoch 280/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.8453 - val_loss: 53.4660\n",
      "Epoch 281/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 57.8167 - val_loss: 53.4513\n",
      "Epoch 282/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.7814 - val_loss: 53.4199\n",
      "Epoch 283/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.7481 - val_loss: 53.3898\n",
      "Epoch 284/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.7214 - val_loss: 53.3581\n",
      "Epoch 285/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.6877 - val_loss: 53.3273\n",
      "Epoch 286/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 57.6691 - val_loss: 53.2967\n",
      "Epoch 287/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.6332 - val_loss: 53.2658\n",
      "Epoch 288/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.6099 - val_loss: 53.2347\n",
      "Epoch 289/5000\n",
      "1126/1126 [==============================] - 0s 45us/step - loss: 57.5795 - val_loss: 53.2071\n",
      "Epoch 290/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.5536 - val_loss: 53.1905\n",
      "Epoch 291/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.5260 - val_loss: 53.1669\n",
      "Epoch 292/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 57.4944 - val_loss: 53.1563\n",
      "Epoch 293/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 57.4635 - val_loss: 53.1483\n",
      "Epoch 294/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 57.4384 - val_loss: 53.1186\n",
      "Epoch 295/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 57.4154 - val_loss: 53.0767\n",
      "Epoch 296/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.3867 - val_loss: 53.0516\n",
      "Epoch 297/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.3570 - val_loss: 53.0268\n",
      "Epoch 298/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.3265 - val_loss: 53.0191\n",
      "Epoch 299/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.3055 - val_loss: 52.9900\n",
      "Epoch 300/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.2761 - val_loss: 52.9712\n",
      "Epoch 301/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.2534 - val_loss: 52.9461\n",
      "Epoch 302/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.2379 - val_loss: 52.9179\n",
      "Epoch 303/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.2125 - val_loss: 52.9034\n",
      "Epoch 304/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.1852 - val_loss: 52.8758\n",
      "Epoch 305/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 57.1614 - val_loss: 52.8426\n",
      "Epoch 306/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.1322 - val_loss: 52.8171\n",
      "Epoch 307/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.1229 - val_loss: 52.7906\n",
      "Epoch 308/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 57.0899 - val_loss: 52.7725\n",
      "Epoch 309/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 57.0643 - val_loss: 52.7592\n",
      "Epoch 310/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.0473 - val_loss: 52.7229\n",
      "Epoch 311/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.0214 - val_loss: 52.7022\n",
      "Epoch 312/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 56.9918 - val_loss: 52.6809\n",
      "Epoch 313/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.9687 - val_loss: 52.6550\n",
      "Epoch 314/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.9490 - val_loss: 52.6293\n",
      "Epoch 315/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.9274 - val_loss: 52.6065\n",
      "Epoch 316/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 56.9062 - val_loss: 52.5986\n",
      "Epoch 317/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 56.8873 - val_loss: 52.5834\n",
      "Epoch 318/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.8559 - val_loss: 52.5577\n",
      "Epoch 319/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.8354 - val_loss: 52.5412\n",
      "Epoch 320/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 56.8155 - val_loss: 52.5269\n",
      "Epoch 321/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.7973 - val_loss: 52.5234\n",
      "Epoch 322/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.7706 - val_loss: 52.5064\n",
      "Epoch 323/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.7565 - val_loss: 52.4755\n",
      "Epoch 324/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.7299 - val_loss: 52.4645\n",
      "Epoch 325/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 56.7039 - val_loss: 52.4450\n",
      "Epoch 326/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 56.6885 - val_loss: 52.4216\n",
      "Epoch 327/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 56.6708 - val_loss: 52.3914\n",
      "Epoch 328/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 56.6472 - val_loss: 52.3724\n",
      "Epoch 329/5000\n",
      "1126/1126 [==============================] - 0s 44us/step - loss: 56.6264 - val_loss: 52.3505\n",
      "Epoch 330/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 56.6068 - val_loss: 52.3336\n",
      "Epoch 331/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 56.5864 - val_loss: 52.3092\n",
      "Epoch 332/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.5724 - val_loss: 52.2872\n",
      "Epoch 333/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.5439 - val_loss: 52.2819\n",
      "Epoch 334/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.5267 - val_loss: 52.2737\n",
      "Epoch 335/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.5030 - val_loss: 52.2727\n",
      "Epoch 336/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.4923 - val_loss: 52.2649\n",
      "Epoch 337/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.4716 - val_loss: 52.2405\n",
      "Epoch 338/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.4533 - val_loss: 52.2158\n",
      "Epoch 339/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.4392 - val_loss: 52.1906\n",
      "Epoch 340/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.4149 - val_loss: 52.1747\n",
      "Epoch 341/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 56.3944 - val_loss: 52.1644\n",
      "Epoch 342/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.3786 - val_loss: 52.1546\n",
      "Epoch 343/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 56.3605 - val_loss: 52.1441\n",
      "Epoch 344/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.3444 - val_loss: 52.1256\n",
      "Epoch 345/5000\n",
      "1126/1126 [==============================] - ETA: 0s - loss: 75.36 - 0s 37us/step - loss: 56.3253 - val_loss: 52.0938\n",
      "Epoch 346/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.3081 - val_loss: 52.0751\n",
      "Epoch 347/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 56.2862 - val_loss: 52.0505\n",
      "Epoch 348/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.2737 - val_loss: 52.0378\n",
      "Epoch 349/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.2529 - val_loss: 52.0201\n",
      "Epoch 350/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.2364 - val_loss: 51.9918\n",
      "Epoch 351/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.2214 - val_loss: 51.9684\n",
      "Epoch 352/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.2028 - val_loss: 51.9503\n",
      "Epoch 353/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.1831 - val_loss: 51.9220\n",
      "Epoch 354/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.1626 - val_loss: 51.9046\n",
      "Epoch 355/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.1533 - val_loss: 51.8861\n",
      "Epoch 356/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.1313 - val_loss: 51.8662\n",
      "Epoch 357/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.1138 - val_loss: 51.8553\n",
      "Epoch 358/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 56.0982 - val_loss: 51.8482\n",
      "Epoch 359/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 56.0804 - val_loss: 51.8210\n",
      "Epoch 360/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 56.0657 - val_loss: 51.8033\n",
      "Epoch 361/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 56.0492 - val_loss: 51.7837\n",
      "Epoch 362/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.0365 - val_loss: 51.7738\n",
      "Epoch 363/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.0150 - val_loss: 51.7619\n",
      "Epoch 364/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.9984 - val_loss: 51.7481\n",
      "Epoch 365/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.9777 - val_loss: 51.7307\n",
      "Epoch 366/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.9674 - val_loss: 51.7181\n",
      "Epoch 367/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 55.9512 - val_loss: 51.7208\n",
      "Epoch 368/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.9310 - val_loss: 51.7057\n",
      "Epoch 369/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.9210 - val_loss: 51.6918\n",
      "Epoch 370/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.8986 - val_loss: 51.6752\n",
      "Epoch 371/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.8897 - val_loss: 51.6558\n",
      "Epoch 372/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.8682 - val_loss: 51.6238\n",
      "Epoch 373/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.8520 - val_loss: 51.6038\n",
      "Epoch 374/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.8439 - val_loss: 51.6023\n",
      "Epoch 375/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.8298 - val_loss: 51.6034\n",
      "Epoch 376/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.8098 - val_loss: 51.5916\n",
      "Epoch 377/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.7880 - val_loss: 51.5629\n",
      "Epoch 378/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.7710 - val_loss: 51.5324\n",
      "Epoch 379/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.7589 - val_loss: 51.5075\n",
      "Epoch 380/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.7382 - val_loss: 51.4878\n",
      "Epoch 381/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.7323 - val_loss: 51.4746\n",
      "Epoch 382/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.7106 - val_loss: 51.4613\n",
      "Epoch 383/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.7033 - val_loss: 51.4495\n",
      "Epoch 384/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.6853 - val_loss: 51.4463\n",
      "Epoch 385/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.6687 - val_loss: 51.4368\n",
      "Epoch 386/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.6526 - val_loss: 51.4329\n",
      "Epoch 387/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.6426 - val_loss: 51.4143\n",
      "Epoch 388/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.6243 - val_loss: 51.3963\n",
      "Epoch 389/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.6137 - val_loss: 51.3721\n",
      "Epoch 390/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.5926 - val_loss: 51.3598\n",
      "Epoch 391/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.5839 - val_loss: 51.3412\n",
      "Epoch 392/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.5659 - val_loss: 51.3211\n",
      "Epoch 393/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.5560 - val_loss: 51.3095\n",
      "Epoch 394/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 55.5368 - val_loss: 51.2984\n",
      "Epoch 395/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 55.5228 - val_loss: 51.2776\n",
      "Epoch 396/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.5032 - val_loss: 51.2618\n",
      "Epoch 397/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 55.4939 - val_loss: 51.2469\n",
      "Epoch 398/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.4840 - val_loss: 51.2343\n",
      "Epoch 399/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 55.4685 - val_loss: 51.2059\n",
      "Epoch 400/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.4512 - val_loss: 51.1908\n",
      "Epoch 401/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.4408 - val_loss: 51.1918\n",
      "Epoch 402/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.4279 - val_loss: 51.1746\n",
      "Epoch 403/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 55.4073 - val_loss: 51.1599\n",
      "Epoch 404/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 55.3897 - val_loss: 51.1479\n",
      "Epoch 405/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.3797 - val_loss: 51.1314\n",
      "Epoch 406/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.3650 - val_loss: 51.1311\n",
      "Epoch 407/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.3523 - val_loss: 51.1225\n",
      "Epoch 408/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.3357 - val_loss: 51.1164\n",
      "Epoch 409/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 55.3196 - val_loss: 51.1054\n",
      "Epoch 410/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.3159 - val_loss: 51.0977\n",
      "Epoch 411/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.2999 - val_loss: 51.0901\n",
      "Epoch 412/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.2890 - val_loss: 51.0796\n",
      "Epoch 413/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.2686 - val_loss: 51.0589\n",
      "Epoch 414/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.2493 - val_loss: 51.0460\n",
      "Epoch 415/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.2436 - val_loss: 51.0291\n",
      "Epoch 416/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.2238 - val_loss: 51.0304\n",
      "Epoch 417/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.2116 - val_loss: 51.0094\n",
      "Epoch 418/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.2009 - val_loss: 50.9977\n",
      "Epoch 419/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 55.1866 - val_loss: 50.9753\n",
      "Epoch 420/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.1727 - val_loss: 50.9587\n",
      "Epoch 421/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 55.1584 - val_loss: 50.9514\n",
      "Epoch 422/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.1410 - val_loss: 50.9364\n",
      "Epoch 423/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.1271 - val_loss: 50.9272\n",
      "Epoch 424/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.1175 - val_loss: 50.9159\n",
      "Epoch 425/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.0990 - val_loss: 50.9001\n",
      "Epoch 426/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.0920 - val_loss: 50.8824\n",
      "Epoch 427/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.0771 - val_loss: 50.8619\n",
      "Epoch 428/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.0641 - val_loss: 50.8415\n",
      "Epoch 429/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.0476 - val_loss: 50.8299\n",
      "Epoch 430/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.0340 - val_loss: 50.8188\n",
      "Epoch 431/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.0198 - val_loss: 50.8050\n",
      "Epoch 432/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.0075 - val_loss: 50.7935\n",
      "Epoch 433/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.9957 - val_loss: 50.7833\n",
      "Epoch 434/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.9841 - val_loss: 50.7686\n",
      "Epoch 435/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.9654 - val_loss: 50.7590\n",
      "Epoch 436/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.9543 - val_loss: 50.7427\n",
      "Epoch 437/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 54.9429 - val_loss: 50.7174\n",
      "Epoch 438/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 54.9331 - val_loss: 50.7237\n",
      "Epoch 439/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 54.9209 - val_loss: 50.7057\n",
      "Epoch 440/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 54.9051 - val_loss: 50.6945\n",
      "Epoch 441/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 54.8904 - val_loss: 50.6804\n",
      "Epoch 442/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 54.8754 - val_loss: 50.6689\n",
      "Epoch 443/5000\n",
      "1126/1126 [==============================] - 0s 44us/step - loss: 54.8647 - val_loss: 50.6665\n",
      "Epoch 444/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 54.8538 - val_loss: 50.6555\n",
      "Epoch 445/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 54.8371 - val_loss: 50.6394\n",
      "Epoch 446/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 54.8194 - val_loss: 50.6257\n",
      "Epoch 447/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.8084 - val_loss: 50.6092\n",
      "Epoch 448/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.7959 - val_loss: 50.5908\n",
      "Epoch 449/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 54.7842 - val_loss: 50.5797\n",
      "Epoch 450/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.7745 - val_loss: 50.5754\n",
      "Epoch 451/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.7576 - val_loss: 50.5661\n",
      "Epoch 452/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.7394 - val_loss: 50.5516\n",
      "Epoch 453/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 54.7277 - val_loss: 50.5507\n",
      "Epoch 454/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.7161 - val_loss: 50.5306\n",
      "Epoch 455/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.7033 - val_loss: 50.5293\n",
      "Epoch 456/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.6864 - val_loss: 50.5140\n",
      "Epoch 457/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.6738 - val_loss: 50.4931\n",
      "Epoch 458/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.6625 - val_loss: 50.4923\n",
      "Epoch 459/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.6476 - val_loss: 50.4952\n",
      "Epoch 460/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.6387 - val_loss: 50.4892\n",
      "Epoch 461/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.6227 - val_loss: 50.4777\n",
      "Epoch 462/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.6085 - val_loss: 50.4678\n",
      "Epoch 463/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.5951 - val_loss: 50.4604\n",
      "Epoch 464/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 54.5765 - val_loss: 50.4522\n",
      "Epoch 465/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.5709 - val_loss: 50.4488\n",
      "Epoch 466/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 54.5565 - val_loss: 50.4334\n",
      "Epoch 467/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.5426 - val_loss: 50.4123\n",
      "Epoch 468/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.5331 - val_loss: 50.4081\n",
      "Epoch 469/5000\n",
      "1126/1126 [==============================] - 0s 44us/step - loss: 54.5181 - val_loss: 50.3924\n",
      "Epoch 470/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.5069 - val_loss: 50.3737\n",
      "Epoch 471/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.4904 - val_loss: 50.3676\n",
      "Epoch 472/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.4787 - val_loss: 50.3521\n",
      "Epoch 473/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 54.4630 - val_loss: 50.3469\n",
      "Epoch 474/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.4560 - val_loss: 50.3319\n",
      "Epoch 475/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.4396 - val_loss: 50.3211\n",
      "Epoch 476/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.4251 - val_loss: 50.3139\n",
      "Epoch 477/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.4186 - val_loss: 50.3281\n",
      "Epoch 478/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.4094 - val_loss: 50.3085\n",
      "Epoch 479/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 54.3910 - val_loss: 50.3020\n",
      "Epoch 480/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.3798 - val_loss: 50.2867\n",
      "Epoch 481/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.3639 - val_loss: 50.2798\n",
      "Epoch 482/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.3500 - val_loss: 50.2646\n",
      "Epoch 483/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.3412 - val_loss: 50.2516\n",
      "Epoch 484/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.3300 - val_loss: 50.2378\n",
      "Epoch 485/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.3180 - val_loss: 50.2339\n",
      "Epoch 486/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.3049 - val_loss: 50.2232\n",
      "Epoch 487/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.2977 - val_loss: 50.2068\n",
      "Epoch 488/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.2786 - val_loss: 50.1857\n",
      "Epoch 489/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.2706 - val_loss: 50.1726\n",
      "Epoch 490/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.2561 - val_loss: 50.1653\n",
      "Epoch 491/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.2454 - val_loss: 50.1491\n",
      "Epoch 492/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.2344 - val_loss: 50.1345\n",
      "Epoch 493/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.2254 - val_loss: 50.1224\n",
      "Epoch 494/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.2079 - val_loss: 50.1114\n",
      "Epoch 495/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.1988 - val_loss: 50.1155\n",
      "Epoch 496/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 54.1904 - val_loss: 50.1001\n",
      "Epoch 497/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 54.1779 - val_loss: 50.0904\n",
      "Epoch 498/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.1692 - val_loss: 50.0759\n",
      "Epoch 499/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 54.1571 - val_loss: 50.0621\n",
      "Epoch 500/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.1423 - val_loss: 50.0493\n",
      "Epoch 501/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.1365 - val_loss: 50.0351\n",
      "Epoch 502/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.1213 - val_loss: 50.0208\n",
      "Epoch 503/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.1138 - val_loss: 50.0118\n",
      "Epoch 504/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.1018 - val_loss: 49.9979\n",
      "Epoch 505/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0916 - val_loss: 49.9794\n",
      "Epoch 506/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0802 - val_loss: 49.9793\n",
      "Epoch 507/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0732 - val_loss: 49.9671\n",
      "Epoch 508/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0614 - val_loss: 49.9553\n",
      "Epoch 509/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0527 - val_loss: 49.9405\n",
      "Epoch 510/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 54.0423 - val_loss: 49.9264\n",
      "Epoch 511/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0281 - val_loss: 49.9175\n",
      "Epoch 512/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.0184 - val_loss: 49.9186\n",
      "Epoch 513/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 54.0088 - val_loss: 49.9226\n",
      "Epoch 514/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.9975 - val_loss: 49.9036\n",
      "Epoch 515/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.9897 - val_loss: 49.8900\n",
      "Epoch 516/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.9768 - val_loss: 49.8793\n",
      "Epoch 517/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.9720 - val_loss: 49.8737\n",
      "Epoch 518/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.9556 - val_loss: 49.8696\n",
      "Epoch 519/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.9484 - val_loss: 49.8552\n",
      "Epoch 520/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.9417 - val_loss: 49.8501\n",
      "Epoch 521/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.9304 - val_loss: 49.8377\n",
      "Epoch 522/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.9177 - val_loss: 49.8193\n",
      "Epoch 523/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.9120 - val_loss: 49.8126\n",
      "Epoch 524/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.9042 - val_loss: 49.8043\n",
      "Epoch 525/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.9007 - val_loss: 49.7908\n",
      "Epoch 526/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.8807 - val_loss: 49.7802\n",
      "Epoch 527/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.8708 - val_loss: 49.7784\n",
      "Epoch 528/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.8656 - val_loss: 49.7738\n",
      "Epoch 529/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.8607 - val_loss: 49.7549\n",
      "Epoch 530/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.8493 - val_loss: 49.7445\n",
      "Epoch 531/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.8410 - val_loss: 49.7358\n",
      "Epoch 532/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 53.8305 - val_loss: 49.7297\n",
      "Epoch 533/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.8205 - val_loss: 49.7241\n",
      "Epoch 534/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.8063 - val_loss: 49.7147\n",
      "Epoch 535/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.8048 - val_loss: 49.7050\n",
      "Epoch 536/5000\n",
      "1126/1126 [==============================] - ETA: 0s - loss: 76.12 - 0s 32us/step - loss: 53.7892 - val_loss: 49.7056\n",
      "Epoch 537/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.7858 - val_loss: 49.7047\n",
      "Epoch 538/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.7750 - val_loss: 49.7031\n",
      "Epoch 539/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.7678 - val_loss: 49.6984\n",
      "Epoch 540/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.7602 - val_loss: 49.6899\n",
      "Epoch 541/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7493 - val_loss: 49.6723\n",
      "Epoch 542/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7455 - val_loss: 49.6585\n",
      "Epoch 543/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.7351 - val_loss: 49.6560\n",
      "Epoch 544/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.7279 - val_loss: 49.6484\n",
      "Epoch 545/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.7146 - val_loss: 49.6443\n",
      "Epoch 546/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7110 - val_loss: 49.6343\n",
      "Epoch 547/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7010 - val_loss: 49.6279\n",
      "Epoch 548/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.6939 - val_loss: 49.6129\n",
      "Epoch 549/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 53.6851 - val_loss: 49.6016\n",
      "Epoch 550/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.6772 - val_loss: 49.5962\n",
      "Epoch 551/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.6634 - val_loss: 49.5843\n",
      "Epoch 552/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.6634 - val_loss: 49.5698\n",
      "Epoch 553/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.6554 - val_loss: 49.5606\n",
      "Epoch 554/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.6426 - val_loss: 49.5606\n",
      "Epoch 555/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.6376 - val_loss: 49.5585\n",
      "Epoch 556/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.6288 - val_loss: 49.5520\n",
      "Epoch 557/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.6212 - val_loss: 49.5452\n",
      "Epoch 558/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.6153 - val_loss: 49.5440\n",
      "Epoch 559/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.6044 - val_loss: 49.5395\n",
      "Epoch 560/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.5964 - val_loss: 49.5399\n",
      "Epoch 561/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.5896 - val_loss: 49.5310\n",
      "Epoch 562/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.5800 - val_loss: 49.5375\n",
      "Epoch 563/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.5745 - val_loss: 49.5239\n",
      "Epoch 564/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.5644 - val_loss: 49.5194\n",
      "Epoch 565/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.5626 - val_loss: 49.5150\n",
      "Epoch 566/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.5533 - val_loss: 49.5027\n",
      "Epoch 567/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.5452 - val_loss: 49.4938\n",
      "Epoch 568/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.5328 - val_loss: 49.4954\n",
      "Epoch 569/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.5269 - val_loss: 49.4822\n",
      "Epoch 570/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.5200 - val_loss: 49.4648\n",
      "Epoch 571/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 53.5152 - val_loss: 49.4565\n",
      "Epoch 572/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.5072 - val_loss: 49.4606\n",
      "Epoch 573/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4943 - val_loss: 49.4633\n",
      "Epoch 574/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.4924 - val_loss: 49.4537\n",
      "Epoch 575/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4822 - val_loss: 49.4437\n",
      "Epoch 576/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4741 - val_loss: 49.4468\n",
      "Epoch 577/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.4693 - val_loss: 49.4302\n",
      "Epoch 578/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4638 - val_loss: 49.4296\n",
      "Epoch 579/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.4606 - val_loss: 49.4221\n",
      "Epoch 580/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.4473 - val_loss: 49.4114\n",
      "Epoch 581/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4425 - val_loss: 49.4044\n",
      "Epoch 582/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.4372 - val_loss: 49.4045\n",
      "Epoch 583/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 53.4236 - val_loss: 49.3717\n",
      "Epoch 584/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.4209 - val_loss: 49.3789\n",
      "Epoch 585/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.4123 - val_loss: 49.3693\n",
      "Epoch 586/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.4021 - val_loss: 49.3597\n",
      "Epoch 587/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.3937 - val_loss: 49.3528\n",
      "Epoch 588/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3938 - val_loss: 49.3436\n",
      "Epoch 589/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3834 - val_loss: 49.3332\n",
      "Epoch 590/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3747 - val_loss: 49.3231\n",
      "Epoch 591/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.3671 - val_loss: 49.3128\n",
      "Epoch 592/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3608 - val_loss: 49.2969\n",
      "Epoch 593/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3577 - val_loss: 49.2919\n",
      "Epoch 594/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.3480 - val_loss: 49.2898\n",
      "Epoch 595/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.3419 - val_loss: 49.2831\n",
      "Epoch 596/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.3343 - val_loss: 49.2732\n",
      "Epoch 597/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3218 - val_loss: 49.2659\n",
      "Epoch 598/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3157 - val_loss: 49.2599\n",
      "Epoch 599/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.3098 - val_loss: 49.2546\n",
      "Epoch 600/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3018 - val_loss: 49.2421\n",
      "Epoch 601/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.2922 - val_loss: 49.2420\n",
      "Epoch 602/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.2902 - val_loss: 49.2374\n",
      "Epoch 603/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2806 - val_loss: 49.2282\n",
      "Epoch 604/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.2783 - val_loss: 49.2229\n",
      "Epoch 605/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2683 - val_loss: 49.2269\n",
      "Epoch 606/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2611 - val_loss: 49.2165\n",
      "Epoch 607/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2488 - val_loss: 49.2127\n",
      "Epoch 608/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2425 - val_loss: 49.2060\n",
      "Epoch 609/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2449 - val_loss: 49.1954\n",
      "Epoch 610/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2333 - val_loss: 49.1807\n",
      "Epoch 611/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.2272 - val_loss: 49.1793\n",
      "Epoch 612/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.2172 - val_loss: 49.1696\n",
      "Epoch 613/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.2099 - val_loss: 49.1692\n",
      "Epoch 614/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.2064 - val_loss: 49.1709\n",
      "Epoch 615/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.2084 - val_loss: 49.1622\n",
      "Epoch 616/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1966 - val_loss: 49.1662\n",
      "Epoch 617/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.1845 - val_loss: 49.1694\n",
      "Epoch 618/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1729 - val_loss: 49.1714\n",
      "Epoch 619/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.1673 - val_loss: 49.1647\n",
      "Epoch 620/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1643 - val_loss: 49.1666\n",
      "Epoch 621/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 53.1595 - val_loss: 49.1639\n",
      "Epoch 622/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1480 - val_loss: 49.1516\n",
      "Epoch 623/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1388 - val_loss: 49.1436\n",
      "Epoch 624/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.1352 - val_loss: 49.1399\n",
      "Epoch 625/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1271 - val_loss: 49.1274\n",
      "Epoch 626/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.1222 - val_loss: 49.1154\n",
      "Epoch 627/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.1148 - val_loss: 49.0988\n",
      "Epoch 628/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.1089 - val_loss: 49.0974\n",
      "Epoch 629/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.0992 - val_loss: 49.0845\n",
      "Epoch 630/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.0941 - val_loss: 49.0822\n",
      "Epoch 631/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.0875 - val_loss: 49.0682\n",
      "Epoch 632/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 53.0797 - val_loss: 49.0525\n",
      "Epoch 633/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.0740 - val_loss: 49.0518\n",
      "Epoch 634/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 53.0661 - val_loss: 49.0466\n",
      "Epoch 635/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.0611 - val_loss: 49.0422\n",
      "Epoch 636/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.0527 - val_loss: 49.0417\n",
      "Epoch 637/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.0506 - val_loss: 49.0367\n",
      "Epoch 638/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.0373 - val_loss: 49.0267\n",
      "Epoch 639/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.0361 - val_loss: 49.0179\n",
      "Epoch 640/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.0305 - val_loss: 49.0113\n",
      "Epoch 641/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.0202 - val_loss: 49.0145\n",
      "Epoch 642/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.0146 - val_loss: 48.9950\n",
      "Epoch 643/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.0061 - val_loss: 48.9866\n",
      "Epoch 644/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 53.0026 - val_loss: 48.9823\n",
      "Epoch 645/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9930 - val_loss: 48.9756\n",
      "Epoch 646/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9860 - val_loss: 48.9792\n",
      "Epoch 647/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.9800 - val_loss: 48.9778\n",
      "Epoch 648/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9721 - val_loss: 48.9630\n",
      "Epoch 649/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.9654 - val_loss: 48.9568\n",
      "Epoch 650/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.9594 - val_loss: 48.9473\n",
      "Epoch 651/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9542 - val_loss: 48.9483\n",
      "Epoch 652/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.9451 - val_loss: 48.9498\n",
      "Epoch 653/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9417 - val_loss: 48.9463\n",
      "Epoch 654/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.9360 - val_loss: 48.9415\n",
      "Epoch 655/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9284 - val_loss: 48.9329\n",
      "Epoch 656/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9202 - val_loss: 48.9279\n",
      "Epoch 657/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.9186 - val_loss: 48.9222\n",
      "Epoch 658/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.9064 - val_loss: 48.9167\n",
      "Epoch 659/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.9046 - val_loss: 48.9232\n",
      "Epoch 660/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8991 - val_loss: 48.9201\n",
      "Epoch 661/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.8881 - val_loss: 48.9169\n",
      "Epoch 662/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.8893 - val_loss: 48.9135\n",
      "Epoch 663/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8778 - val_loss: 48.9195\n",
      "Epoch 664/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.8671 - val_loss: 48.9095\n",
      "Epoch 665/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8590 - val_loss: 48.8986\n",
      "Epoch 666/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8626 - val_loss: 48.8922\n",
      "Epoch 667/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8516 - val_loss: 48.8883\n",
      "Epoch 668/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.8459 - val_loss: 48.8883\n",
      "Epoch 669/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8396 - val_loss: 48.8795\n",
      "Epoch 670/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8340 - val_loss: 48.8772\n",
      "Epoch 671/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8266 - val_loss: 48.8687\n",
      "Epoch 672/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8189 - val_loss: 48.8624\n",
      "Epoch 673/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8112 - val_loss: 48.8572\n",
      "Epoch 674/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.8077 - val_loss: 48.8508\n",
      "Epoch 675/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.8044 - val_loss: 48.8446\n",
      "Epoch 676/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7964 - val_loss: 48.8390\n",
      "Epoch 677/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7879 - val_loss: 48.8338\n",
      "Epoch 678/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7828 - val_loss: 48.8356\n",
      "Epoch 679/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7759 - val_loss: 48.8282\n",
      "Epoch 680/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.7746 - val_loss: 48.8242\n",
      "Epoch 681/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7624 - val_loss: 48.8085\n",
      "Epoch 682/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7585 - val_loss: 48.8049\n",
      "Epoch 683/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.7529 - val_loss: 48.7964\n",
      "Epoch 684/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7464 - val_loss: 48.7988\n",
      "Epoch 685/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.7428 - val_loss: 48.7926\n",
      "Epoch 686/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.7348 - val_loss: 48.7841\n",
      "Epoch 687/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.7298 - val_loss: 48.7757\n",
      "Epoch 688/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7206 - val_loss: 48.7722\n",
      "Epoch 689/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7160 - val_loss: 48.7693\n",
      "Epoch 690/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.7109 - val_loss: 48.7760\n",
      "Epoch 691/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.7071 - val_loss: 48.7700\n",
      "Epoch 692/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.6972 - val_loss: 48.7572\n",
      "Epoch 693/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6947 - val_loss: 48.7613\n",
      "Epoch 694/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6857 - val_loss: 48.7605\n",
      "Epoch 695/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6798 - val_loss: 48.7573\n",
      "Epoch 696/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6721 - val_loss: 48.7586\n",
      "Epoch 697/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.6661 - val_loss: 48.7441\n",
      "Epoch 698/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.6607 - val_loss: 48.7305\n",
      "Epoch 699/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6555 - val_loss: 48.7310\n",
      "Epoch 700/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6488 - val_loss: 48.7360\n",
      "Epoch 701/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6417 - val_loss: 48.7288\n",
      "Epoch 702/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6365 - val_loss: 48.7226\n",
      "Epoch 703/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.6308 - val_loss: 48.7113\n",
      "Epoch 704/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.6231 - val_loss: 48.6998\n",
      "Epoch 705/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6143 - val_loss: 48.6941\n",
      "Epoch 706/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6111 - val_loss: 48.6828\n",
      "Epoch 707/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.6065 - val_loss: 48.6740\n",
      "Epoch 708/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.6046 - val_loss: 48.6724\n",
      "Epoch 709/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5932 - val_loss: 48.6647\n",
      "Epoch 710/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5880 - val_loss: 48.6603\n",
      "Epoch 711/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.5878 - val_loss: 48.6571\n",
      "Epoch 712/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5797 - val_loss: 48.6514\n",
      "Epoch 713/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.5750 - val_loss: 48.6359\n",
      "Epoch 714/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5667 - val_loss: 48.6298\n",
      "Epoch 715/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.5648 - val_loss: 48.6273\n",
      "Epoch 716/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.5626 - val_loss: 48.6152\n",
      "Epoch 717/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5548 - val_loss: 48.6075\n",
      "Epoch 718/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.5505 - val_loss: 48.6057\n",
      "Epoch 719/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5423 - val_loss: 48.6021\n",
      "Epoch 720/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5375 - val_loss: 48.6017\n",
      "Epoch 721/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.5294 - val_loss: 48.6001\n",
      "Epoch 722/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5216 - val_loss: 48.5915\n",
      "Epoch 723/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5192 - val_loss: 48.5846\n",
      "Epoch 724/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5160 - val_loss: 48.5780\n",
      "Epoch 725/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.5074 - val_loss: 48.5724\n",
      "Epoch 726/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.5030 - val_loss: 48.5669\n",
      "Epoch 727/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.4957 - val_loss: 48.5581\n",
      "Epoch 728/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4921 - val_loss: 48.5573\n",
      "Epoch 729/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4841 - val_loss: 48.5515\n",
      "Epoch 730/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.4807 - val_loss: 48.5467\n",
      "Epoch 731/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4747 - val_loss: 48.5477\n",
      "Epoch 732/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.4759 - val_loss: 48.5378\n",
      "Epoch 733/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4630 - val_loss: 48.5315\n",
      "Epoch 734/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4596 - val_loss: 48.5194\n",
      "Epoch 735/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.4538 - val_loss: 48.5148\n",
      "Epoch 736/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.4488 - val_loss: 48.5085\n",
      "Epoch 737/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.4423 - val_loss: 48.5045\n",
      "Epoch 738/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4343 - val_loss: 48.5013\n",
      "Epoch 739/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 52.4312 - val_loss: 48.4925\n",
      "Epoch 740/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.4230 - val_loss: 48.4844\n",
      "Epoch 741/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.4190 - val_loss: 48.4905\n",
      "Epoch 742/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 52.4174 - val_loss: 48.4921\n",
      "Epoch 743/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 52.4106 - val_loss: 48.4952\n",
      "Epoch 744/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.4053 - val_loss: 48.4853\n",
      "Epoch 745/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.3976 - val_loss: 48.4836\n",
      "Epoch 746/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.3901 - val_loss: 48.4834\n",
      "Epoch 747/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3872 - val_loss: 48.4808\n",
      "Epoch 748/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.3824 - val_loss: 48.4762\n",
      "Epoch 749/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3797 - val_loss: 48.4738\n",
      "Epoch 750/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3717 - val_loss: 48.4812\n",
      "Epoch 751/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.3666 - val_loss: 48.4894\n",
      "Epoch 752/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3624 - val_loss: 48.4744\n",
      "Epoch 753/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3553 - val_loss: 48.4645\n",
      "Epoch 754/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.3514 - val_loss: 48.4566\n",
      "Epoch 755/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.3470 - val_loss: 48.4519\n",
      "Epoch 756/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3387 - val_loss: 48.4481\n",
      "Epoch 757/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.3342 - val_loss: 48.4538\n",
      "Epoch 758/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.3331 - val_loss: 48.4444\n",
      "Epoch 759/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.3245 - val_loss: 48.4365\n",
      "Epoch 760/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3162 - val_loss: 48.4307\n",
      "Epoch 761/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.3180 - val_loss: 48.4210\n",
      "Epoch 762/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.3098 - val_loss: 48.4129\n",
      "Epoch 763/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.3054 - val_loss: 48.4037\n",
      "Epoch 764/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2978 - val_loss: 48.3985\n",
      "Epoch 765/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2957 - val_loss: 48.3959\n",
      "Epoch 766/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2889 - val_loss: 48.3963\n",
      "Epoch 767/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.2796 - val_loss: 48.3938\n",
      "Epoch 768/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.2784 - val_loss: 48.3866\n",
      "Epoch 769/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2717 - val_loss: 48.3813\n",
      "Epoch 770/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2681 - val_loss: 48.3770\n",
      "Epoch 771/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.2642 - val_loss: 48.3712\n",
      "Epoch 772/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.2563 - val_loss: 48.3664\n",
      "Epoch 773/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2502 - val_loss: 48.3591\n",
      "Epoch 774/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.2443 - val_loss: 48.3521\n",
      "Epoch 775/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2402 - val_loss: 48.3446\n",
      "Epoch 776/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2370 - val_loss: 48.3363\n",
      "Epoch 777/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2345 - val_loss: 48.3317\n",
      "Epoch 778/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.2239 - val_loss: 48.3312\n",
      "Epoch 779/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.2190 - val_loss: 48.3222\n",
      "Epoch 780/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2167 - val_loss: 48.3179\n",
      "Epoch 781/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2088 - val_loss: 48.3169\n",
      "Epoch 782/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.2031 - val_loss: 48.3139\n",
      "Epoch 783/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.2033 - val_loss: 48.3053\n",
      "Epoch 784/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1919 - val_loss: 48.3039\n",
      "Epoch 785/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1880 - val_loss: 48.3014\n",
      "Epoch 786/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.1873 - val_loss: 48.2951\n",
      "Epoch 787/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 52.1818 - val_loss: 48.2904\n",
      "Epoch 788/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 52.1741 - val_loss: 48.2834\n",
      "Epoch 789/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 52.1699 - val_loss: 48.2770\n",
      "Epoch 790/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.1655 - val_loss: 48.2763\n",
      "Epoch 791/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 52.1602 - val_loss: 48.2757\n",
      "Epoch 792/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 52.1558 - val_loss: 48.2748\n",
      "Epoch 793/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.1474 - val_loss: 48.2701\n",
      "Epoch 794/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1430 - val_loss: 48.2595\n",
      "Epoch 795/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.1346 - val_loss: 48.2616\n",
      "Epoch 796/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1309 - val_loss: 48.2581\n",
      "Epoch 797/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.1312 - val_loss: 48.2524\n",
      "Epoch 798/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.1257 - val_loss: 48.2520\n",
      "Epoch 799/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1187 - val_loss: 48.2410\n",
      "Epoch 800/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.1132 - val_loss: 48.2398\n",
      "Epoch 801/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1109 - val_loss: 48.2387\n",
      "Epoch 802/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.1041 - val_loss: 48.2434\n",
      "Epoch 803/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.0973 - val_loss: 48.2366\n",
      "Epoch 804/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0909 - val_loss: 48.2292\n",
      "Epoch 805/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0859 - val_loss: 48.2348\n",
      "Epoch 806/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.0798 - val_loss: 48.2313\n",
      "Epoch 807/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0775 - val_loss: 48.2243\n",
      "Epoch 808/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0757 - val_loss: 48.2260\n",
      "Epoch 809/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0661 - val_loss: 48.2279\n",
      "Epoch 810/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0613 - val_loss: 48.2222\n",
      "Epoch 811/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0600 - val_loss: 48.2169\n",
      "Epoch 812/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 52.0553 - val_loss: 48.2022\n",
      "Epoch 813/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0491 - val_loss: 48.2036\n",
      "Epoch 814/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.0405 - val_loss: 48.1964\n",
      "Epoch 815/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.0390 - val_loss: 48.1881\n",
      "Epoch 816/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.0298 - val_loss: 48.1822\n",
      "Epoch 817/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0289 - val_loss: 48.1848\n",
      "Epoch 818/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0247 - val_loss: 48.1754\n",
      "Epoch 819/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.0200 - val_loss: 48.1696\n",
      "Epoch 820/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.0116 - val_loss: 48.1688\n",
      "Epoch 821/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 52.0080 - val_loss: 48.1603\n",
      "Epoch 822/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.0069 - val_loss: 48.1549\n",
      "Epoch 823/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.0002 - val_loss: 48.1419\n",
      "Epoch 824/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9947 - val_loss: 48.1323\n",
      "Epoch 825/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9879 - val_loss: 48.1295\n",
      "Epoch 826/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.9834 - val_loss: 48.1202\n",
      "Epoch 827/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.9799 - val_loss: 48.1130\n",
      "Epoch 828/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.9750 - val_loss: 48.1101\n",
      "Epoch 829/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9719 - val_loss: 48.1067\n",
      "Epoch 830/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9647 - val_loss: 48.1117\n",
      "Epoch 831/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9599 - val_loss: 48.1029\n",
      "Epoch 832/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9584 - val_loss: 48.0976\n",
      "Epoch 833/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9520 - val_loss: 48.0892\n",
      "Epoch 834/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9433 - val_loss: 48.0768\n",
      "Epoch 835/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9386 - val_loss: 48.0739\n",
      "Epoch 836/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.9336 - val_loss: 48.0706\n",
      "Epoch 837/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.9321 - val_loss: 48.0764\n",
      "Epoch 838/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9293 - val_loss: 48.0733\n",
      "Epoch 839/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.9214 - val_loss: 48.0645\n",
      "Epoch 840/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9152 - val_loss: 48.0657\n",
      "Epoch 841/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9142 - val_loss: 48.0590\n",
      "Epoch 842/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9101 - val_loss: 48.0570\n",
      "Epoch 843/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.9044 - val_loss: 48.0492\n",
      "Epoch 844/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.9013 - val_loss: 48.0429\n",
      "Epoch 845/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.8969 - val_loss: 48.0324\n",
      "Epoch 846/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.8955 - val_loss: 48.0301\n",
      "Epoch 847/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.8879 - val_loss: 48.0225\n",
      "Epoch 848/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.8836 - val_loss: 48.0136\n",
      "Epoch 849/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.8810 - val_loss: 48.0209\n",
      "Epoch 850/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8754 - val_loss: 48.0153\n",
      "Epoch 851/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8716 - val_loss: 48.0170\n",
      "Epoch 852/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8619 - val_loss: 48.0263\n",
      "Epoch 853/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.8586 - val_loss: 48.0207\n",
      "Epoch 854/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.8573 - val_loss: 48.0124\n",
      "Epoch 855/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.8493 - val_loss: 48.0076\n",
      "Epoch 856/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.8470 - val_loss: 48.0052\n",
      "Epoch 857/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8443 - val_loss: 47.9954\n",
      "Epoch 858/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.8432 - val_loss: 47.9938\n",
      "Epoch 859/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8327 - val_loss: 47.9930\n",
      "Epoch 860/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8311 - val_loss: 47.9929\n",
      "Epoch 861/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8244 - val_loss: 47.9928\n",
      "Epoch 862/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8234 - val_loss: 47.9885\n",
      "Epoch 863/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.8155 - val_loss: 47.9823\n",
      "Epoch 864/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.8119 - val_loss: 47.9802\n",
      "Epoch 865/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.8029 - val_loss: 47.9773\n",
      "Epoch 866/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.8054 - val_loss: 47.9746\n",
      "Epoch 867/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7915 - val_loss: 47.9685\n",
      "Epoch 868/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.7908 - val_loss: 47.9666\n",
      "Epoch 869/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.7876 - val_loss: 47.9587\n",
      "Epoch 870/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.7840 - val_loss: 47.9524\n",
      "Epoch 871/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 51.7817 - val_loss: 47.9402\n",
      "Epoch 872/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.7771 - val_loss: 47.9378\n",
      "Epoch 873/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.7697 - val_loss: 47.9348\n",
      "Epoch 874/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.7679 - val_loss: 47.9304\n",
      "Epoch 875/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.7601 - val_loss: 47.9333\n",
      "Epoch 876/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.7579 - val_loss: 47.9312\n",
      "Epoch 877/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7515 - val_loss: 47.9330\n",
      "Epoch 878/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7487 - val_loss: 47.9353\n",
      "Epoch 879/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7467 - val_loss: 47.9254\n",
      "Epoch 880/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7410 - val_loss: 47.9232\n",
      "Epoch 881/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.7363 - val_loss: 47.9244\n",
      "Epoch 882/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7323 - val_loss: 47.9250\n",
      "Epoch 883/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.7271 - val_loss: 47.9210\n",
      "Epoch 884/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.7247 - val_loss: 47.9142\n",
      "Epoch 885/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.7202 - val_loss: 47.9043\n",
      "Epoch 886/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7154 - val_loss: 47.8969\n",
      "Epoch 887/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.7112 - val_loss: 47.9014\n",
      "Epoch 888/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7058 - val_loss: 47.8968\n",
      "Epoch 889/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.7001 - val_loss: 47.8989\n",
      "Epoch 890/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6948 - val_loss: 47.8930\n",
      "Epoch 891/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6904 - val_loss: 47.8903\n",
      "Epoch 892/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6868 - val_loss: 47.8872\n",
      "Epoch 893/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6846 - val_loss: 47.8811\n",
      "Epoch 894/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6781 - val_loss: 47.8737\n",
      "Epoch 895/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6742 - val_loss: 47.8743\n",
      "Epoch 896/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6715 - val_loss: 47.8692\n",
      "Epoch 897/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.6705 - val_loss: 47.8693\n",
      "Epoch 898/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6605 - val_loss: 47.8686\n",
      "Epoch 899/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6550 - val_loss: 47.8626\n",
      "Epoch 900/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.6527 - val_loss: 47.8557\n",
      "Epoch 901/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6512 - val_loss: 47.8488\n",
      "Epoch 902/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6457 - val_loss: 47.8438\n",
      "Epoch 903/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6404 - val_loss: 47.8470\n",
      "Epoch 904/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6373 - val_loss: 47.8454\n",
      "Epoch 905/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.6278 - val_loss: 47.8374\n",
      "Epoch 906/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6280 - val_loss: 47.8480\n",
      "Epoch 907/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.6253 - val_loss: 47.8407\n",
      "Epoch 908/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.6167 - val_loss: 47.8423\n",
      "Epoch 909/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6139 - val_loss: 47.8378\n",
      "Epoch 910/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6088 - val_loss: 47.8405\n",
      "Epoch 911/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6049 - val_loss: 47.8372\n",
      "Epoch 912/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.6040 - val_loss: 47.8300\n",
      "Epoch 913/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5965 - val_loss: 47.8176\n",
      "Epoch 914/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5924 - val_loss: 47.8120\n",
      "Epoch 915/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5875 - val_loss: 47.8090\n",
      "Epoch 916/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.5870 - val_loss: 47.8025\n",
      "Epoch 917/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.5821 - val_loss: 47.7923\n",
      "Epoch 918/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5750 - val_loss: 47.7893\n",
      "Epoch 919/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.5711 - val_loss: 47.7858\n",
      "Epoch 920/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5659 - val_loss: 47.7804\n",
      "Epoch 921/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5638 - val_loss: 47.7789\n",
      "Epoch 922/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5618 - val_loss: 47.7789\n",
      "Epoch 923/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.5538 - val_loss: 47.7743\n",
      "Epoch 924/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.5485 - val_loss: 47.7724\n",
      "Epoch 925/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5466 - val_loss: 47.7681\n",
      "Epoch 926/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5411 - val_loss: 47.7657\n",
      "Epoch 927/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5376 - val_loss: 47.7649\n",
      "Epoch 928/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.5343 - val_loss: 47.7553\n",
      "Epoch 929/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.5282 - val_loss: 47.7526\n",
      "Epoch 930/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5237 - val_loss: 47.7550\n",
      "Epoch 931/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.5227 - val_loss: 47.7601\n",
      "Epoch 932/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5164 - val_loss: 47.7621\n",
      "Epoch 933/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5126 - val_loss: 47.7573\n",
      "Epoch 934/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.5076 - val_loss: 47.7603\n",
      "Epoch 935/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.5037 - val_loss: 47.7521\n",
      "Epoch 936/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4977 - val_loss: 47.7571\n",
      "Epoch 937/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4939 - val_loss: 47.7518\n",
      "Epoch 938/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4905 - val_loss: 47.7444\n",
      "Epoch 939/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4870 - val_loss: 47.7426\n",
      "Epoch 940/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.4807 - val_loss: 47.7356\n",
      "Epoch 941/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4772 - val_loss: 47.7320\n",
      "Epoch 942/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.4737 - val_loss: 47.7368\n",
      "Epoch 943/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4673 - val_loss: 47.7351\n",
      "Epoch 944/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4661 - val_loss: 47.7272\n",
      "Epoch 945/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4609 - val_loss: 47.7275\n",
      "Epoch 946/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4530 - val_loss: 47.7280\n",
      "Epoch 947/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4505 - val_loss: 47.7227\n",
      "Epoch 948/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.4484 - val_loss: 47.7119\n",
      "Epoch 949/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.4435 - val_loss: 47.7078\n",
      "Epoch 950/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4396 - val_loss: 47.7018\n",
      "Epoch 951/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4322 - val_loss: 47.6996\n",
      "Epoch 952/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.4299 - val_loss: 47.6996\n",
      "Epoch 953/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4251 - val_loss: 47.7005\n",
      "Epoch 954/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4205 - val_loss: 47.6924\n",
      "Epoch 955/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4172 - val_loss: 47.6890\n",
      "Epoch 956/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4133 - val_loss: 47.6858\n",
      "Epoch 957/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.4128 - val_loss: 47.6823\n",
      "Epoch 958/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.4057 - val_loss: 47.6823\n",
      "Epoch 959/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.4006 - val_loss: 47.6774\n",
      "Epoch 960/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3995 - val_loss: 47.6825\n",
      "Epoch 961/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3910 - val_loss: 47.6836\n",
      "Epoch 962/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3904 - val_loss: 47.6804\n",
      "Epoch 963/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3850 - val_loss: 47.6796\n",
      "Epoch 964/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3785 - val_loss: 47.6786\n",
      "Epoch 965/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3791 - val_loss: 47.6774\n",
      "Epoch 966/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3722 - val_loss: 47.6718\n",
      "Epoch 967/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3704 - val_loss: 47.6670\n",
      "Epoch 968/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.3642 - val_loss: 47.6601\n",
      "Epoch 969/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3590 - val_loss: 47.6525\n",
      "Epoch 970/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.3585 - val_loss: 47.6525\n",
      "Epoch 971/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.3494 - val_loss: 47.6437\n",
      "Epoch 972/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.3484 - val_loss: 47.6412\n",
      "Epoch 973/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.3446 - val_loss: 47.6385\n",
      "Epoch 974/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3450 - val_loss: 47.6354\n",
      "Epoch 975/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3372 - val_loss: 47.6361\n",
      "Epoch 976/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3306 - val_loss: 47.6371\n",
      "Epoch 977/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3287 - val_loss: 47.6352\n",
      "Epoch 978/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.3198 - val_loss: 47.6326\n",
      "Epoch 979/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3170 - val_loss: 47.6266\n",
      "Epoch 980/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3116 - val_loss: 47.6214\n",
      "Epoch 981/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3113 - val_loss: 47.6201\n",
      "Epoch 982/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.3029 - val_loss: 47.6193\n",
      "Epoch 983/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.3024 - val_loss: 47.6170\n",
      "Epoch 984/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2964 - val_loss: 47.6157\n",
      "Epoch 985/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2914 - val_loss: 47.6032\n",
      "Epoch 986/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2901 - val_loss: 47.6019\n",
      "Epoch 987/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.2847 - val_loss: 47.5947\n",
      "Epoch 988/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.2810 - val_loss: 47.6043\n",
      "Epoch 989/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2768 - val_loss: 47.6023\n",
      "Epoch 990/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2714 - val_loss: 47.6017\n",
      "Epoch 991/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2668 - val_loss: 47.5957\n",
      "Epoch 992/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2650 - val_loss: 47.5876\n",
      "Epoch 993/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2624 - val_loss: 47.5827\n",
      "Epoch 994/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2560 - val_loss: 47.5784\n",
      "Epoch 995/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2534 - val_loss: 47.5815\n",
      "Epoch 996/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2465 - val_loss: 47.5834\n",
      "Epoch 997/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2442 - val_loss: 47.5802\n",
      "Epoch 998/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2384 - val_loss: 47.5800\n",
      "Epoch 999/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.2372 - val_loss: 47.5735\n",
      "Epoch 1000/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.2309 - val_loss: 47.5673\n",
      "Epoch 1001/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2277 - val_loss: 47.5618\n",
      "Epoch 1002/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2229 - val_loss: 47.5605\n",
      "Epoch 1003/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.2213 - val_loss: 47.5494\n",
      "Epoch 1004/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2138 - val_loss: 47.5446\n",
      "Epoch 1005/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.2115 - val_loss: 47.5469\n",
      "Epoch 1006/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 51.2083 - val_loss: 47.5381\n",
      "Epoch 1007/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.2027 - val_loss: 47.5293\n",
      "Epoch 1008/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1984 - val_loss: 47.5222\n",
      "Epoch 1009/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.1959 - val_loss: 47.5246\n",
      "Epoch 1010/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.1949 - val_loss: 47.5198\n",
      "Epoch 1011/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.1883 - val_loss: 47.5172\n",
      "Epoch 1012/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.1827 - val_loss: 47.5132\n",
      "Epoch 1013/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1841 - val_loss: 47.5062\n",
      "Epoch 1014/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1740 - val_loss: 47.5026\n",
      "Epoch 1015/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1706 - val_loss: 47.5002\n",
      "Epoch 1016/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1727 - val_loss: 47.4970\n",
      "Epoch 1017/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1624 - val_loss: 47.4940\n",
      "Epoch 1018/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1590 - val_loss: 47.4888\n",
      "Epoch 1019/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 51.1544 - val_loss: 47.4883\n",
      "Epoch 1020/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1547 - val_loss: 47.4916\n",
      "Epoch 1021/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.1460 - val_loss: 47.4870\n",
      "Epoch 1022/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1427 - val_loss: 47.4925\n",
      "Epoch 1023/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.1399 - val_loss: 47.4979\n",
      "Epoch 1024/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 51.1357 - val_loss: 47.4931\n",
      "Epoch 1025/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.1322 - val_loss: 47.4821\n",
      "Epoch 1026/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.1299 - val_loss: 47.4755\n",
      "Epoch 1027/5000\n",
      "1126/1126 [==============================] - 0s 44us/step - loss: 51.1224 - val_loss: 47.4729\n",
      "Epoch 1028/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.1177 - val_loss: 47.4705\n",
      "Epoch 1029/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 51.1126 - val_loss: 47.4728\n",
      "Epoch 1030/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.1100 - val_loss: 47.4631\n",
      "Epoch 1031/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.1089 - val_loss: 47.4607\n",
      "Epoch 1032/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.1057 - val_loss: 47.4595\n",
      "Epoch 1033/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.1045 - val_loss: 47.4521\n",
      "Epoch 1034/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.0957 - val_loss: 47.4506\n",
      "Epoch 1035/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0917 - val_loss: 47.4424\n",
      "Epoch 1036/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0899 - val_loss: 47.4414\n",
      "Epoch 1037/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0814 - val_loss: 47.4397\n",
      "Epoch 1038/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.0774 - val_loss: 47.4374\n",
      "Epoch 1039/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0745 - val_loss: 47.4329\n",
      "Epoch 1040/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0703 - val_loss: 47.4368\n",
      "Epoch 1041/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0690 - val_loss: 47.4365\n",
      "Epoch 1042/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.0639 - val_loss: 47.4282\n",
      "Epoch 1043/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.0577 - val_loss: 47.4294\n",
      "Epoch 1044/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.0538 - val_loss: 47.4273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 51.0519 - val_loss: 47.4210\n",
      "Epoch 1046/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.0460 - val_loss: 47.4210\n",
      "Epoch 1047/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0404 - val_loss: 47.4270\n",
      "Epoch 1048/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0411 - val_loss: 47.4196\n",
      "Epoch 1049/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0334 - val_loss: 47.4126\n",
      "Epoch 1050/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0335 - val_loss: 47.4082\n",
      "Epoch 1051/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 51.0271 - val_loss: 47.4102\n",
      "Epoch 1052/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0202 - val_loss: 47.4040\n",
      "Epoch 1053/5000\n",
      "1126/1126 [==============================] - 0s 48us/step - loss: 51.0197 - val_loss: 47.3976\n",
      "Epoch 1054/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.0162 - val_loss: 47.3952\n",
      "Epoch 1055/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.0106 - val_loss: 47.3924\n",
      "Epoch 1056/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 51.0085 - val_loss: 47.3909\n",
      "Epoch 1057/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0009 - val_loss: 47.3873\n",
      "Epoch 1058/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.9981 - val_loss: 47.3828\n",
      "Epoch 1059/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.9955 - val_loss: 47.3825\n",
      "Epoch 1060/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.9883 - val_loss: 47.3820\n",
      "Epoch 1061/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.9861 - val_loss: 47.3783\n",
      "Epoch 1062/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.9839 - val_loss: 47.3821\n",
      "Epoch 1063/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.9807 - val_loss: 47.3701\n",
      "Epoch 1064/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.9746 - val_loss: 47.3694\n",
      "Epoch 1065/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.9734 - val_loss: 47.3748\n",
      "Epoch 1066/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.9685 - val_loss: 47.3776\n",
      "Epoch 1067/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.9628 - val_loss: 47.3792\n",
      "Epoch 1068/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.9595 - val_loss: 47.3737\n",
      "Epoch 1069/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.9603 - val_loss: 47.3628\n",
      "Epoch 1070/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.9511 - val_loss: 47.3631\n",
      "Epoch 1071/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.9499 - val_loss: 47.3646\n",
      "Epoch 1072/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.9457 - val_loss: 47.3570\n",
      "Epoch 1073/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.9427 - val_loss: 47.3523\n",
      "Epoch 1074/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.9367 - val_loss: 47.3472\n",
      "Epoch 1075/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.9334 - val_loss: 47.3430\n",
      "Epoch 1076/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 50.9289 - val_loss: 47.3380\n",
      "Epoch 1077/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.9258 - val_loss: 47.3384\n",
      "Epoch 1078/5000\n",
      "1126/1126 [==============================] - 0s 43us/step - loss: 50.9235 - val_loss: 47.3308\n",
      "Epoch 1079/5000\n",
      "1126/1126 [==============================] - 0s 44us/step - loss: 50.9186 - val_loss: 47.3270\n",
      "Epoch 1080/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 50.9159 - val_loss: 47.3143\n",
      "Epoch 1081/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.9147 - val_loss: 47.3138\n",
      "Epoch 1082/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.9124 - val_loss: 47.3086\n",
      "Epoch 1083/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.9022 - val_loss: 47.3065\n",
      "Epoch 1084/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.9012 - val_loss: 47.3022\n",
      "Epoch 1085/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8949 - val_loss: 47.3002\n",
      "Epoch 1086/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.8909 - val_loss: 47.2961\n",
      "Epoch 1087/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8852 - val_loss: 47.2961\n",
      "Epoch 1088/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8850 - val_loss: 47.2928\n",
      "Epoch 1089/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8799 - val_loss: 47.2884\n",
      "Epoch 1090/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8757 - val_loss: 47.2875\n",
      "Epoch 1091/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8732 - val_loss: 47.2827\n",
      "Epoch 1092/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8685 - val_loss: 47.2760\n",
      "Epoch 1093/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8664 - val_loss: 47.2737\n",
      "Epoch 1094/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8594 - val_loss: 47.2779\n",
      "Epoch 1095/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.8586 - val_loss: 47.2661\n",
      "Epoch 1096/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8560 - val_loss: 47.2630\n",
      "Epoch 1097/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8510 - val_loss: 47.2592\n",
      "Epoch 1098/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8435 - val_loss: 47.2567\n",
      "Epoch 1099/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.8424 - val_loss: 47.2487\n",
      "Epoch 1100/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.8370 - val_loss: 47.2531\n",
      "Epoch 1101/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8360 - val_loss: 47.2492\n",
      "Epoch 1102/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8328 - val_loss: 47.2499\n",
      "Epoch 1103/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8289 - val_loss: 47.2457\n",
      "Epoch 1104/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8216 - val_loss: 47.2440\n",
      "Epoch 1105/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8210 - val_loss: 47.2449\n",
      "Epoch 1106/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.8186 - val_loss: 47.2342\n",
      "Epoch 1107/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8152 - val_loss: 47.2365\n",
      "Epoch 1108/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8089 - val_loss: 47.2353\n",
      "Epoch 1109/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8066 - val_loss: 47.2298\n",
      "Epoch 1110/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.8031 - val_loss: 47.2243\n",
      "Epoch 1111/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.8005 - val_loss: 47.2207\n",
      "Epoch 1112/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7947 - val_loss: 47.2169\n",
      "Epoch 1113/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7917 - val_loss: 47.2169\n",
      "Epoch 1114/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7889 - val_loss: 47.2155\n",
      "Epoch 1115/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.7852 - val_loss: 47.2169\n",
      "Epoch 1116/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.7822 - val_loss: 47.2104\n",
      "Epoch 1117/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7749 - val_loss: 47.2070\n",
      "Epoch 1118/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7732 - val_loss: 47.2004\n",
      "Epoch 1119/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7691 - val_loss: 47.1922\n",
      "Epoch 1120/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7673 - val_loss: 47.1915\n",
      "Epoch 1121/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.7604 - val_loss: 47.1807\n",
      "Epoch 1122/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7575 - val_loss: 47.1793\n",
      "Epoch 1123/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.7549 - val_loss: 47.1868\n",
      "Epoch 1124/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.7510 - val_loss: 47.1837\n",
      "Epoch 1125/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7459 - val_loss: 47.1907\n",
      "Epoch 1126/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.7417 - val_loss: 47.1908\n",
      "Epoch 1127/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7400 - val_loss: 47.1806\n",
      "Epoch 1128/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7349 - val_loss: 47.1820\n",
      "Epoch 1129/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.7335 - val_loss: 47.1843\n",
      "Epoch 1130/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7303 - val_loss: 47.1800\n",
      "Epoch 1131/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7270 - val_loss: 47.1761\n",
      "Epoch 1132/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7205 - val_loss: 47.1732\n",
      "Epoch 1133/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.7175 - val_loss: 47.1691\n",
      "Epoch 1134/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.7149 - val_loss: 47.1552\n",
      "Epoch 1135/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.7152 - val_loss: 47.1520\n",
      "Epoch 1136/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.7105 - val_loss: 47.1474\n",
      "Epoch 1137/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.7047 - val_loss: 47.1448\n",
      "Epoch 1138/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6997 - val_loss: 47.1489\n",
      "Epoch 1139/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6949 - val_loss: 47.1452\n",
      "Epoch 1140/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.6950 - val_loss: 47.1434\n",
      "Epoch 1141/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6887 - val_loss: 47.1429\n",
      "Epoch 1142/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6856 - val_loss: 47.1336\n",
      "Epoch 1143/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6837 - val_loss: 47.1380\n",
      "Epoch 1144/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6773 - val_loss: 47.1328\n",
      "Epoch 1145/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6757 - val_loss: 47.1303\n",
      "Epoch 1146/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6730 - val_loss: 47.1222\n",
      "Epoch 1147/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6694 - val_loss: 47.1139\n",
      "Epoch 1148/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6683 - val_loss: 47.1071\n",
      "Epoch 1149/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6615 - val_loss: 47.1021\n",
      "Epoch 1150/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6588 - val_loss: 47.1018\n",
      "Epoch 1151/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6562 - val_loss: 47.0992\n",
      "Epoch 1152/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6549 - val_loss: 47.0916\n",
      "Epoch 1153/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6503 - val_loss: 47.0846\n",
      "Epoch 1154/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6403 - val_loss: 47.0843\n",
      "Epoch 1155/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6395 - val_loss: 47.0830\n",
      "Epoch 1156/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6366 - val_loss: 47.0776\n",
      "Epoch 1157/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 50.6380 - val_loss: 47.0727\n",
      "Epoch 1158/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.6300 - val_loss: 47.0645\n",
      "Epoch 1159/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6290 - val_loss: 47.0600\n",
      "Epoch 1160/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.6240 - val_loss: 47.0527\n",
      "Epoch 1161/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.6215 - val_loss: 47.0545\n",
      "Epoch 1162/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.6224 - val_loss: 47.0499\n",
      "Epoch 1163/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6171 - val_loss: 47.0498\n",
      "Epoch 1164/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.6135 - val_loss: 47.0501\n",
      "Epoch 1165/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.6103 - val_loss: 47.0532\n",
      "Epoch 1166/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6073 - val_loss: 47.0523\n",
      "Epoch 1167/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6041 - val_loss: 47.0457\n",
      "Epoch 1168/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.6041 - val_loss: 47.0483\n",
      "Epoch 1169/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.5939 - val_loss: 47.0505\n",
      "Epoch 1170/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5926 - val_loss: 47.0507\n",
      "Epoch 1171/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.5875 - val_loss: 47.0456\n",
      "Epoch 1172/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5855 - val_loss: 47.0403\n",
      "Epoch 1173/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5843 - val_loss: 47.0304\n",
      "Epoch 1174/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5782 - val_loss: 47.0248\n",
      "Epoch 1175/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5745 - val_loss: 47.0218\n",
      "Epoch 1176/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.5727 - val_loss: 47.0251\n",
      "Epoch 1177/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.5695 - val_loss: 47.0187\n",
      "Epoch 1178/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5654 - val_loss: 47.0130\n",
      "Epoch 1179/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5634 - val_loss: 47.0082\n",
      "Epoch 1180/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5598 - val_loss: 47.0037\n",
      "Epoch 1181/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5582 - val_loss: 47.0053\n",
      "Epoch 1182/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.5514 - val_loss: 47.0051\n",
      "Epoch 1183/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5498 - val_loss: 46.9986\n",
      "Epoch 1184/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5469 - val_loss: 47.0016\n",
      "Epoch 1185/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5434 - val_loss: 46.9951\n",
      "Epoch 1186/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5416 - val_loss: 46.9970\n",
      "Epoch 1187/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5386 - val_loss: 46.9945\n",
      "Epoch 1188/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.5354 - val_loss: 46.9956\n",
      "Epoch 1189/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5323 - val_loss: 46.9903\n",
      "Epoch 1190/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.5294 - val_loss: 46.9921\n",
      "Epoch 1191/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5260 - val_loss: 46.9912\n",
      "Epoch 1192/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.5206 - val_loss: 46.9885\n",
      "Epoch 1193/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5198 - val_loss: 46.9884\n",
      "Epoch 1194/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.5158 - val_loss: 46.9845\n",
      "Epoch 1195/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.5131 - val_loss: 46.9751\n",
      "Epoch 1196/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5131 - val_loss: 46.9726\n",
      "Epoch 1197/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5061 - val_loss: 46.9753\n",
      "Epoch 1198/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.5023 - val_loss: 46.9739\n",
      "Epoch 1199/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4998 - val_loss: 46.9718\n",
      "Epoch 1200/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4973 - val_loss: 46.9759\n",
      "Epoch 1201/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4962 - val_loss: 46.9734\n",
      "Epoch 1202/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4912 - val_loss: 46.9697\n",
      "Epoch 1203/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.4878 - val_loss: 46.9686\n",
      "Epoch 1204/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.4843 - val_loss: 46.9631\n",
      "Epoch 1205/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4815 - val_loss: 46.9637\n",
      "Epoch 1206/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.4804 - val_loss: 46.9576\n",
      "Epoch 1207/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4761 - val_loss: 46.9617\n",
      "Epoch 1208/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4729 - val_loss: 46.9602\n",
      "Epoch 1209/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4718 - val_loss: 46.9591\n",
      "Epoch 1210/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4687 - val_loss: 46.9565\n",
      "Epoch 1211/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.4622 - val_loss: 46.9582\n",
      "Epoch 1212/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.4619 - val_loss: 46.9539\n",
      "Epoch 1213/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.4561 - val_loss: 46.9484\n",
      "Epoch 1214/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.4556 - val_loss: 46.9481\n",
      "Epoch 1215/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 50.4530 - val_loss: 46.9452\n",
      "Epoch 1216/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 50.4449 - val_loss: 46.9396\n",
      "Epoch 1217/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.4424 - val_loss: 46.9368\n",
      "Epoch 1218/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.4434 - val_loss: 46.9327\n",
      "Epoch 1219/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.4369 - val_loss: 46.9397\n",
      "Epoch 1220/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.4343 - val_loss: 46.9358\n",
      "Epoch 1221/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.4308 - val_loss: 46.9346\n",
      "Epoch 1222/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.4264 - val_loss: 46.9335\n",
      "Epoch 1223/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.4253 - val_loss: 46.9273\n",
      "Epoch 1224/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.4256 - val_loss: 46.9241\n",
      "Epoch 1225/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.4195 - val_loss: 46.9229\n",
      "Epoch 1226/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.4134 - val_loss: 46.9237\n",
      "Epoch 1227/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.4149 - val_loss: 46.9193\n",
      "Epoch 1228/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.4103 - val_loss: 46.9205\n",
      "Epoch 1229/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.4052 - val_loss: 46.9156\n",
      "Epoch 1230/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.4052 - val_loss: 46.9175\n",
      "Epoch 1231/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3998 - val_loss: 46.9073\n",
      "Epoch 1232/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 50.3985 - val_loss: 46.9030\n",
      "Epoch 1233/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3929 - val_loss: 46.9048\n",
      "Epoch 1234/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.3914 - val_loss: 46.8950\n",
      "Epoch 1235/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.3882 - val_loss: 46.8913\n",
      "Epoch 1236/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3836 - val_loss: 46.8898\n",
      "Epoch 1237/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3808 - val_loss: 46.8850\n",
      "Epoch 1238/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.3786 - val_loss: 46.8817\n",
      "Epoch 1239/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3732 - val_loss: 46.8790\n",
      "Epoch 1240/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3711 - val_loss: 46.8757\n",
      "Epoch 1241/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.3685 - val_loss: 46.8726\n",
      "Epoch 1242/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.3660 - val_loss: 46.8696\n",
      "Epoch 1243/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.3624 - val_loss: 46.8658\n",
      "Epoch 1244/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.3608 - val_loss: 46.8600\n",
      "Epoch 1245/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3591 - val_loss: 46.8590\n",
      "Epoch 1246/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3550 - val_loss: 46.8531\n",
      "Epoch 1247/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3487 - val_loss: 46.8481\n",
      "Epoch 1248/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 50.3455 - val_loss: 46.8484\n",
      "Epoch 1249/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.3452 - val_loss: 46.8499\n",
      "Epoch 1250/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3442 - val_loss: 46.8475\n",
      "Epoch 1251/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.3397 - val_loss: 46.8479\n",
      "Epoch 1252/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3354 - val_loss: 46.8461\n",
      "Epoch 1253/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3355 - val_loss: 46.8471\n",
      "Epoch 1254/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.3272 - val_loss: 46.8436\n",
      "Epoch 1255/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.3267 - val_loss: 46.8383\n",
      "Epoch 1256/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.3242 - val_loss: 46.8362\n",
      "Epoch 1257/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.3192 - val_loss: 46.8343\n",
      "Epoch 1258/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.3176 - val_loss: 46.8314\n",
      "Epoch 1259/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3158 - val_loss: 46.8305\n",
      "Epoch 1260/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3127 - val_loss: 46.8249\n",
      "Epoch 1261/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3104 - val_loss: 46.8224\n",
      "Epoch 1262/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.3078 - val_loss: 46.8212\n",
      "Epoch 1263/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.3020 - val_loss: 46.8183\n",
      "Epoch 1264/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.2993 - val_loss: 46.8212\n",
      "Epoch 1265/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2962 - val_loss: 46.8242\n",
      "Epoch 1266/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2967 - val_loss: 46.8206\n",
      "Epoch 1267/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2947 - val_loss: 46.8151\n",
      "Epoch 1268/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.2912 - val_loss: 46.8135\n",
      "Epoch 1269/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2860 - val_loss: 46.8151\n",
      "Epoch 1270/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2831 - val_loss: 46.8130\n",
      "Epoch 1271/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2806 - val_loss: 46.8025\n",
      "Epoch 1272/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2771 - val_loss: 46.8037\n",
      "Epoch 1273/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2753 - val_loss: 46.7952\n",
      "Epoch 1274/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2748 - val_loss: 46.7937\n",
      "Epoch 1275/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.2741 - val_loss: 46.7881\n",
      "Epoch 1276/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.2667 - val_loss: 46.7827\n",
      "Epoch 1277/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2620 - val_loss: 46.7711\n",
      "Epoch 1278/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2627 - val_loss: 46.7700\n",
      "Epoch 1279/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2577 - val_loss: 46.7717\n",
      "Epoch 1280/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2569 - val_loss: 46.7713\n",
      "Epoch 1281/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2536 - val_loss: 46.7738\n",
      "Epoch 1282/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2495 - val_loss: 46.7743\n",
      "Epoch 1283/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.2453 - val_loss: 46.7753\n",
      "Epoch 1284/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2428 - val_loss: 46.7708\n",
      "Epoch 1285/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2396 - val_loss: 46.7705\n",
      "Epoch 1286/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2399 - val_loss: 46.7721\n",
      "Epoch 1287/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2333 - val_loss: 46.7720\n",
      "Epoch 1288/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2308 - val_loss: 46.7692\n",
      "Epoch 1289/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2304 - val_loss: 46.7641\n",
      "Epoch 1290/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2255 - val_loss: 46.7601\n",
      "Epoch 1291/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.2231 - val_loss: 46.7571\n",
      "Epoch 1292/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2213 - val_loss: 46.7514\n",
      "Epoch 1293/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2161 - val_loss: 46.7506\n",
      "Epoch 1294/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.2153 - val_loss: 46.7466\n",
      "Epoch 1295/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.2104 - val_loss: 46.7426\n",
      "Epoch 1296/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.2097 - val_loss: 46.7409\n",
      "Epoch 1297/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.2059 - val_loss: 46.7451\n",
      "Epoch 1298/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2012 - val_loss: 46.7420\n",
      "Epoch 1299/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1993 - val_loss: 46.7452\n",
      "Epoch 1300/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1968 - val_loss: 46.7386\n",
      "Epoch 1301/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1928 - val_loss: 46.7353\n",
      "Epoch 1302/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.1914 - val_loss: 46.7320\n",
      "Epoch 1303/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1860 - val_loss: 46.7271\n",
      "Epoch 1304/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1843 - val_loss: 46.7207\n",
      "Epoch 1305/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1839 - val_loss: 46.7203\n",
      "Epoch 1306/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1786 - val_loss: 46.7144\n",
      "Epoch 1307/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1776 - val_loss: 46.7098\n",
      "Epoch 1308/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1736 - val_loss: 46.7058\n",
      "Epoch 1309/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1710 - val_loss: 46.7049\n",
      "Epoch 1310/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1689 - val_loss: 46.6974\n",
      "Epoch 1311/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1650 - val_loss: 46.6888\n",
      "Epoch 1312/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1612 - val_loss: 46.6846\n",
      "Epoch 1313/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1575 - val_loss: 46.6816\n",
      "Epoch 1314/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1548 - val_loss: 46.6817\n",
      "Epoch 1315/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1531 - val_loss: 46.6797\n",
      "Epoch 1316/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1508 - val_loss: 46.6823\n",
      "Epoch 1317/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.1471 - val_loss: 46.6772\n",
      "Epoch 1318/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1445 - val_loss: 46.6740\n",
      "Epoch 1319/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1433 - val_loss: 46.6744\n",
      "Epoch 1320/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1426 - val_loss: 46.6862\n",
      "Epoch 1321/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.1360 - val_loss: 46.6814\n",
      "Epoch 1322/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.1334 - val_loss: 46.6767\n",
      "Epoch 1323/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.1284 - val_loss: 46.6737\n",
      "Epoch 1324/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1305 - val_loss: 46.6707\n",
      "Epoch 1325/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1232 - val_loss: 46.6724\n",
      "Epoch 1326/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1220 - val_loss: 46.6668\n",
      "Epoch 1327/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1192 - val_loss: 46.6632\n",
      "Epoch 1328/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1141 - val_loss: 46.6583\n",
      "Epoch 1329/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.1126 - val_loss: 46.6494\n",
      "Epoch 1330/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1090 - val_loss: 46.6476\n",
      "Epoch 1331/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.1055 - val_loss: 46.6449\n",
      "Epoch 1332/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1080 - val_loss: 46.6419\n",
      "Epoch 1333/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.1021 - val_loss: 46.6391\n",
      "Epoch 1334/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0973 - val_loss: 46.6411\n",
      "Epoch 1335/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0943 - val_loss: 46.6357\n",
      "Epoch 1336/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0942 - val_loss: 46.6328\n",
      "Epoch 1337/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0903 - val_loss: 46.6284\n",
      "Epoch 1338/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0873 - val_loss: 46.6326\n",
      "Epoch 1339/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0874 - val_loss: 46.6248\n",
      "Epoch 1340/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0831 - val_loss: 46.6217\n",
      "Epoch 1341/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0820 - val_loss: 46.6228\n",
      "Epoch 1342/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0748 - val_loss: 46.6217\n",
      "Epoch 1343/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0753 - val_loss: 46.6199\n",
      "Epoch 1344/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0684 - val_loss: 46.6192\n",
      "Epoch 1345/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0657 - val_loss: 46.6183\n",
      "Epoch 1346/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0631 - val_loss: 46.6144\n",
      "Epoch 1347/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0609 - val_loss: 46.6160\n",
      "Epoch 1348/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0652 - val_loss: 46.6119\n",
      "Epoch 1349/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0542 - val_loss: 46.6105\n",
      "Epoch 1350/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.0536 - val_loss: 46.6129\n",
      "Epoch 1351/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0482 - val_loss: 46.6115\n",
      "Epoch 1352/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0447 - val_loss: 46.6092\n",
      "Epoch 1353/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.0469 - val_loss: 46.6017\n",
      "Epoch 1354/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0439 - val_loss: 46.5954\n",
      "Epoch 1355/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.0415 - val_loss: 46.5957\n",
      "Epoch 1356/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0395 - val_loss: 46.5920\n",
      "Epoch 1357/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0330 - val_loss: 46.5996\n",
      "Epoch 1358/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 50.0294 - val_loss: 46.5999\n",
      "Epoch 1359/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0303 - val_loss: 46.5963\n",
      "Epoch 1360/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0244 - val_loss: 46.5975\n",
      "Epoch 1361/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.0227 - val_loss: 46.6013\n",
      "Epoch 1362/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0239 - val_loss: 46.6010\n",
      "Epoch 1363/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0183 - val_loss: 46.5958\n",
      "Epoch 1364/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0156 - val_loss: 46.5900\n",
      "Epoch 1365/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0088 - val_loss: 46.5845\n",
      "Epoch 1366/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.0083 - val_loss: 46.5840\n",
      "Epoch 1367/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0060 - val_loss: 46.5814\n",
      "Epoch 1368/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0012 - val_loss: 46.5757\n",
      "Epoch 1369/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9998 - val_loss: 46.5743\n",
      "Epoch 1370/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9985 - val_loss: 46.5730\n",
      "Epoch 1371/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.9937 - val_loss: 46.5731\n",
      "Epoch 1372/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.9974 - val_loss: 46.5724\n",
      "Epoch 1373/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.9902 - val_loss: 46.5773\n",
      "Epoch 1374/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9864 - val_loss: 46.5723\n",
      "Epoch 1375/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9849 - val_loss: 46.5700\n",
      "Epoch 1376/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.9799 - val_loss: 46.5637\n",
      "Epoch 1377/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.9785 - val_loss: 46.5582\n",
      "Epoch 1378/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.9734 - val_loss: 46.5525\n",
      "Epoch 1379/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9740 - val_loss: 46.5483\n",
      "Epoch 1380/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9686 - val_loss: 46.5450\n",
      "Epoch 1381/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 49.9648 - val_loss: 46.5443\n",
      "Epoch 1382/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.9630 - val_loss: 46.5399\n",
      "Epoch 1383/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9623 - val_loss: 46.5364\n",
      "Epoch 1384/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9605 - val_loss: 46.5411\n",
      "Epoch 1385/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9540 - val_loss: 46.5365\n",
      "Epoch 1386/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9547 - val_loss: 46.5302\n",
      "Epoch 1387/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9494 - val_loss: 46.5267\n",
      "Epoch 1388/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9505 - val_loss: 46.5214\n",
      "Epoch 1389/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9465 - val_loss: 46.5180\n",
      "Epoch 1390/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.9390 - val_loss: 46.5148\n",
      "Epoch 1391/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9387 - val_loss: 46.5160\n",
      "Epoch 1392/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9365 - val_loss: 46.5133\n",
      "Epoch 1393/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9322 - val_loss: 46.5114\n",
      "Epoch 1394/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9316 - val_loss: 46.5127\n",
      "Epoch 1395/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9277 - val_loss: 46.5153\n",
      "Epoch 1396/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9291 - val_loss: 46.5119\n",
      "Epoch 1397/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9216 - val_loss: 46.5122\n",
      "Epoch 1398/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9190 - val_loss: 46.5125\n",
      "Epoch 1399/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9164 - val_loss: 46.5123\n",
      "Epoch 1400/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9148 - val_loss: 46.5092\n",
      "Epoch 1401/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9103 - val_loss: 46.5108\n",
      "Epoch 1402/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9089 - val_loss: 46.5014\n",
      "Epoch 1403/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9060 - val_loss: 46.5011\n",
      "Epoch 1404/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.9041 - val_loss: 46.5070\n",
      "Epoch 1405/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 49.8994 - val_loss: 46.5135\n",
      "Epoch 1406/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8981 - val_loss: 46.5091\n",
      "Epoch 1407/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.8919 - val_loss: 46.5094\n",
      "Epoch 1408/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8908 - val_loss: 46.5123\n",
      "Epoch 1409/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8900 - val_loss: 46.5071\n",
      "Epoch 1410/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8844 - val_loss: 46.5015\n",
      "Epoch 1411/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8845 - val_loss: 46.5007\n",
      "Epoch 1412/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8834 - val_loss: 46.4956\n",
      "Epoch 1413/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8814 - val_loss: 46.4913\n",
      "Epoch 1414/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8771 - val_loss: 46.4856\n",
      "Epoch 1415/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8739 - val_loss: 46.4909\n",
      "Epoch 1416/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8712 - val_loss: 46.4840\n",
      "Epoch 1417/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.8680 - val_loss: 46.4846\n",
      "Epoch 1418/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8670 - val_loss: 46.4760\n",
      "Epoch 1419/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8634 - val_loss: 46.4700\n",
      "Epoch 1420/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8603 - val_loss: 46.4609\n",
      "Epoch 1421/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8590 - val_loss: 46.4579\n",
      "Epoch 1422/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8568 - val_loss: 46.4595\n",
      "Epoch 1423/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8560 - val_loss: 46.4612\n",
      "Epoch 1424/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8495 - val_loss: 46.4543\n",
      "Epoch 1425/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8466 - val_loss: 46.4594\n",
      "Epoch 1426/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8438 - val_loss: 46.4546\n",
      "Epoch 1427/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8426 - val_loss: 46.4488\n",
      "Epoch 1428/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8407 - val_loss: 46.4488\n",
      "Epoch 1429/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8395 - val_loss: 46.4438\n",
      "Epoch 1430/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8379 - val_loss: 46.4362\n",
      "Epoch 1431/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.8329 - val_loss: 46.4375\n",
      "Epoch 1432/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.8281 - val_loss: 46.4333\n",
      "Epoch 1433/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8309 - val_loss: 46.4332\n",
      "Epoch 1434/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8262 - val_loss: 46.4289\n",
      "Epoch 1435/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8226 - val_loss: 46.4241\n",
      "Epoch 1436/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8204 - val_loss: 46.4246\n",
      "Epoch 1437/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8175 - val_loss: 46.4205\n",
      "Epoch 1438/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8205 - val_loss: 46.4217\n",
      "Epoch 1439/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.8148 - val_loss: 46.4210\n",
      "Epoch 1440/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8093 - val_loss: 46.4208\n",
      "Epoch 1441/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8063 - val_loss: 46.4200\n",
      "Epoch 1442/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8057 - val_loss: 46.4199\n",
      "Epoch 1443/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8054 - val_loss: 46.4190\n",
      "Epoch 1444/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.8011 - val_loss: 46.4161\n",
      "Epoch 1445/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.7956 - val_loss: 46.4180\n",
      "Epoch 1446/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7960 - val_loss: 46.4143\n",
      "Epoch 1447/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.7986 - val_loss: 46.4147\n",
      "Epoch 1448/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7911 - val_loss: 46.4195\n",
      "Epoch 1449/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7891 - val_loss: 46.4234\n",
      "Epoch 1450/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.7823 - val_loss: 46.4164\n",
      "Epoch 1451/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7815 - val_loss: 46.4173\n",
      "Epoch 1452/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7757 - val_loss: 46.4208\n",
      "Epoch 1453/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7778 - val_loss: 46.4152\n",
      "Epoch 1454/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.7757 - val_loss: 46.4117\n",
      "Epoch 1455/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7730 - val_loss: 46.4094\n",
      "Epoch 1456/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7720 - val_loss: 46.4071\n",
      "Epoch 1457/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7676 - val_loss: 46.4039\n",
      "Epoch 1458/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7623 - val_loss: 46.3992\n",
      "Epoch 1459/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7629 - val_loss: 46.3975\n",
      "Epoch 1460/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.7583 - val_loss: 46.3961\n",
      "Epoch 1461/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.7550 - val_loss: 46.4013\n",
      "Epoch 1462/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7548 - val_loss: 46.3965\n",
      "Epoch 1463/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.7512 - val_loss: 46.3940\n",
      "Epoch 1464/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.7502 - val_loss: 46.3906\n",
      "Epoch 1465/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 49.7469 - val_loss: 46.3887\n",
      "Epoch 1466/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.7455 - val_loss: 46.3848\n",
      "Epoch 1467/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7409 - val_loss: 46.3848\n",
      "Epoch 1468/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7396 - val_loss: 46.3832\n",
      "Epoch 1469/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.7393 - val_loss: 46.3736\n",
      "Epoch 1470/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.7334 - val_loss: 46.3713\n",
      "Epoch 1471/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7323 - val_loss: 46.3752\n",
      "Epoch 1472/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7288 - val_loss: 46.3744\n",
      "Epoch 1473/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7251 - val_loss: 46.3710\n",
      "Epoch 1474/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7233 - val_loss: 46.3665\n",
      "Epoch 1475/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7211 - val_loss: 46.3649\n",
      "Epoch 1476/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.7189 - val_loss: 46.3586\n",
      "Epoch 1477/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.7143 - val_loss: 46.3559\n",
      "Epoch 1478/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 49.7152 - val_loss: 46.3618\n",
      "Epoch 1479/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 49.7097 - val_loss: 46.3606\n",
      "Epoch 1480/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 49.7076 - val_loss: 46.3541\n",
      "Epoch 1481/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.7039 - val_loss: 46.3500\n",
      "Epoch 1482/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.7062 - val_loss: 46.3445\n",
      "Epoch 1483/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.7021 - val_loss: 46.3472\n",
      "Epoch 1484/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.6999 - val_loss: 46.3446\n",
      "Epoch 1485/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 49.6957 - val_loss: 46.3465\n",
      "Epoch 1486/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6915 - val_loss: 46.3434\n",
      "Epoch 1487/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6923 - val_loss: 46.3375\n",
      "Epoch 1488/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6884 - val_loss: 46.3411\n",
      "Epoch 1489/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6816 - val_loss: 46.3389\n",
      "Epoch 1490/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6820 - val_loss: 46.3310\n",
      "Epoch 1491/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6806 - val_loss: 46.3226\n",
      "Epoch 1492/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6773 - val_loss: 46.3190\n",
      "Epoch 1493/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6753 - val_loss: 46.3128\n",
      "Epoch 1494/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6732 - val_loss: 46.3120\n",
      "Epoch 1495/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6708 - val_loss: 46.3099\n",
      "Epoch 1496/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6672 - val_loss: 46.3064\n",
      "Epoch 1497/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6671 - val_loss: 46.3030\n",
      "Epoch 1498/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6610 - val_loss: 46.3029\n",
      "Epoch 1499/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.6613 - val_loss: 46.3014\n",
      "Epoch 1500/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6615 - val_loss: 46.3030\n",
      "Epoch 1501/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6560 - val_loss: 46.2966\n",
      "Epoch 1502/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6522 - val_loss: 46.2906\n",
      "Epoch 1503/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6474 - val_loss: 46.2897\n",
      "Epoch 1504/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6468 - val_loss: 46.2865\n",
      "Epoch 1505/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6455 - val_loss: 46.2807\n",
      "Epoch 1506/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6445 - val_loss: 46.2814\n",
      "Epoch 1507/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6413 - val_loss: 46.2785\n",
      "Epoch 1508/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.6415 - val_loss: 46.2848\n",
      "Epoch 1509/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6397 - val_loss: 46.2824\n",
      "Epoch 1510/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6323 - val_loss: 46.2826\n",
      "Epoch 1511/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6311 - val_loss: 46.2804\n",
      "Epoch 1512/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6244 - val_loss: 46.2726\n",
      "Epoch 1513/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 49.6236 - val_loss: 46.2660\n",
      "Epoch 1514/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6230 - val_loss: 46.2647\n",
      "Epoch 1515/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.6204 - val_loss: 46.2679\n",
      "Epoch 1516/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.6176 - val_loss: 46.2638\n",
      "Epoch 1517/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.6173 - val_loss: 46.2599\n",
      "Epoch 1518/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 49.6120 - val_loss: 46.2547\n",
      "Epoch 1519/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 49.6093 - val_loss: 46.2562\n",
      "Epoch 1520/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.6119 - val_loss: 46.2544\n",
      "Epoch 1521/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6028 - val_loss: 46.2543\n",
      "Epoch 1522/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6010 - val_loss: 46.2589\n",
      "Epoch 1523/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6025 - val_loss: 46.2577\n",
      "Epoch 1524/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5970 - val_loss: 46.2516\n",
      "Epoch 1525/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5930 - val_loss: 46.2474\n",
      "Epoch 1526/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5915 - val_loss: 46.2402\n",
      "Epoch 1527/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5891 - val_loss: 46.2357\n",
      "Epoch 1528/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5844 - val_loss: 46.2408\n",
      "Epoch 1529/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5817 - val_loss: 46.2368\n",
      "Epoch 1530/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5831 - val_loss: 46.2365\n",
      "Epoch 1531/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5782 - val_loss: 46.2318\n",
      "Epoch 1532/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5782 - val_loss: 46.2216\n",
      "Epoch 1533/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5740 - val_loss: 46.2208\n",
      "Epoch 1534/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5727 - val_loss: 46.2195\n",
      "Epoch 1535/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5702 - val_loss: 46.2151\n",
      "Epoch 1536/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5678 - val_loss: 46.2130\n",
      "Epoch 1537/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5656 - val_loss: 46.2121\n",
      "Epoch 1538/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5619 - val_loss: 46.2094\n",
      "Epoch 1539/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.5606 - val_loss: 46.2079\n",
      "Epoch 1540/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5569 - val_loss: 46.2119\n",
      "Epoch 1541/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5567 - val_loss: 46.2090\n",
      "Epoch 1542/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5526 - val_loss: 46.2063\n",
      "Epoch 1543/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5508 - val_loss: 46.2167\n",
      "Epoch 1544/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5474 - val_loss: 46.2101\n",
      "Epoch 1545/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5446 - val_loss: 46.2048\n",
      "Epoch 1546/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5399 - val_loss: 46.2020\n",
      "Epoch 1547/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5410 - val_loss: 46.1989\n",
      "Epoch 1548/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5375 - val_loss: 46.2022\n",
      "Epoch 1549/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5355 - val_loss: 46.1954\n",
      "Epoch 1550/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.5338 - val_loss: 46.1957\n",
      "Epoch 1551/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.5287 - val_loss: 46.2006\n",
      "Epoch 1552/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.5306 - val_loss: 46.1912\n",
      "Epoch 1553/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.5263 - val_loss: 46.1868\n",
      "Epoch 1554/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.5232 - val_loss: 46.1832\n",
      "Epoch 1555/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.5236 - val_loss: 46.1768\n",
      "Epoch 1556/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.5210 - val_loss: 46.1740\n",
      "Epoch 1557/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.5155 - val_loss: 46.1763\n",
      "Epoch 1558/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.5117 - val_loss: 46.1725\n",
      "Epoch 1559/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.5106 - val_loss: 46.1710\n",
      "Epoch 1560/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.5063 - val_loss: 46.1755\n",
      "Epoch 1561/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5064 - val_loss: 46.1717\n",
      "Epoch 1562/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5021 - val_loss: 46.1718\n",
      "Epoch 1563/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.5007 - val_loss: 46.1681\n",
      "Epoch 1564/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4986 - val_loss: 46.1745\n",
      "Epoch 1565/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4953 - val_loss: 46.1734\n",
      "Epoch 1566/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4921 - val_loss: 46.1656\n",
      "Epoch 1567/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4902 - val_loss: 46.1607\n",
      "Epoch 1568/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.4866 - val_loss: 46.1507\n",
      "Epoch 1569/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4876 - val_loss: 46.1464\n",
      "Epoch 1570/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4832 - val_loss: 46.1484\n",
      "Epoch 1571/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.4802 - val_loss: 46.1440\n",
      "Epoch 1572/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4781 - val_loss: 46.1460\n",
      "Epoch 1573/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.4761 - val_loss: 46.1408\n",
      "Epoch 1574/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4716 - val_loss: 46.1402\n",
      "Epoch 1575/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4746 - val_loss: 46.1414\n",
      "Epoch 1576/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4682 - val_loss: 46.1433\n",
      "Epoch 1577/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4642 - val_loss: 46.1424\n",
      "Epoch 1578/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4621 - val_loss: 46.1417\n",
      "Epoch 1579/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4602 - val_loss: 46.1365\n",
      "Epoch 1580/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4595 - val_loss: 46.1346\n",
      "Epoch 1581/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4578 - val_loss: 46.1313\n",
      "Epoch 1582/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4548 - val_loss: 46.1307\n",
      "Epoch 1583/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4526 - val_loss: 46.1255\n",
      "Epoch 1584/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4496 - val_loss: 46.1263\n",
      "Epoch 1585/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4472 - val_loss: 46.1313\n",
      "Epoch 1586/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4431 - val_loss: 46.1295\n",
      "Epoch 1587/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4409 - val_loss: 46.1225\n",
      "Epoch 1588/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4421 - val_loss: 46.1204\n",
      "Epoch 1589/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4376 - val_loss: 46.1224\n",
      "Epoch 1590/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4349 - val_loss: 46.1174\n",
      "Epoch 1591/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4347 - val_loss: 46.1082\n",
      "Epoch 1592/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4295 - val_loss: 46.1059\n",
      "Epoch 1593/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 49.4279 - val_loss: 46.1040\n",
      "Epoch 1594/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.4273 - val_loss: 46.1051\n",
      "Epoch 1595/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4247 - val_loss: 46.0987\n",
      "Epoch 1596/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4205 - val_loss: 46.0959\n",
      "Epoch 1597/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4210 - val_loss: 46.0928\n",
      "Epoch 1598/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.4186 - val_loss: 46.0920\n",
      "Epoch 1599/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4143 - val_loss: 46.0891\n",
      "Epoch 1600/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4097 - val_loss: 46.0895\n",
      "Epoch 1601/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4085 - val_loss: 46.0889\n",
      "Epoch 1602/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4060 - val_loss: 46.0847\n",
      "Epoch 1603/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.4050 - val_loss: 46.0830\n",
      "Epoch 1604/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.4007 - val_loss: 46.0807\n",
      "Epoch 1605/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3981 - val_loss: 46.0791\n",
      "Epoch 1606/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3968 - val_loss: 46.0777\n",
      "Epoch 1607/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3955 - val_loss: 46.0722\n",
      "Epoch 1608/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3912 - val_loss: 46.0687\n",
      "Epoch 1609/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3920 - val_loss: 46.0655\n",
      "Epoch 1610/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3877 - val_loss: 46.0614\n",
      "Epoch 1611/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3848 - val_loss: 46.0607\n",
      "Epoch 1612/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3842 - val_loss: 46.0633\n",
      "Epoch 1613/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3793 - val_loss: 46.0611\n",
      "Epoch 1614/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3772 - val_loss: 46.0616\n",
      "Epoch 1615/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3781 - val_loss: 46.0590\n",
      "Epoch 1616/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3723 - val_loss: 46.0603\n",
      "Epoch 1617/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3690 - val_loss: 46.0577\n",
      "Epoch 1618/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3678 - val_loss: 46.0558\n",
      "Epoch 1619/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.3644 - val_loss: 46.0572\n",
      "Epoch 1620/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 49.3627 - val_loss: 46.0556\n",
      "Epoch 1621/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 49.3591 - val_loss: 46.0535\n",
      "Epoch 1622/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.3563 - val_loss: 46.0494\n",
      "Epoch 1623/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.3563 - val_loss: 46.0460\n",
      "Epoch 1624/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3530 - val_loss: 46.0441\n",
      "Epoch 1625/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.3509 - val_loss: 46.0438\n",
      "Epoch 1626/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3499 - val_loss: 46.0401\n",
      "Epoch 1627/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3450 - val_loss: 46.0357\n",
      "Epoch 1628/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3435 - val_loss: 46.0342\n",
      "Epoch 1629/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3428 - val_loss: 46.0295\n",
      "Epoch 1630/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.3389 - val_loss: 46.0266\n",
      "Epoch 1631/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3365 - val_loss: 46.0281\n",
      "Epoch 1632/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.3338 - val_loss: 46.0290\n",
      "Epoch 1633/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.3316 - val_loss: 46.0274\n",
      "Epoch 1634/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.3288 - val_loss: 46.0239\n",
      "Epoch 1635/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.3285 - val_loss: 46.0250\n",
      "Epoch 1636/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.3245 - val_loss: 46.0229\n",
      "Epoch 1637/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.3225 - val_loss: 46.0231\n",
      "Epoch 1638/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3183 - val_loss: 46.0192\n",
      "Epoch 1639/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3162 - val_loss: 46.0210\n",
      "Epoch 1640/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3162 - val_loss: 46.0160\n",
      "Epoch 1641/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3120 - val_loss: 46.0139\n",
      "Epoch 1642/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3102 - val_loss: 46.0118\n",
      "Epoch 1643/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3071 - val_loss: 46.0139\n",
      "Epoch 1644/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3051 - val_loss: 46.0131\n",
      "Epoch 1645/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.3049 - val_loss: 46.0183\n",
      "Epoch 1646/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.3013 - val_loss: 46.0220\n",
      "Epoch 1647/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.3011 - val_loss: 46.0197\n",
      "Epoch 1648/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2955 - val_loss: 46.0166\n",
      "Epoch 1649/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2936 - val_loss: 46.0149\n",
      "Epoch 1650/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2896 - val_loss: 46.0174\n",
      "Epoch 1651/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2886 - val_loss: 46.0114\n",
      "Epoch 1652/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2879 - val_loss: 46.0091\n",
      "Epoch 1653/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2848 - val_loss: 46.0059\n",
      "Epoch 1654/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2827 - val_loss: 46.0003\n",
      "Epoch 1655/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2823 - val_loss: 45.9970\n",
      "Epoch 1656/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2776 - val_loss: 45.9929\n",
      "Epoch 1657/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2750 - val_loss: 45.9848\n",
      "Epoch 1658/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.2715 - val_loss: 45.9809\n",
      "Epoch 1659/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2695 - val_loss: 45.9756\n",
      "Epoch 1660/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2673 - val_loss: 45.9753\n",
      "Epoch 1661/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2655 - val_loss: 45.9704\n",
      "Epoch 1662/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2619 - val_loss: 45.9745\n",
      "Epoch 1663/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2619 - val_loss: 45.9677\n",
      "Epoch 1664/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2576 - val_loss: 45.9686\n",
      "Epoch 1665/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2545 - val_loss: 45.9696\n",
      "Epoch 1666/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2533 - val_loss: 45.9685\n",
      "Epoch 1667/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2522 - val_loss: 45.9663\n",
      "Epoch 1668/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2490 - val_loss: 45.9681\n",
      "Epoch 1669/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2470 - val_loss: 45.9637\n",
      "Epoch 1670/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2439 - val_loss: 45.9618\n",
      "Epoch 1671/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2471 - val_loss: 45.9555\n",
      "Epoch 1672/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2393 - val_loss: 45.9534\n",
      "Epoch 1673/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2355 - val_loss: 45.9548\n",
      "Epoch 1674/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2378 - val_loss: 45.9536\n",
      "Epoch 1675/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.2328 - val_loss: 45.9515\n",
      "Epoch 1676/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2290 - val_loss: 45.9473\n",
      "Epoch 1677/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2280 - val_loss: 45.9422\n",
      "Epoch 1678/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2246 - val_loss: 45.9401\n",
      "Epoch 1679/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.2264 - val_loss: 45.9408\n",
      "Epoch 1680/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2212 - val_loss: 45.9433\n",
      "Epoch 1681/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2207 - val_loss: 45.9429\n",
      "Epoch 1682/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2149 - val_loss: 45.9397\n",
      "Epoch 1683/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2143 - val_loss: 45.9415\n",
      "Epoch 1684/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2102 - val_loss: 45.9384\n",
      "Epoch 1685/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2093 - val_loss: 45.9352\n",
      "Epoch 1686/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2086 - val_loss: 45.9308\n",
      "Epoch 1687/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2033 - val_loss: 45.9351\n",
      "Epoch 1688/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2025 - val_loss: 45.9310\n",
      "Epoch 1689/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2006 - val_loss: 45.9371\n",
      "Epoch 1690/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.1963 - val_loss: 45.9358\n",
      "Epoch 1691/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1980 - val_loss: 45.9323\n",
      "Epoch 1692/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1917 - val_loss: 45.9322\n",
      "Epoch 1693/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.1910 - val_loss: 45.9281\n",
      "Epoch 1694/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 49.1877 - val_loss: 45.9275\n",
      "Epoch 1695/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 49.1846 - val_loss: 45.9237\n",
      "Epoch 1696/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.1859 - val_loss: 45.9227\n",
      "Epoch 1697/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1825 - val_loss: 45.9226\n",
      "Epoch 1698/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1803 - val_loss: 45.9213\n",
      "Epoch 1699/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1771 - val_loss: 45.9216\n",
      "Epoch 1700/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.1737 - val_loss: 45.9117\n",
      "Epoch 1701/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.1715 - val_loss: 45.9113\n",
      "Epoch 1702/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1751 - val_loss: 45.9138\n",
      "Epoch 1703/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1690 - val_loss: 45.9142\n",
      "Epoch 1704/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1687 - val_loss: 45.9096\n",
      "Epoch 1705/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1636 - val_loss: 45.9087\n",
      "Epoch 1706/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1635 - val_loss: 45.9101\n",
      "Epoch 1707/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.1618 - val_loss: 45.9074\n",
      "Epoch 1708/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1587 - val_loss: 45.9036\n",
      "Epoch 1709/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1544 - val_loss: 45.9089\n",
      "Epoch 1710/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1525 - val_loss: 45.9075\n",
      "Epoch 1711/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1495 - val_loss: 45.9054\n",
      "Epoch 1712/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1517 - val_loss: 45.9011\n",
      "Epoch 1713/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1467 - val_loss: 45.9026\n",
      "Epoch 1714/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.1431 - val_loss: 45.9071\n",
      "Epoch 1715/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1423 - val_loss: 45.9103\n",
      "Epoch 1716/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1399 - val_loss: 45.9039\n",
      "Epoch 1717/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1368 - val_loss: 45.9034\n",
      "Epoch 1718/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1344 - val_loss: 45.9037\n",
      "Epoch 1719/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.1318 - val_loss: 45.8982\n",
      "Epoch 1720/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1311 - val_loss: 45.8951\n",
      "Epoch 1721/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1289 - val_loss: 45.8905\n",
      "Epoch 1722/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1250 - val_loss: 45.8895\n",
      "Epoch 1723/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1206 - val_loss: 45.8878\n",
      "Epoch 1724/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1211 - val_loss: 45.8849\n",
      "Epoch 1725/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1186 - val_loss: 45.8864\n",
      "Epoch 1726/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1222 - val_loss: 45.8832\n",
      "Epoch 1727/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1142 - val_loss: 45.8811\n",
      "Epoch 1728/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.1122 - val_loss: 45.8796\n",
      "Epoch 1729/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 49.1128 - val_loss: 45.8794\n",
      "Epoch 1730/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.1076 - val_loss: 45.8752\n",
      "Epoch 1731/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.1041 - val_loss: 45.8706\n",
      "Epoch 1732/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.1047 - val_loss: 45.8653\n",
      "Epoch 1733/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.1026 - val_loss: 45.8686\n",
      "Epoch 1734/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1014 - val_loss: 45.8656\n",
      "Epoch 1735/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0964 - val_loss: 45.8636\n",
      "Epoch 1736/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.0947 - val_loss: 45.8624\n",
      "Epoch 1737/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.0925 - val_loss: 45.8615\n",
      "Epoch 1738/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.0874 - val_loss: 45.8592\n",
      "Epoch 1739/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0871 - val_loss: 45.8523\n",
      "Epoch 1740/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0847 - val_loss: 45.8483\n",
      "Epoch 1741/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0846 - val_loss: 45.8486\n",
      "Epoch 1742/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0814 - val_loss: 45.8467\n",
      "Epoch 1743/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0810 - val_loss: 45.8429\n",
      "Epoch 1744/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.0746 - val_loss: 45.8465\n",
      "Epoch 1745/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.0734 - val_loss: 45.8533\n",
      "Epoch 1746/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0727 - val_loss: 45.8497\n",
      "Epoch 1747/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0699 - val_loss: 45.8475\n",
      "Epoch 1748/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0674 - val_loss: 45.8438\n",
      "Epoch 1749/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0672 - val_loss: 45.8398\n",
      "Epoch 1750/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0649 - val_loss: 45.8356\n",
      "Epoch 1751/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0617 - val_loss: 45.8302\n",
      "Epoch 1752/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.0588 - val_loss: 45.8283\n",
      "Epoch 1753/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0590 - val_loss: 45.8253\n",
      "Epoch 1754/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0533 - val_loss: 45.8256\n",
      "Epoch 1755/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0528 - val_loss: 45.8251\n",
      "Epoch 1756/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.0497 - val_loss: 45.8224\n",
      "Epoch 1757/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 49.0475 - val_loss: 45.8208\n",
      "Epoch 1758/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.0444 - val_loss: 45.8132\n",
      "Epoch 1759/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.0446 - val_loss: 45.8131\n",
      "Epoch 1760/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.0401 - val_loss: 45.8090\n",
      "Epoch 1761/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.0389 - val_loss: 45.8070\n",
      "Epoch 1762/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.0354 - val_loss: 45.8086\n",
      "Epoch 1763/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0341 - val_loss: 45.8082\n",
      "Epoch 1764/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0314 - val_loss: 45.8069\n",
      "Epoch 1765/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.0289 - val_loss: 45.8134\n",
      "Epoch 1766/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0282 - val_loss: 45.8138\n",
      "Epoch 1767/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0249 - val_loss: 45.8093\n",
      "Epoch 1768/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0234 - val_loss: 45.8050\n",
      "Epoch 1769/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0185 - val_loss: 45.8010\n",
      "Epoch 1770/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0174 - val_loss: 45.7973\n",
      "Epoch 1771/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0167 - val_loss: 45.7957\n",
      "Epoch 1772/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0178 - val_loss: 45.7997\n",
      "Epoch 1773/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0118 - val_loss: 45.8009\n",
      "Epoch 1774/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0096 - val_loss: 45.7969\n",
      "Epoch 1775/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 49.0102 - val_loss: 45.7985\n",
      "Epoch 1776/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0050 - val_loss: 45.7953\n",
      "Epoch 1777/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0018 - val_loss: 45.7937\n",
      "Epoch 1778/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 49.0023 - val_loss: 45.7973\n",
      "Epoch 1779/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9995 - val_loss: 45.7918\n",
      "Epoch 1780/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9954 - val_loss: 45.7869\n",
      "Epoch 1781/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9948 - val_loss: 45.7827\n",
      "Epoch 1782/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9919 - val_loss: 45.7861\n",
      "Epoch 1783/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9936 - val_loss: 45.7857\n",
      "Epoch 1784/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9870 - val_loss: 45.7860\n",
      "Epoch 1785/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 37us/step - loss: 48.9864 - val_loss: 45.7804\n",
      "Epoch 1786/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9835 - val_loss: 45.7789\n",
      "Epoch 1787/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9828 - val_loss: 45.7761\n",
      "Epoch 1788/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9794 - val_loss: 45.7729\n",
      "Epoch 1789/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9766 - val_loss: 45.7752\n",
      "Epoch 1790/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9746 - val_loss: 45.7735\n",
      "Epoch 1791/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9723 - val_loss: 45.7657\n",
      "Epoch 1792/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9729 - val_loss: 45.7634\n",
      "Epoch 1793/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9682 - val_loss: 45.7666\n",
      "Epoch 1794/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9660 - val_loss: 45.7664\n",
      "Epoch 1795/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9666 - val_loss: 45.7644\n",
      "Epoch 1796/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9632 - val_loss: 45.7640\n",
      "Epoch 1797/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9569 - val_loss: 45.7589\n",
      "Epoch 1798/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9570 - val_loss: 45.7493\n",
      "Epoch 1799/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9557 - val_loss: 45.7467\n",
      "Epoch 1800/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9536 - val_loss: 45.7520\n",
      "Epoch 1801/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9528 - val_loss: 45.7494\n",
      "Epoch 1802/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9510 - val_loss: 45.7496\n",
      "Epoch 1803/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9465 - val_loss: 45.7500\n",
      "Epoch 1804/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9457 - val_loss: 45.7476\n",
      "Epoch 1805/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9425 - val_loss: 45.7442\n",
      "Epoch 1806/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9414 - val_loss: 45.7467\n",
      "Epoch 1807/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9383 - val_loss: 45.7420\n",
      "Epoch 1808/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.9322 - val_loss: 45.7456\n",
      "Epoch 1809/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.9321 - val_loss: 45.7458\n",
      "Epoch 1810/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.9330 - val_loss: 45.7433\n",
      "Epoch 1811/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.9287 - val_loss: 45.7387\n",
      "Epoch 1812/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.9301 - val_loss: 45.7381\n",
      "Epoch 1813/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.9255 - val_loss: 45.7340\n",
      "Epoch 1814/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.9222 - val_loss: 45.7295\n",
      "Epoch 1815/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9195 - val_loss: 45.7284\n",
      "Epoch 1816/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9219 - val_loss: 45.7302\n",
      "Epoch 1817/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9152 - val_loss: 45.7276\n",
      "Epoch 1818/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9120 - val_loss: 45.7244\n",
      "Epoch 1819/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9123 - val_loss: 45.7207\n",
      "Epoch 1820/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.9125 - val_loss: 45.7164\n",
      "Epoch 1821/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9078 - val_loss: 45.7152\n",
      "Epoch 1822/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.9027 - val_loss: 45.7169\n",
      "Epoch 1823/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9023 - val_loss: 45.7168\n",
      "Epoch 1824/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.9000 - val_loss: 45.7143\n",
      "Epoch 1825/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8978 - val_loss: 45.7083\n",
      "Epoch 1826/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8963 - val_loss: 45.7028\n",
      "Epoch 1827/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8924 - val_loss: 45.7042\n",
      "Epoch 1828/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8970 - val_loss: 45.7059\n",
      "Epoch 1829/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8880 - val_loss: 45.7039\n",
      "Epoch 1830/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8854 - val_loss: 45.7055\n",
      "Epoch 1831/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.8863 - val_loss: 45.7030\n",
      "Epoch 1832/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8846 - val_loss: 45.6979\n",
      "Epoch 1833/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8852 - val_loss: 45.6942\n",
      "Epoch 1834/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8810 - val_loss: 45.6925\n",
      "Epoch 1835/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8743 - val_loss: 45.6964\n",
      "Epoch 1836/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8750 - val_loss: 45.6958\n",
      "Epoch 1837/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8716 - val_loss: 45.6933\n",
      "Epoch 1838/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8687 - val_loss: 45.6898\n",
      "Epoch 1839/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8673 - val_loss: 45.6945\n",
      "Epoch 1840/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8681 - val_loss: 45.6917\n",
      "Epoch 1841/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8652 - val_loss: 45.6886\n",
      "Epoch 1842/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.8603 - val_loss: 45.6849\n",
      "Epoch 1843/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.8598 - val_loss: 45.6840\n",
      "Epoch 1844/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.8578 - val_loss: 45.6836\n",
      "Epoch 1845/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8534 - val_loss: 45.6792\n",
      "Epoch 1846/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.8503 - val_loss: 45.6733\n",
      "Epoch 1847/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.8486 - val_loss: 45.6692\n",
      "Epoch 1848/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.8456 - val_loss: 45.6674\n",
      "Epoch 1849/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8459 - val_loss: 45.6667\n",
      "Epoch 1850/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.8444 - val_loss: 45.6653\n",
      "Epoch 1851/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8414 - val_loss: 45.6679\n",
      "Epoch 1852/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8386 - val_loss: 45.6654\n",
      "Epoch 1853/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8358 - val_loss: 45.6631\n",
      "Epoch 1854/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8366 - val_loss: 45.6728\n",
      "Epoch 1855/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8307 - val_loss: 45.6724\n",
      "Epoch 1856/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8311 - val_loss: 45.6674\n",
      "Epoch 1857/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8278 - val_loss: 45.6660\n",
      "Epoch 1858/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8254 - val_loss: 45.6660\n",
      "Epoch 1859/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8254 - val_loss: 45.6639\n",
      "Epoch 1860/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8230 - val_loss: 45.6568\n",
      "Epoch 1861/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8180 - val_loss: 45.6553\n",
      "Epoch 1862/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8145 - val_loss: 45.6534\n",
      "Epoch 1863/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.8129 - val_loss: 45.6504\n",
      "Epoch 1864/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8125 - val_loss: 45.6449\n",
      "Epoch 1865/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8103 - val_loss: 45.6420\n",
      "Epoch 1866/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8082 - val_loss: 45.6390\n",
      "Epoch 1867/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.8055 - val_loss: 45.6364\n",
      "Epoch 1868/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8059 - val_loss: 45.6344\n",
      "Epoch 1869/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.8016 - val_loss: 45.6328\n",
      "Epoch 1870/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7990 - val_loss: 45.6352\n",
      "Epoch 1871/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.7950 - val_loss: 45.6322\n",
      "Epoch 1872/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.7948 - val_loss: 45.6351\n",
      "Epoch 1873/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.7926 - val_loss: 45.6336\n",
      "Epoch 1874/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7929 - val_loss: 45.6316\n",
      "Epoch 1875/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7883 - val_loss: 45.6319\n",
      "Epoch 1876/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7873 - val_loss: 45.6326\n",
      "Epoch 1877/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.7861 - val_loss: 45.6288\n",
      "Epoch 1878/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7844 - val_loss: 45.6262\n",
      "Epoch 1879/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.7807 - val_loss: 45.6264\n",
      "Epoch 1880/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7797 - val_loss: 45.6186\n",
      "Epoch 1881/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7774 - val_loss: 45.6203\n",
      "Epoch 1882/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7742 - val_loss: 45.6224\n",
      "Epoch 1883/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7736 - val_loss: 45.6174\n",
      "Epoch 1884/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7688 - val_loss: 45.6145\n",
      "Epoch 1885/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7696 - val_loss: 45.6187\n",
      "Epoch 1886/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7694 - val_loss: 45.6162\n",
      "Epoch 1887/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7658 - val_loss: 45.6160\n",
      "Epoch 1888/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7617 - val_loss: 45.6132\n",
      "Epoch 1889/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7595 - val_loss: 45.6139\n",
      "Epoch 1890/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7569 - val_loss: 45.6133\n",
      "Epoch 1891/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7555 - val_loss: 45.6084\n",
      "Epoch 1892/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7564 - val_loss: 45.6040\n",
      "Epoch 1893/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7503 - val_loss: 45.5946\n",
      "Epoch 1894/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7526 - val_loss: 45.5905\n",
      "Epoch 1895/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7479 - val_loss: 45.5861\n",
      "Epoch 1896/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7444 - val_loss: 45.5842\n",
      "Epoch 1897/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7439 - val_loss: 45.5795\n",
      "Epoch 1898/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7409 - val_loss: 45.5754\n",
      "Epoch 1899/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7349 - val_loss: 45.5749\n",
      "Epoch 1900/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7351 - val_loss: 45.5725\n",
      "Epoch 1901/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7334 - val_loss: 45.5673\n",
      "Epoch 1902/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7321 - val_loss: 45.5645\n",
      "Epoch 1903/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7303 - val_loss: 45.5631\n",
      "Epoch 1904/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7270 - val_loss: 45.5585\n",
      "Epoch 1905/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7256 - val_loss: 45.5558\n",
      "Epoch 1906/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7236 - val_loss: 45.5523\n",
      "Epoch 1907/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.7216 - val_loss: 45.5491\n",
      "Epoch 1908/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7197 - val_loss: 45.5471\n",
      "Epoch 1909/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7152 - val_loss: 45.5453\n",
      "Epoch 1910/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7142 - val_loss: 45.5424\n",
      "Epoch 1911/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7120 - val_loss: 45.5394\n",
      "Epoch 1912/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.7099 - val_loss: 45.5347\n",
      "Epoch 1913/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.7088 - val_loss: 45.5352\n",
      "Epoch 1914/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.7063 - val_loss: 45.5343\n",
      "Epoch 1915/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.7045 - val_loss: 45.5312\n",
      "Epoch 1916/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7039 - val_loss: 45.5340\n",
      "Epoch 1917/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.7018 - val_loss: 45.5321\n",
      "Epoch 1918/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.6999 - val_loss: 45.5328\n",
      "Epoch 1919/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6937 - val_loss: 45.5341\n",
      "Epoch 1920/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6940 - val_loss: 45.5291\n",
      "Epoch 1921/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6900 - val_loss: 45.5252\n",
      "Epoch 1922/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6887 - val_loss: 45.5225\n",
      "Epoch 1923/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6891 - val_loss: 45.5265\n",
      "Epoch 1924/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6846 - val_loss: 45.5181\n",
      "Epoch 1925/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6825 - val_loss: 45.5183\n",
      "Epoch 1926/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6815 - val_loss: 45.5201\n",
      "Epoch 1927/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6803 - val_loss: 45.5215\n",
      "Epoch 1928/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6784 - val_loss: 45.5234\n",
      "Epoch 1929/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6763 - val_loss: 45.5208\n",
      "Epoch 1930/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6717 - val_loss: 45.5186\n",
      "Epoch 1931/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 48.6707 - val_loss: 45.5144\n",
      "Epoch 1932/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6683 - val_loss: 45.5119\n",
      "Epoch 1933/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6658 - val_loss: 45.5112\n",
      "Epoch 1934/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6676 - val_loss: 45.5088\n",
      "Epoch 1935/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6624 - val_loss: 45.5121\n",
      "Epoch 1936/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6596 - val_loss: 45.5119\n",
      "Epoch 1937/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6580 - val_loss: 45.5120\n",
      "Epoch 1938/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6557 - val_loss: 45.5057\n",
      "Epoch 1939/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6555 - val_loss: 45.5044\n",
      "Epoch 1940/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6514 - val_loss: 45.5043\n",
      "Epoch 1941/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6500 - val_loss: 45.5044\n",
      "Epoch 1942/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6482 - val_loss: 45.5096\n",
      "Epoch 1943/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6475 - val_loss: 45.5120\n",
      "Epoch 1944/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6461 - val_loss: 45.5098\n",
      "Epoch 1945/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6443 - val_loss: 45.5070\n",
      "Epoch 1946/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6428 - val_loss: 45.5021\n",
      "Epoch 1947/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6375 - val_loss: 45.5034\n",
      "Epoch 1948/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6365 - val_loss: 45.5037\n",
      "Epoch 1949/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6347 - val_loss: 45.5002\n",
      "Epoch 1950/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6296 - val_loss: 45.4965\n",
      "Epoch 1951/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6303 - val_loss: 45.4940\n",
      "Epoch 1952/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6281 - val_loss: 45.4914\n",
      "Epoch 1953/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6269 - val_loss: 45.4883\n",
      "Epoch 1954/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6250 - val_loss: 45.4887\n",
      "Epoch 1955/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 48.6216 - val_loss: 45.4851\n",
      "Epoch 1956/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.6183 - val_loss: 45.4828\n",
      "Epoch 1957/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6193 - val_loss: 45.4812\n",
      "Epoch 1958/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6179 - val_loss: 45.4806\n",
      "Epoch 1959/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6135 - val_loss: 45.4820\n",
      "Epoch 1960/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.6140 - val_loss: 45.4822\n",
      "Epoch 1961/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6084 - val_loss: 45.4781\n",
      "Epoch 1962/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6083 - val_loss: 45.4732\n",
      "Epoch 1963/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6051 - val_loss: 45.4676\n",
      "Epoch 1964/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.6072 - val_loss: 45.4680\n",
      "Epoch 1965/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6030 - val_loss: 45.4654\n",
      "Epoch 1966/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.6005 - val_loss: 45.4607\n",
      "Epoch 1967/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.5970 - val_loss: 45.4588\n",
      "Epoch 1968/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5953 - val_loss: 45.4577\n",
      "Epoch 1969/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5946 - val_loss: 45.4570\n",
      "Epoch 1970/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5897 - val_loss: 45.4570\n",
      "Epoch 1971/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5907 - val_loss: 45.4555\n",
      "Epoch 1972/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5896 - val_loss: 45.4505\n",
      "Epoch 1973/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5858 - val_loss: 45.4553\n",
      "Epoch 1974/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5844 - val_loss: 45.4545\n",
      "Epoch 1975/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5824 - val_loss: 45.4511\n",
      "Epoch 1976/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5781 - val_loss: 45.4472\n",
      "Epoch 1977/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5794 - val_loss: 45.4442\n",
      "Epoch 1978/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5753 - val_loss: 45.4412\n",
      "Epoch 1979/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5729 - val_loss: 45.4391\n",
      "Epoch 1980/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5718 - val_loss: 45.4380\n",
      "Epoch 1981/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5692 - val_loss: 45.4398\n",
      "Epoch 1982/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5698 - val_loss: 45.4410\n",
      "Epoch 1983/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5651 - val_loss: 45.4386\n",
      "Epoch 1984/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5658 - val_loss: 45.4372\n",
      "Epoch 1985/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5617 - val_loss: 45.4386\n",
      "Epoch 1986/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5616 - val_loss: 45.4394\n",
      "Epoch 1987/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5621 - val_loss: 45.4456\n",
      "Epoch 1988/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5591 - val_loss: 45.4418\n",
      "Epoch 1989/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5555 - val_loss: 45.4412\n",
      "Epoch 1990/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5537 - val_loss: 45.4364\n",
      "Epoch 1991/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5520 - val_loss: 45.4341\n",
      "Epoch 1992/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5484 - val_loss: 45.4340\n",
      "Epoch 1993/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5463 - val_loss: 45.4336\n",
      "Epoch 1994/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5430 - val_loss: 45.4389\n",
      "Epoch 1995/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5413 - val_loss: 45.4389\n",
      "Epoch 1996/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5415 - val_loss: 45.4380\n",
      "Epoch 1997/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5389 - val_loss: 45.4338\n",
      "Epoch 1998/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5374 - val_loss: 45.4395\n",
      "Epoch 1999/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5401 - val_loss: 45.4349\n",
      "Epoch 2000/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5353 - val_loss: 45.4358\n",
      "Epoch 2001/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5322 - val_loss: 45.4268\n",
      "Epoch 2002/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5292 - val_loss: 45.4231\n",
      "Epoch 2003/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5267 - val_loss: 45.4187\n",
      "Epoch 2004/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5235 - val_loss: 45.4124\n",
      "Epoch 2005/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5236 - val_loss: 45.4113\n",
      "Epoch 2006/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5210 - val_loss: 45.4034\n",
      "Epoch 2007/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5193 - val_loss: 45.4129\n",
      "Epoch 2008/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5171 - val_loss: 45.4122\n",
      "Epoch 2009/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5158 - val_loss: 45.4074\n",
      "Epoch 2010/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5110 - val_loss: 45.4027\n",
      "Epoch 2011/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.5123 - val_loss: 45.3932\n",
      "Epoch 2012/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.5085 - val_loss: 45.3916\n",
      "Epoch 2013/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5042 - val_loss: 45.3927\n",
      "Epoch 2014/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.5041 - val_loss: 45.3929\n",
      "Epoch 2015/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5014 - val_loss: 45.3914\n",
      "Epoch 2016/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.5001 - val_loss: 45.3900\n",
      "Epoch 2017/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.5001 - val_loss: 45.3906\n",
      "Epoch 2018/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.4977 - val_loss: 45.3874\n",
      "Epoch 2019/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.4948 - val_loss: 45.3870\n",
      "Epoch 2020/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4916 - val_loss: 45.3838\n",
      "Epoch 2021/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4936 - val_loss: 45.3854\n",
      "Epoch 2022/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4882 - val_loss: 45.3832\n",
      "Epoch 2023/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4855 - val_loss: 45.3788\n",
      "Epoch 2024/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.4832 - val_loss: 45.3772\n",
      "Epoch 2025/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4837 - val_loss: 45.3750\n",
      "Epoch 2026/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4791 - val_loss: 45.3788\n",
      "Epoch 2027/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4776 - val_loss: 45.3807\n",
      "Epoch 2028/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4751 - val_loss: 45.3783\n",
      "Epoch 2029/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4761 - val_loss: 45.3763\n",
      "Epoch 2030/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.4733 - val_loss: 45.3705\n",
      "Epoch 2031/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4707 - val_loss: 45.3705\n",
      "Epoch 2032/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.4670 - val_loss: 45.3702\n",
      "Epoch 2033/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4669 - val_loss: 45.3701\n",
      "Epoch 2034/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4668 - val_loss: 45.3727\n",
      "Epoch 2035/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4643 - val_loss: 45.3715\n",
      "Epoch 2036/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4610 - val_loss: 45.3645\n",
      "Epoch 2037/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4619 - val_loss: 45.3633\n",
      "Epoch 2038/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.4580 - val_loss: 45.3625\n",
      "Epoch 2039/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4540 - val_loss: 45.3598\n",
      "Epoch 2040/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4540 - val_loss: 45.3527\n",
      "Epoch 2041/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.4492 - val_loss: 45.3466\n",
      "Epoch 2042/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4483 - val_loss: 45.3489\n",
      "Epoch 2043/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4466 - val_loss: 45.3485\n",
      "Epoch 2044/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4452 - val_loss: 45.3445\n",
      "Epoch 2045/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4417 - val_loss: 45.3436\n",
      "Epoch 2046/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4419 - val_loss: 45.3394\n",
      "Epoch 2047/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4403 - val_loss: 45.3343\n",
      "Epoch 2048/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4370 - val_loss: 45.3330\n",
      "Epoch 2049/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4358 - val_loss: 45.3296\n",
      "Epoch 2050/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4316 - val_loss: 45.3289\n",
      "Epoch 2051/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4315 - val_loss: 45.3272\n",
      "Epoch 2052/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4286 - val_loss: 45.3292\n",
      "Epoch 2053/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4270 - val_loss: 45.3354\n",
      "Epoch 2054/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4237 - val_loss: 45.3291\n",
      "Epoch 2055/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.4246 - val_loss: 45.3311\n",
      "Epoch 2056/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4198 - val_loss: 45.3366\n",
      "Epoch 2057/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4222 - val_loss: 45.3330\n",
      "Epoch 2058/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.4172 - val_loss: 45.3306\n",
      "Epoch 2059/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4136 - val_loss: 45.3282\n",
      "Epoch 2060/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4153 - val_loss: 45.3298\n",
      "Epoch 2061/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4122 - val_loss: 45.3234\n",
      "Epoch 2062/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.4078 - val_loss: 45.3192\n",
      "Epoch 2063/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.4085 - val_loss: 45.3151\n",
      "Epoch 2064/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.4062 - val_loss: 45.3143\n",
      "Epoch 2065/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.4025 - val_loss: 45.3108\n",
      "Epoch 2066/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.4034 - val_loss: 45.3102\n",
      "Epoch 2067/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3987 - val_loss: 45.3145\n",
      "Epoch 2068/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3990 - val_loss: 45.3098\n",
      "Epoch 2069/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3931 - val_loss: 45.3072\n",
      "Epoch 2070/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.3953 - val_loss: 45.3037\n",
      "Epoch 2071/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3922 - val_loss: 45.3080\n",
      "Epoch 2072/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3912 - val_loss: 45.3041\n",
      "Epoch 2073/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3885 - val_loss: 45.3067\n",
      "Epoch 2074/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3887 - val_loss: 45.3090\n",
      "Epoch 2075/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3831 - val_loss: 45.3081\n",
      "Epoch 2076/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.3840 - val_loss: 45.3064\n",
      "Epoch 2077/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3818 - val_loss: 45.3042\n",
      "Epoch 2078/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3786 - val_loss: 45.3034\n",
      "Epoch 2079/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3754 - val_loss: 45.2969\n",
      "Epoch 2080/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3752 - val_loss: 45.2924\n",
      "Epoch 2081/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3755 - val_loss: 45.2890\n",
      "Epoch 2082/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.3689 - val_loss: 45.2888\n",
      "Epoch 2083/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.3696 - val_loss: 45.2853\n",
      "Epoch 2084/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3668 - val_loss: 45.2849\n",
      "Epoch 2085/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3659 - val_loss: 45.2812\n",
      "Epoch 2086/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3654 - val_loss: 45.2822\n",
      "Epoch 2087/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3624 - val_loss: 45.2793\n",
      "Epoch 2088/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3625 - val_loss: 45.2762\n",
      "Epoch 2089/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.3572 - val_loss: 45.2722\n",
      "Epoch 2090/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.3546 - val_loss: 45.2717\n",
      "Epoch 2091/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 48.3518 - val_loss: 45.2690\n",
      "Epoch 2092/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 48.3496 - val_loss: 45.2581\n",
      "Epoch 2093/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.3489 - val_loss: 45.2542\n",
      "Epoch 2094/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3446 - val_loss: 45.2516\n",
      "Epoch 2095/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.3458 - val_loss: 45.2501\n",
      "Epoch 2096/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3430 - val_loss: 45.2460\n",
      "Epoch 2097/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3402 - val_loss: 45.2450\n",
      "Epoch 2098/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.3381 - val_loss: 45.2471\n",
      "Epoch 2099/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.3384 - val_loss: 45.2537\n",
      "Epoch 2100/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3372 - val_loss: 45.2538\n",
      "Epoch 2101/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3328 - val_loss: 45.2510\n",
      "Epoch 2102/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3301 - val_loss: 45.2512\n",
      "Epoch 2103/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3287 - val_loss: 45.2484\n",
      "Epoch 2104/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3252 - val_loss: 45.2485\n",
      "Epoch 2105/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3247 - val_loss: 45.2449\n",
      "Epoch 2106/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3272 - val_loss: 45.2399\n",
      "Epoch 2107/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3217 - val_loss: 45.2428\n",
      "Epoch 2108/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3231 - val_loss: 45.2372\n",
      "Epoch 2109/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3172 - val_loss: 45.2386\n",
      "Epoch 2110/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.3156 - val_loss: 45.2364\n",
      "Epoch 2111/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.3128 - val_loss: 45.2337\n",
      "Epoch 2112/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.3130 - val_loss: 45.2384\n",
      "Epoch 2113/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.3084 - val_loss: 45.2359\n",
      "Epoch 2114/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.3106 - val_loss: 45.2365\n",
      "Epoch 2115/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.3091 - val_loss: 45.2356\n",
      "Epoch 2116/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.3042 - val_loss: 45.2372\n",
      "Epoch 2117/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.3032 - val_loss: 45.2341\n",
      "Epoch 2118/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2987 - val_loss: 45.2283\n",
      "Epoch 2119/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.3005 - val_loss: 45.2281\n",
      "Epoch 2120/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2978 - val_loss: 45.2245\n",
      "Epoch 2121/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2975 - val_loss: 45.2186\n",
      "Epoch 2122/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2925 - val_loss: 45.2168\n",
      "Epoch 2123/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2925 - val_loss: 45.2145\n",
      "Epoch 2124/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.2894 - val_loss: 45.2149\n",
      "Epoch 2125/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2873 - val_loss: 45.2207\n",
      "Epoch 2126/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.2856 - val_loss: 45.2209\n",
      "Epoch 2127/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 48.2833 - val_loss: 45.2171\n",
      "Epoch 2128/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.2801 - val_loss: 45.2129\n",
      "Epoch 2129/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.2782 - val_loss: 45.2110\n",
      "Epoch 2130/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.2770 - val_loss: 45.2072\n",
      "Epoch 2131/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.2744 - val_loss: 45.2019\n",
      "Epoch 2132/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2768 - val_loss: 45.2032\n",
      "Epoch 2133/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.2728 - val_loss: 45.2023\n",
      "Epoch 2134/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.2673 - val_loss: 45.1997\n",
      "Epoch 2135/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.2688 - val_loss: 45.1979\n",
      "Epoch 2136/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.2668 - val_loss: 45.1996\n",
      "Epoch 2137/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.2637 - val_loss: 45.1978\n",
      "Epoch 2138/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2618 - val_loss: 45.1964\n",
      "Epoch 2139/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.2616 - val_loss: 45.1935\n",
      "Epoch 2140/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.2572 - val_loss: 45.1912\n",
      "Epoch 2141/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.2569 - val_loss: 45.1842\n",
      "Epoch 2142/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2548 - val_loss: 45.1807\n",
      "Epoch 2143/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2520 - val_loss: 45.1780\n",
      "Epoch 2144/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2501 - val_loss: 45.1798\n",
      "Epoch 2145/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2476 - val_loss: 45.1780\n",
      "Epoch 2146/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2466 - val_loss: 45.1820\n",
      "Epoch 2147/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2461 - val_loss: 45.1807\n",
      "Epoch 2148/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.2446 - val_loss: 45.1773\n",
      "Epoch 2149/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.2409 - val_loss: 45.1761\n",
      "Epoch 2150/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.2389 - val_loss: 45.1721\n",
      "Epoch 2151/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2366 - val_loss: 45.1718\n",
      "Epoch 2152/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.2356 - val_loss: 45.1662\n",
      "Epoch 2153/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.2348 - val_loss: 45.1653\n",
      "Epoch 2154/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.2296 - val_loss: 45.1664\n",
      "Epoch 2155/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2295 - val_loss: 45.1644\n",
      "Epoch 2156/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2277 - val_loss: 45.1637\n",
      "Epoch 2157/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2289 - val_loss: 45.1652\n",
      "Epoch 2158/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2226 - val_loss: 45.1610\n",
      "Epoch 2159/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2215 - val_loss: 45.1646\n",
      "Epoch 2160/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2223 - val_loss: 45.1578\n",
      "Epoch 2161/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2196 - val_loss: 45.1564\n",
      "Epoch 2162/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2197 - val_loss: 45.1547\n",
      "Epoch 2163/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2152 - val_loss: 45.1512\n",
      "Epoch 2164/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2129 - val_loss: 45.1481\n",
      "Epoch 2165/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2098 - val_loss: 45.1461\n",
      "Epoch 2166/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2076 - val_loss: 45.1482\n",
      "Epoch 2167/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.2059 - val_loss: 45.1479\n",
      "Epoch 2168/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.2053 - val_loss: 45.1450\n",
      "Epoch 2169/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2021 - val_loss: 45.1425\n",
      "Epoch 2170/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.2001 - val_loss: 45.1455\n",
      "Epoch 2171/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1970 - val_loss: 45.1432\n",
      "Epoch 2172/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1979 - val_loss: 45.1448\n",
      "Epoch 2173/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.1958 - val_loss: 45.1429\n",
      "Epoch 2174/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1909 - val_loss: 45.1444\n",
      "Epoch 2175/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1935 - val_loss: 45.1376\n",
      "Epoch 2176/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1891 - val_loss: 45.1353\n",
      "Epoch 2177/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1878 - val_loss: 45.1312\n",
      "Epoch 2178/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1836 - val_loss: 45.1304\n",
      "Epoch 2179/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1814 - val_loss: 45.1275\n",
      "Epoch 2180/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.1808 - val_loss: 45.1260\n",
      "Epoch 2181/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.1784 - val_loss: 45.1232\n",
      "Epoch 2182/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1761 - val_loss: 45.1242\n",
      "Epoch 2183/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1739 - val_loss: 45.1287\n",
      "Epoch 2184/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1755 - val_loss: 45.1287\n",
      "Epoch 2185/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1713 - val_loss: 45.1237\n",
      "Epoch 2186/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1686 - val_loss: 45.1273\n",
      "Epoch 2187/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1712 - val_loss: 45.1255\n",
      "Epoch 2188/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1686 - val_loss: 45.1175\n",
      "Epoch 2189/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1650 - val_loss: 45.1148\n",
      "Epoch 2190/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1615 - val_loss: 45.1107\n",
      "Epoch 2191/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1612 - val_loss: 45.1055\n",
      "Epoch 2192/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1595 - val_loss: 45.0977\n",
      "Epoch 2193/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1584 - val_loss: 45.1002\n",
      "Epoch 2194/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1572 - val_loss: 45.0990\n",
      "Epoch 2195/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.1523 - val_loss: 45.0948\n",
      "Epoch 2196/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1490 - val_loss: 45.0920\n",
      "Epoch 2197/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1473 - val_loss: 45.0924\n",
      "Epoch 2198/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1469 - val_loss: 45.0881\n",
      "Epoch 2199/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1454 - val_loss: 45.0927\n",
      "Epoch 2200/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1426 - val_loss: 45.0899\n",
      "Epoch 2201/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1395 - val_loss: 45.0917\n",
      "Epoch 2202/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1391 - val_loss: 45.0906\n",
      "Epoch 2203/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1351 - val_loss: 45.0900\n",
      "Epoch 2204/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1366 - val_loss: 45.0885\n",
      "Epoch 2205/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1329 - val_loss: 45.0865\n",
      "Epoch 2206/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1284 - val_loss: 45.0867\n",
      "Epoch 2207/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1292 - val_loss: 45.0876\n",
      "Epoch 2208/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.1268 - val_loss: 45.0930\n",
      "Epoch 2209/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1239 - val_loss: 45.0920\n",
      "Epoch 2210/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.1210 - val_loss: 45.0848\n",
      "Epoch 2211/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1179 - val_loss: 45.0797\n",
      "Epoch 2212/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.1155 - val_loss: 45.0728\n",
      "Epoch 2213/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1153 - val_loss: 45.0721\n",
      "Epoch 2214/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1141 - val_loss: 45.0699\n",
      "Epoch 2215/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1096 - val_loss: 45.0665\n",
      "Epoch 2216/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.1056 - val_loss: 45.0683\n",
      "Epoch 2217/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.1073 - val_loss: 45.0695\n",
      "Epoch 2218/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1026 - val_loss: 45.0656\n",
      "Epoch 2219/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.1021 - val_loss: 45.0668\n",
      "Epoch 2220/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.1013 - val_loss: 45.0633\n",
      "Epoch 2221/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0983 - val_loss: 45.0590\n",
      "Epoch 2222/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0997 - val_loss: 45.0556\n",
      "Epoch 2223/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0967 - val_loss: 45.0491\n",
      "Epoch 2224/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0950 - val_loss: 45.0466\n",
      "Epoch 2225/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0921 - val_loss: 45.0457\n",
      "Epoch 2226/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0873 - val_loss: 45.0447\n",
      "Epoch 2227/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0874 - val_loss: 45.0401\n",
      "Epoch 2228/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0839 - val_loss: 45.0384\n",
      "Epoch 2229/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0835 - val_loss: 45.0362\n",
      "Epoch 2230/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0830 - val_loss: 45.0364\n",
      "Epoch 2231/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0796 - val_loss: 45.0391\n",
      "Epoch 2232/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0768 - val_loss: 45.0345\n",
      "Epoch 2233/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0731 - val_loss: 45.0319\n",
      "Epoch 2234/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0708 - val_loss: 45.0285\n",
      "Epoch 2235/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0705 - val_loss: 45.0243\n",
      "Epoch 2236/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0674 - val_loss: 45.0215\n",
      "Epoch 2237/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0667 - val_loss: 45.0224\n",
      "Epoch 2238/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0658 - val_loss: 45.0265\n",
      "Epoch 2239/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0625 - val_loss: 45.0189\n",
      "Epoch 2240/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0611 - val_loss: 45.0154\n",
      "Epoch 2241/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0599 - val_loss: 45.0133\n",
      "Epoch 2242/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0569 - val_loss: 45.0161\n",
      "Epoch 2243/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0574 - val_loss: 45.0126\n",
      "Epoch 2244/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0532 - val_loss: 45.0117\n",
      "Epoch 2245/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0502 - val_loss: 45.0106\n",
      "Epoch 2246/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0490 - val_loss: 45.0113\n",
      "Epoch 2247/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0472 - val_loss: 45.0074\n",
      "Epoch 2248/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0454 - val_loss: 45.0069\n",
      "Epoch 2249/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0419 - val_loss: 45.0061\n",
      "Epoch 2250/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0400 - val_loss: 45.0030\n",
      "Epoch 2251/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0391 - val_loss: 45.0025\n",
      "Epoch 2252/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0369 - val_loss: 45.0018\n",
      "Epoch 2253/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.0356 - val_loss: 45.0006\n",
      "Epoch 2254/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0327 - val_loss: 44.9982\n",
      "Epoch 2255/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0317 - val_loss: 44.9970\n",
      "Epoch 2256/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0291 - val_loss: 44.9942\n",
      "Epoch 2257/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 48.0275 - val_loss: 44.9926\n",
      "Epoch 2258/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0259 - val_loss: 44.9930\n",
      "Epoch 2259/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0235 - val_loss: 44.9936\n",
      "Epoch 2260/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0190 - val_loss: 44.9889\n",
      "Epoch 2261/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0185 - val_loss: 44.9869\n",
      "Epoch 2262/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0150 - val_loss: 44.9818\n",
      "Epoch 2263/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.0169 - val_loss: 44.9825\n",
      "Epoch 2264/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0122 - val_loss: 44.9791\n",
      "Epoch 2265/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0130 - val_loss: 44.9798\n",
      "Epoch 2266/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 48.0093 - val_loss: 44.9673\n",
      "Epoch 2267/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.0074 - val_loss: 44.9689\n",
      "Epoch 2268/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0018 - val_loss: 44.9677\n",
      "Epoch 2269/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 48.0028 - val_loss: 44.9679\n",
      "Epoch 2270/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9993 - val_loss: 44.9678\n",
      "Epoch 2271/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.9982 - val_loss: 44.9667\n",
      "Epoch 2272/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9970 - val_loss: 44.9644\n",
      "Epoch 2273/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9927 - val_loss: 44.9647\n",
      "Epoch 2274/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9944 - val_loss: 44.9637\n",
      "Epoch 2275/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9907 - val_loss: 44.9614\n",
      "Epoch 2276/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9881 - val_loss: 44.9650\n",
      "Epoch 2277/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9879 - val_loss: 44.9616\n",
      "Epoch 2278/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.9836 - val_loss: 44.9621\n",
      "Epoch 2279/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9824 - val_loss: 44.9631\n",
      "Epoch 2280/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9784 - val_loss: 44.9567\n",
      "Epoch 2281/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.9776 - val_loss: 44.9558\n",
      "Epoch 2282/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9762 - val_loss: 44.9581\n",
      "Epoch 2283/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9730 - val_loss: 44.9582\n",
      "Epoch 2284/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9739 - val_loss: 44.9585\n",
      "Epoch 2285/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9712 - val_loss: 44.9556\n",
      "Epoch 2286/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9685 - val_loss: 44.9572\n",
      "Epoch 2287/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9659 - val_loss: 44.9589\n",
      "Epoch 2288/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9649 - val_loss: 44.9560\n",
      "Epoch 2289/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9632 - val_loss: 44.9574\n",
      "Epoch 2290/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.9609 - val_loss: 44.9576\n",
      "Epoch 2291/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9583 - val_loss: 44.9544\n",
      "Epoch 2292/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9558 - val_loss: 44.9580\n",
      "Epoch 2293/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9554 - val_loss: 44.9587\n",
      "Epoch 2294/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9518 - val_loss: 44.9558\n",
      "Epoch 2295/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9545 - val_loss: 44.9575\n",
      "Epoch 2296/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 47.9489 - val_loss: 44.9539\n",
      "Epoch 2297/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.9482 - val_loss: 44.9525\n",
      "Epoch 2298/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.9452 - val_loss: 44.9500\n",
      "Epoch 2299/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.9436 - val_loss: 44.9517\n",
      "Epoch 2300/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9416 - val_loss: 44.9453\n",
      "Epoch 2301/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9377 - val_loss: 44.9446\n",
      "Epoch 2302/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9375 - val_loss: 44.9394\n",
      "Epoch 2303/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9347 - val_loss: 44.9348\n",
      "Epoch 2304/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9317 - val_loss: 44.9307\n",
      "Epoch 2305/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9284 - val_loss: 44.9286\n",
      "Epoch 2306/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.9286 - val_loss: 44.9246\n",
      "Epoch 2307/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9252 - val_loss: 44.9234\n",
      "Epoch 2308/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9258 - val_loss: 44.9198\n",
      "Epoch 2309/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9221 - val_loss: 44.9184\n",
      "Epoch 2310/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9210 - val_loss: 44.9163\n",
      "Epoch 2311/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9175 - val_loss: 44.9140\n",
      "Epoch 2312/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9151 - val_loss: 44.9115\n",
      "Epoch 2313/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9125 - val_loss: 44.9072\n",
      "Epoch 2314/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9112 - val_loss: 44.9008\n",
      "Epoch 2315/5000\n",
      "1126/1126 [==============================] - 0s 28us/step - loss: 47.9101 - val_loss: 44.9031\n",
      "Epoch 2316/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9086 - val_loss: 44.9003\n",
      "Epoch 2317/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.9046 - val_loss: 44.8980\n",
      "Epoch 2318/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9033 - val_loss: 44.8969\n",
      "Epoch 2319/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9039 - val_loss: 44.8920\n",
      "Epoch 2320/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.9009 - val_loss: 44.8836\n",
      "Epoch 2321/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8972 - val_loss: 44.8820\n",
      "Epoch 2322/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8975 - val_loss: 44.8816\n",
      "Epoch 2323/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8933 - val_loss: 44.8795\n",
      "Epoch 2324/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8942 - val_loss: 44.8833\n",
      "Epoch 2325/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.8893 - val_loss: 44.8900\n",
      "Epoch 2326/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8868 - val_loss: 44.8876\n",
      "Epoch 2327/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.8840 - val_loss: 44.8841\n",
      "Epoch 2328/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8825 - val_loss: 44.8817\n",
      "Epoch 2329/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8819 - val_loss: 44.8808\n",
      "Epoch 2330/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.8807 - val_loss: 44.8796\n",
      "Epoch 2331/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.8767 - val_loss: 44.8786\n",
      "Epoch 2332/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.8754 - val_loss: 44.8764\n",
      "Epoch 2333/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 47.8746 - val_loss: 44.8738\n",
      "Epoch 2334/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.8726 - val_loss: 44.8714\n",
      "Epoch 2335/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.8711 - val_loss: 44.8689\n",
      "Epoch 2336/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.8695 - val_loss: 44.8705\n",
      "Epoch 2337/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.8647 - val_loss: 44.8681\n",
      "Epoch 2338/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8622 - val_loss: 44.8669\n",
      "Epoch 2339/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.8612 - val_loss: 44.8701\n",
      "Epoch 2340/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8576 - val_loss: 44.8660\n",
      "Epoch 2341/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8557 - val_loss: 44.8641\n",
      "Epoch 2342/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8543 - val_loss: 44.8617\n",
      "Epoch 2343/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8511 - val_loss: 44.8605\n",
      "Epoch 2344/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8515 - val_loss: 44.8606\n",
      "Epoch 2345/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8492 - val_loss: 44.8596\n",
      "Epoch 2346/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8487 - val_loss: 44.8617\n",
      "Epoch 2347/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8451 - val_loss: 44.8582\n",
      "Epoch 2348/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8449 - val_loss: 44.8585\n",
      "Epoch 2349/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8432 - val_loss: 44.8551\n",
      "Epoch 2350/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.8396 - val_loss: 44.8552\n",
      "Epoch 2351/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8368 - val_loss: 44.8544\n",
      "Epoch 2352/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.8338 - val_loss: 44.8559\n",
      "Epoch 2353/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8346 - val_loss: 44.8507\n",
      "Epoch 2354/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8338 - val_loss: 44.8452\n",
      "Epoch 2355/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8275 - val_loss: 44.8462\n",
      "Epoch 2356/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8268 - val_loss: 44.8475\n",
      "Epoch 2357/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8252 - val_loss: 44.8446\n",
      "Epoch 2358/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8225 - val_loss: 44.8467\n",
      "Epoch 2359/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8238 - val_loss: 44.8440\n",
      "Epoch 2360/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8171 - val_loss: 44.8445\n",
      "Epoch 2361/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8185 - val_loss: 44.8415\n",
      "Epoch 2362/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8163 - val_loss: 44.8378\n",
      "Epoch 2363/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.8147 - val_loss: 44.8343\n",
      "Epoch 2364/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.8134 - val_loss: 44.8289\n",
      "Epoch 2365/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.8098 - val_loss: 44.8278\n",
      "Epoch 2366/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 47.8045 - val_loss: 44.8243\n",
      "Epoch 2367/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 47.8054 - val_loss: 44.8272\n",
      "Epoch 2368/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.8037 - val_loss: 44.8273\n",
      "Epoch 2369/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.8019 - val_loss: 44.8280\n",
      "Epoch 2370/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7992 - val_loss: 44.8237\n",
      "Epoch 2371/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7974 - val_loss: 44.8248\n",
      "Epoch 2372/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7960 - val_loss: 44.8244\n",
      "Epoch 2373/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7922 - val_loss: 44.8226\n",
      "Epoch 2374/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.7951 - val_loss: 44.8221\n",
      "Epoch 2375/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7892 - val_loss: 44.8185\n",
      "Epoch 2376/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7864 - val_loss: 44.8151\n",
      "Epoch 2377/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7891 - val_loss: 44.8118\n",
      "Epoch 2378/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7805 - val_loss: 44.8079\n",
      "Epoch 2379/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7792 - val_loss: 44.8068\n",
      "Epoch 2380/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 47.7801 - val_loss: 44.8023\n",
      "Epoch 2381/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7758 - val_loss: 44.8019\n",
      "Epoch 2382/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7757 - val_loss: 44.8015\n",
      "Epoch 2383/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7728 - val_loss: 44.7996\n",
      "Epoch 2384/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 47.7703 - val_loss: 44.7973\n",
      "Epoch 2385/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.7683 - val_loss: 44.7973\n",
      "Epoch 2386/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7654 - val_loss: 44.7930\n",
      "Epoch 2387/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7655 - val_loss: 44.7934\n",
      "Epoch 2388/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.7638 - val_loss: 44.7925\n",
      "Epoch 2389/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.7619 - val_loss: 44.7910\n",
      "Epoch 2390/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7593 - val_loss: 44.7882\n",
      "Epoch 2391/5000\n",
      "1126/1126 [==============================] - 0s 29us/step - loss: 47.7583 - val_loss: 44.7852\n",
      "Epoch 2392/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7570 - val_loss: 44.7847\n",
      "Epoch 2393/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7534 - val_loss: 44.7823\n",
      "Epoch 2394/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7490 - val_loss: 44.7836\n",
      "Epoch 2395/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7511 - val_loss: 44.7864\n",
      "Epoch 2396/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7454 - val_loss: 44.7816\n",
      "Epoch 2397/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7443 - val_loss: 44.7778\n",
      "Epoch 2398/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.7435 - val_loss: 44.7773\n",
      "Epoch 2399/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 47.7415 - val_loss: 44.7774\n",
      "Epoch 2400/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.7398 - val_loss: 44.7754\n",
      "Epoch 2401/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.7354 - val_loss: 44.7726\n",
      "Epoch 2402/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.7347 - val_loss: 44.7712\n",
      "Epoch 2403/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7321 - val_loss: 44.7748\n",
      "Epoch 2404/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7326 - val_loss: 44.7725\n",
      "Epoch 2405/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 47.7295 - val_loss: 44.7714\n",
      "Epoch 2406/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 47.7274 - val_loss: 44.7688\n",
      "Epoch 2407/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.7251 - val_loss: 44.7643\n",
      "Epoch 2408/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7210 - val_loss: 44.7586\n",
      "Epoch 2409/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7228 - val_loss: 44.7540\n",
      "Epoch 2410/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7163 - val_loss: 44.7522\n",
      "Epoch 2411/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7169 - val_loss: 44.7509\n",
      "Epoch 2412/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7130 - val_loss: 44.7489\n",
      "Epoch 2413/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7108 - val_loss: 44.7467\n",
      "Epoch 2414/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 47.7093 - val_loss: 44.7483\n",
      "Epoch 2415/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7089 - val_loss: 44.7480\n",
      "Epoch 2416/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.7073 - val_loss: 44.7415\n",
      "Epoch 2417/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.7041 - val_loss: 44.7378\n",
      "Epoch 2418/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6997 - val_loss: 44.7354\n",
      "Epoch 2419/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6994 - val_loss: 44.7294\n",
      "Epoch 2420/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6953 - val_loss: 44.7227\n",
      "Epoch 2421/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6924 - val_loss: 44.7214\n",
      "Epoch 2422/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6915 - val_loss: 44.7200\n",
      "Epoch 2423/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6881 - val_loss: 44.7194\n",
      "Epoch 2424/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6878 - val_loss: 44.7123\n",
      "Epoch 2425/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6858 - val_loss: 44.7146\n",
      "Epoch 2426/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6830 - val_loss: 44.7159\n",
      "Epoch 2427/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6812 - val_loss: 44.7192\n",
      "Epoch 2428/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6787 - val_loss: 44.7160\n",
      "Epoch 2429/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6754 - val_loss: 44.7131\n",
      "Epoch 2430/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6742 - val_loss: 44.7111\n",
      "Epoch 2431/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6706 - val_loss: 44.7116\n",
      "Epoch 2432/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6687 - val_loss: 44.7108\n",
      "Epoch 2433/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6646 - val_loss: 44.7039\n",
      "Epoch 2434/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.6645 - val_loss: 44.7004\n",
      "Epoch 2435/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6622 - val_loss: 44.7010\n",
      "Epoch 2436/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6592 - val_loss: 44.6948\n",
      "Epoch 2437/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6585 - val_loss: 44.6926\n",
      "Epoch 2438/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6553 - val_loss: 44.6919\n",
      "Epoch 2439/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6537 - val_loss: 44.6896\n",
      "Epoch 2440/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6526 - val_loss: 44.6889\n",
      "Epoch 2441/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6513 - val_loss: 44.6899\n",
      "Epoch 2442/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6473 - val_loss: 44.6872\n",
      "Epoch 2443/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6440 - val_loss: 44.6822\n",
      "Epoch 2444/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6409 - val_loss: 44.6769\n",
      "Epoch 2445/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 47.6402 - val_loss: 44.6730\n",
      "Epoch 2446/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6390 - val_loss: 44.6694\n",
      "Epoch 2447/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6372 - val_loss: 44.6745\n",
      "Epoch 2448/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6344 - val_loss: 44.6768\n",
      "Epoch 2449/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6337 - val_loss: 44.6756\n",
      "Epoch 2450/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 47.6278 - val_loss: 44.6714\n",
      "Epoch 2451/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.6282 - val_loss: 44.6711\n",
      "Epoch 2452/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.6247 - val_loss: 44.6763\n",
      "Epoch 2453/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.6219 - val_loss: 44.6752\n",
      "Epoch 2454/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.6205 - val_loss: 44.6723\n",
      "Epoch 2455/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.6178 - val_loss: 44.6724\n",
      "Epoch 2456/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6154 - val_loss: 44.6711\n",
      "Epoch 02456: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model(32, X_train.shape[1], 'mean_squared_error', 'adagrad')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d2017a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d20116080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.title('Coût et coût de validation')\n",
    "line1,=plt.plot(history.history['loss'], label=\"Loss\", linestyle='-', color='r')\n",
    "line2,=plt.plot(history.history['val_loss'], label=\"Val loss\", linestyle='-', color='b')\n",
    "first_legend = plt.legend(handles=[line1, line2], loc=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.title('y_pred en fonction de y_test')\n",
    "\n",
    "plt.plot(y_pred[:], y_test[:], '+')\n",
    "plt.ylabel('Test')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deux couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                384       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 929\n",
      "Trainable params: 929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "def double_dense_model(dense_size, input_dim, loss='mean_squared_error', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_size, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(dense_size//2, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = double_dense_model(32, X_train.shape[1], 'mean_squared_error', 'adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1126 samples, validate on 563 samples\n",
      "Epoch 1/5000\n",
      "1126/1126 [==============================] - 0s 207us/step - loss: 513.6292 - val_loss: 341.9324\n",
      "Epoch 2/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 295.2430 - val_loss: 211.5845\n",
      "Epoch 3/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 206.2777 - val_loss: 176.1067\n",
      "Epoch 4/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 176.3227 - val_loss: 158.6451\n",
      "Epoch 5/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 158.2905 - val_loss: 144.5900\n",
      "Epoch 6/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 143.8037 - val_loss: 132.4243\n",
      "Epoch 7/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 132.1777 - val_loss: 122.8753\n",
      "Epoch 8/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 122.9299 - val_loss: 115.1075\n",
      "Epoch 9/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 115.2442 - val_loss: 108.3211\n",
      "Epoch 10/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 108.9292 - val_loss: 102.4313\n",
      "Epoch 11/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 103.4622 - val_loss: 97.9095\n",
      "Epoch 12/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 99.2027 - val_loss: 93.3950\n",
      "Epoch 13/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 95.2626 - val_loss: 89.5985\n",
      "Epoch 14/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 91.8403 - val_loss: 86.6827\n",
      "Epoch 15/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 88.9924 - val_loss: 84.0087\n",
      "Epoch 16/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 86.3843 - val_loss: 81.5524\n",
      "Epoch 17/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 84.2725 - val_loss: 79.4491\n",
      "Epoch 18/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 82.3225 - val_loss: 77.7460\n",
      "Epoch 19/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 80.7736 - val_loss: 75.9792\n",
      "Epoch 20/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 79.3615 - val_loss: 74.5155\n",
      "Epoch 21/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 78.0212 - val_loss: 73.5308\n",
      "Epoch 22/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 76.8713 - val_loss: 72.4260\n",
      "Epoch 23/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 75.9522 - val_loss: 71.1830\n",
      "Epoch 24/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 75.0874 - val_loss: 70.5131\n",
      "Epoch 25/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 74.3156 - val_loss: 69.7969\n",
      "Epoch 26/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 73.5485 - val_loss: 69.1781\n",
      "Epoch 27/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 72.9048 - val_loss: 68.2636\n",
      "Epoch 28/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 72.3016 - val_loss: 67.7679\n",
      "Epoch 29/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 71.6940 - val_loss: 67.1850\n",
      "Epoch 30/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 71.2022 - val_loss: 66.9348\n",
      "Epoch 31/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 70.8398 - val_loss: 66.4530\n",
      "Epoch 32/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 70.3871 - val_loss: 66.3953\n",
      "Epoch 33/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 70.1317 - val_loss: 65.8681\n",
      "Epoch 34/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 69.7682 - val_loss: 65.4470\n",
      "Epoch 35/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 69.4314 - val_loss: 65.1624\n",
      "Epoch 36/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 69.2105 - val_loss: 64.8755\n",
      "Epoch 37/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 68.9924 - val_loss: 64.5753\n",
      "Epoch 38/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 68.7579 - val_loss: 64.3138\n",
      "Epoch 39/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 68.5607 - val_loss: 64.1449\n",
      "Epoch 40/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 68.3742 - val_loss: 63.8724\n",
      "Epoch 41/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 68.1677 - val_loss: 63.7140\n",
      "Epoch 42/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 67.9699 - val_loss: 63.7477\n",
      "Epoch 43/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 67.8572 - val_loss: 63.5677\n",
      "Epoch 44/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 67.6486 - val_loss: 63.2473\n",
      "Epoch 45/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 67.4846 - val_loss: 63.2160\n",
      "Epoch 46/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 67.3486 - val_loss: 62.9265\n",
      "Epoch 47/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 67.2113 - val_loss: 62.6617\n",
      "Epoch 48/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 67.0354 - val_loss: 62.5539\n",
      "Epoch 49/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 66.9097 - val_loss: 62.4211\n",
      "Epoch 50/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 66.7959 - val_loss: 62.3419\n",
      "Epoch 51/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 66.7051 - val_loss: 62.1596\n",
      "Epoch 52/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 66.5698 - val_loss: 62.0656\n",
      "Epoch 53/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 66.5015 - val_loss: 61.9208\n",
      "Epoch 54/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 66.3365 - val_loss: 61.9290\n",
      "Epoch 55/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 66.2370 - val_loss: 61.6270\n",
      "Epoch 56/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 66.1755 - val_loss: 61.6620\n",
      "Epoch 57/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 66.0642 - val_loss: 61.5513\n",
      "Epoch 58/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 65.9701 - val_loss: 61.4860\n",
      "Epoch 59/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 65.8849 - val_loss: 61.4351\n",
      "Epoch 60/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.7897 - val_loss: 61.3559\n",
      "Epoch 61/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 65.7080 - val_loss: 61.3129\n",
      "Epoch 62/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.6273 - val_loss: 61.1132\n",
      "Epoch 63/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.5419 - val_loss: 61.0583\n",
      "Epoch 64/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.4619 - val_loss: 61.0487\n",
      "Epoch 65/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 65.3871 - val_loss: 60.9608\n",
      "Epoch 66/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 65.3809 - val_loss: 60.8011\n",
      "Epoch 67/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 65.2809 - val_loss: 60.7476\n",
      "Epoch 68/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 65.1746 - val_loss: 60.7093\n",
      "Epoch 69/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 65.1023 - val_loss: 60.7429\n",
      "Epoch 70/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 65.0400 - val_loss: 60.6124\n",
      "Epoch 71/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 64.9857 - val_loss: 60.4764\n",
      "Epoch 72/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.9039 - val_loss: 60.4559\n",
      "Epoch 73/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 64.8322 - val_loss: 60.2707\n",
      "Epoch 74/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 64.7566 - val_loss: 60.2608\n",
      "Epoch 75/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 64.6930 - val_loss: 60.1807\n",
      "Epoch 76/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.6312 - val_loss: 60.1173\n",
      "Epoch 77/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 64.5703 - val_loss: 60.0490\n",
      "Epoch 78/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 64.5081 - val_loss: 59.9945\n",
      "Epoch 79/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 64.4182 - val_loss: 59.8583\n",
      "Epoch 80/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.3868 - val_loss: 59.8559\n",
      "Epoch 81/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.3078 - val_loss: 59.9187\n",
      "Epoch 82/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.2551 - val_loss: 60.0155\n",
      "Epoch 83/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 64.2304 - val_loss: 59.8261\n",
      "Epoch 84/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 64.1259 - val_loss: 59.6807\n",
      "Epoch 85/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.0799 - val_loss: 59.6218\n",
      "Epoch 86/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 64.0083 - val_loss: 59.5912\n",
      "Epoch 87/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.9453 - val_loss: 59.4288\n",
      "Epoch 88/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.8881 - val_loss: 59.3784\n",
      "Epoch 89/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.8619 - val_loss: 59.3665\n",
      "Epoch 90/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.7871 - val_loss: 59.3374\n",
      "Epoch 91/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 63.7428 - val_loss: 59.2892\n",
      "Epoch 92/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 63.6868 - val_loss: 59.2336\n",
      "Epoch 93/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.6542 - val_loss: 59.1430\n",
      "Epoch 94/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.5930 - val_loss: 59.1045\n",
      "Epoch 95/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.5391 - val_loss: 59.0364\n",
      "Epoch 96/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.4836 - val_loss: 59.0380\n",
      "Epoch 97/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.4247 - val_loss: 58.9328\n",
      "Epoch 98/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.3899 - val_loss: 58.9345\n",
      "Epoch 99/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.3266 - val_loss: 58.9352\n",
      "Epoch 100/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.2902 - val_loss: 58.8842\n",
      "Epoch 101/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.2267 - val_loss: 58.8110\n",
      "Epoch 102/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 63.1884 - val_loss: 58.7443\n",
      "Epoch 103/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 63.1613 - val_loss: 58.6777\n",
      "Epoch 104/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.1248 - val_loss: 58.6067\n",
      "Epoch 105/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 63.0754 - val_loss: 58.6125\n",
      "Epoch 106/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.9849 - val_loss: 58.5144\n",
      "Epoch 107/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.9693 - val_loss: 58.4965\n",
      "Epoch 108/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.9038 - val_loss: 58.4965\n",
      "Epoch 109/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.8752 - val_loss: 58.4422\n",
      "Epoch 110/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.8489 - val_loss: 58.3427\n",
      "Epoch 111/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 62.7905 - val_loss: 58.3163\n",
      "Epoch 112/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.7438 - val_loss: 58.2777\n",
      "Epoch 113/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.6957 - val_loss: 58.2403\n",
      "Epoch 114/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.6477 - val_loss: 58.2581\n",
      "Epoch 115/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.6100 - val_loss: 58.2048\n",
      "Epoch 116/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 62.5785 - val_loss: 58.1011\n",
      "Epoch 117/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 62.5310 - val_loss: 58.0470\n",
      "Epoch 118/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 62.4785 - val_loss: 57.9892\n",
      "Epoch 119/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.4467 - val_loss: 57.9416\n",
      "Epoch 120/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.4295 - val_loss: 57.9375\n",
      "Epoch 121/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.3727 - val_loss: 57.8750\n",
      "Epoch 122/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.3016 - val_loss: 57.9497\n",
      "Epoch 123/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 62.2966 - val_loss: 57.8965\n",
      "Epoch 124/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.2729 - val_loss: 57.7611\n",
      "Epoch 125/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 62.2031 - val_loss: 57.7456\n",
      "Epoch 126/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.1437 - val_loss: 57.7982\n",
      "Epoch 127/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.1270 - val_loss: 57.6061\n",
      "Epoch 128/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 62.0508 - val_loss: 57.6119\n",
      "Epoch 129/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 62.0377 - val_loss: 57.5282\n",
      "Epoch 130/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 61.9978 - val_loss: 57.5164\n",
      "Epoch 131/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.9027 - val_loss: 57.4857\n",
      "Epoch 132/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.8570 - val_loss: 57.3728\n",
      "Epoch 133/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.7806 - val_loss: 57.3134\n",
      "Epoch 134/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 61.7333 - val_loss: 57.2543\n",
      "Epoch 135/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.7039 - val_loss: 57.3091\n",
      "Epoch 136/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 61.6639 - val_loss: 57.2368\n",
      "Epoch 137/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 61.6001 - val_loss: 57.1417\n",
      "Epoch 138/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.5432 - val_loss: 57.2111\n",
      "Epoch 139/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.5084 - val_loss: 57.1596\n",
      "Epoch 140/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.4667 - val_loss: 57.0236\n",
      "Epoch 141/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.3895 - val_loss: 56.9434\n",
      "Epoch 142/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.3278 - val_loss: 56.8009\n",
      "Epoch 143/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.3010 - val_loss: 56.7668\n",
      "Epoch 144/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.2385 - val_loss: 56.7241\n",
      "Epoch 145/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.1783 - val_loss: 56.7300\n",
      "Epoch 146/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.1301 - val_loss: 56.6263\n",
      "Epoch 147/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 61.0621 - val_loss: 56.5671\n",
      "Epoch 148/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 61.0411 - val_loss: 56.5724\n",
      "Epoch 149/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.9631 - val_loss: 56.4987\n",
      "Epoch 150/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.9282 - val_loss: 56.4145\n",
      "Epoch 151/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.8773 - val_loss: 56.3696\n",
      "Epoch 152/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 60.8430 - val_loss: 56.4268\n",
      "Epoch 153/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.7679 - val_loss: 56.3559\n",
      "Epoch 154/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.7316 - val_loss: 56.2758\n",
      "Epoch 155/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.6636 - val_loss: 56.2209\n",
      "Epoch 156/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.6378 - val_loss: 56.3374\n",
      "Epoch 157/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 60.5912 - val_loss: 56.4143\n",
      "Epoch 158/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.5888 - val_loss: 56.2622\n",
      "Epoch 159/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.4960 - val_loss: 56.1649\n",
      "Epoch 160/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.4323 - val_loss: 56.1592\n",
      "Epoch 161/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 60.3832 - val_loss: 56.0218\n",
      "Epoch 162/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.3564 - val_loss: 55.9424\n",
      "Epoch 163/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 60.2981 - val_loss: 55.8689\n",
      "Epoch 164/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 60.2647 - val_loss: 56.0085\n",
      "Epoch 165/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 60.2515 - val_loss: 55.8919\n",
      "Epoch 166/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 60.1714 - val_loss: 55.8095\n",
      "Epoch 167/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 60.1236 - val_loss: 55.7592\n",
      "Epoch 168/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 60.1005 - val_loss: 55.7273\n",
      "Epoch 169/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 60.0291 - val_loss: 55.6724\n",
      "Epoch 170/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 59.9829 - val_loss: 55.6990\n",
      "Epoch 171/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 59.9635 - val_loss: 55.6253\n",
      "Epoch 172/5000\n",
      "1126/1126 [==============================] - 0s 46us/step - loss: 59.8936 - val_loss: 55.5279\n",
      "Epoch 173/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 59.8383 - val_loss: 55.5251\n",
      "Epoch 174/5000\n",
      "1126/1126 [==============================] - 0s 43us/step - loss: 59.8115 - val_loss: 55.5517\n",
      "Epoch 175/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 59.7634 - val_loss: 55.4369\n",
      "Epoch 176/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 59.7117 - val_loss: 55.3511\n",
      "Epoch 177/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 59.6942 - val_loss: 55.2590\n",
      "Epoch 178/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 59.6323 - val_loss: 55.2007\n",
      "Epoch 179/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 59.6070 - val_loss: 55.2453\n",
      "Epoch 180/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 59.5459 - val_loss: 55.2091\n",
      "Epoch 181/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.5188 - val_loss: 55.1818\n",
      "Epoch 182/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.4885 - val_loss: 55.2002\n",
      "Epoch 183/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.4365 - val_loss: 55.1488\n",
      "Epoch 184/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 59.3788 - val_loss: 54.9819\n",
      "Epoch 185/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.3571 - val_loss: 54.9831\n",
      "Epoch 186/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.2850 - val_loss: 54.9642\n",
      "Epoch 187/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.2586 - val_loss: 54.8948\n",
      "Epoch 188/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.2068 - val_loss: 54.8124\n",
      "Epoch 189/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.1737 - val_loss: 54.7423\n",
      "Epoch 190/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 59.1227 - val_loss: 54.7277\n",
      "Epoch 191/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.1021 - val_loss: 54.6820\n",
      "Epoch 192/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 59.0757 - val_loss: 54.6150\n",
      "Epoch 193/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 59.0041 - val_loss: 54.6813\n",
      "Epoch 194/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.9663 - val_loss: 54.5744\n",
      "Epoch 195/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.9181 - val_loss: 54.5401\n",
      "Epoch 196/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 58.8793 - val_loss: 54.5071\n",
      "Epoch 197/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.8540 - val_loss: 54.4620\n",
      "Epoch 198/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 58.7944 - val_loss: 54.4875\n",
      "Epoch 199/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 58.7386 - val_loss: 54.4703\n",
      "Epoch 200/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 58.7188 - val_loss: 54.4254\n",
      "Epoch 201/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 58.6602 - val_loss: 54.3769\n",
      "Epoch 202/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.6358 - val_loss: 54.3017\n",
      "Epoch 203/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 58.6110 - val_loss: 54.2322\n",
      "Epoch 204/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 58.5352 - val_loss: 54.1830\n",
      "Epoch 205/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 58.5250 - val_loss: 54.1315\n",
      "Epoch 206/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.4711 - val_loss: 54.1157\n",
      "Epoch 207/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 58.4207 - val_loss: 54.0802\n",
      "Epoch 208/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 58.3787 - val_loss: 54.0548\n",
      "Epoch 209/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.3546 - val_loss: 53.9771\n",
      "Epoch 210/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.3145 - val_loss: 53.9445\n",
      "Epoch 211/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 58.2707 - val_loss: 53.9642\n",
      "Epoch 212/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 58.2314 - val_loss: 53.9314\n",
      "Epoch 213/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 58.1719 - val_loss: 53.8286\n",
      "Epoch 214/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 58.1579 - val_loss: 53.7916\n",
      "Epoch 215/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 58.1065 - val_loss: 53.7707\n",
      "Epoch 216/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 58.0733 - val_loss: 53.7312\n",
      "Epoch 217/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 58.0128 - val_loss: 53.6607\n",
      "Epoch 218/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.9947 - val_loss: 53.6492\n",
      "Epoch 219/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 57.9499 - val_loss: 53.6242\n",
      "Epoch 220/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.8854 - val_loss: 53.5407\n",
      "Epoch 221/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.8525 - val_loss: 53.4928\n",
      "Epoch 222/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.8159 - val_loss: 53.4411\n",
      "Epoch 223/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.7744 - val_loss: 53.4215\n",
      "Epoch 224/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.7301 - val_loss: 53.4249\n",
      "Epoch 225/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.6924 - val_loss: 53.3718\n",
      "Epoch 226/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.6364 - val_loss: 53.2719\n",
      "Epoch 227/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 57.6022 - val_loss: 53.2587\n",
      "Epoch 228/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 57.5648 - val_loss: 53.1285\n",
      "Epoch 229/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.5330 - val_loss: 53.1472\n",
      "Epoch 230/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.5085 - val_loss: 53.1080\n",
      "Epoch 231/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 57.4474 - val_loss: 53.1170\n",
      "Epoch 232/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.4102 - val_loss: 53.1529\n",
      "Epoch 233/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.3859 - val_loss: 53.0218\n",
      "Epoch 234/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 57.3198 - val_loss: 52.9837\n",
      "Epoch 235/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 57.2859 - val_loss: 52.9970\n",
      "Epoch 236/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.2465 - val_loss: 52.9413\n",
      "Epoch 237/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.2140 - val_loss: 52.8892\n",
      "Epoch 238/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 57.1659 - val_loss: 52.8635\n",
      "Epoch 239/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 57.1428 - val_loss: 52.9280\n",
      "Epoch 240/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 57.0702 - val_loss: 52.9368\n",
      "Epoch 241/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 57.0374 - val_loss: 52.8275\n",
      "Epoch 242/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 57.0131 - val_loss: 52.7700\n",
      "Epoch 243/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.9689 - val_loss: 52.7107\n",
      "Epoch 244/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.9409 - val_loss: 52.7413\n",
      "Epoch 245/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.8785 - val_loss: 52.5983\n",
      "Epoch 246/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.8360 - val_loss: 52.4955\n",
      "Epoch 247/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.8009 - val_loss: 52.4619\n",
      "Epoch 248/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.7713 - val_loss: 52.3955\n",
      "Epoch 249/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.7313 - val_loss: 52.4676\n",
      "Epoch 250/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.6915 - val_loss: 52.3999\n",
      "Epoch 251/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 56.6593 - val_loss: 52.3746\n",
      "Epoch 252/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.6142 - val_loss: 52.2907\n",
      "Epoch 253/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 56.5662 - val_loss: 52.2838\n",
      "Epoch 254/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.5213 - val_loss: 52.2631\n",
      "Epoch 255/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.5066 - val_loss: 52.2730\n",
      "Epoch 256/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 56.4739 - val_loss: 52.2490\n",
      "Epoch 257/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 56.4209 - val_loss: 52.1790\n",
      "Epoch 258/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.3841 - val_loss: 52.1285\n",
      "Epoch 259/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.3405 - val_loss: 52.1920\n",
      "Epoch 260/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.3062 - val_loss: 52.1461\n",
      "Epoch 261/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.2630 - val_loss: 52.0479\n",
      "Epoch 262/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.2154 - val_loss: 51.9818\n",
      "Epoch 263/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.1820 - val_loss: 51.9963\n",
      "Epoch 264/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.1482 - val_loss: 51.9937\n",
      "Epoch 265/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 56.1158 - val_loss: 51.9139\n",
      "Epoch 266/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 56.0622 - val_loss: 51.8351\n",
      "Epoch 267/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 56.0264 - val_loss: 51.7550\n",
      "Epoch 268/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.9934 - val_loss: 51.8378\n",
      "Epoch 269/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.9425 - val_loss: 51.6862\n",
      "Epoch 270/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.9137 - val_loss: 51.6780\n",
      "Epoch 271/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.8756 - val_loss: 51.6528\n",
      "Epoch 272/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.8313 - val_loss: 51.7554\n",
      "Epoch 273/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.7958 - val_loss: 51.6276\n",
      "Epoch 274/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.7714 - val_loss: 51.5969\n",
      "Epoch 275/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.7274 - val_loss: 51.5963\n",
      "Epoch 276/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 55.7055 - val_loss: 51.5446\n",
      "Epoch 277/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.6510 - val_loss: 51.5118\n",
      "Epoch 278/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 55.6169 - val_loss: 51.5591\n",
      "Epoch 279/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.5907 - val_loss: 51.4904\n",
      "Epoch 280/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.5358 - val_loss: 51.4409\n",
      "Epoch 281/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.5220 - val_loss: 51.3883\n",
      "Epoch 282/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.4648 - val_loss: 51.3700\n",
      "Epoch 283/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.4360 - val_loss: 51.2999\n",
      "Epoch 284/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.3845 - val_loss: 51.2422\n",
      "Epoch 285/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.3648 - val_loss: 51.1809\n",
      "Epoch 286/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.3120 - val_loss: 51.1692\n",
      "Epoch 287/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 55.2930 - val_loss: 51.1333\n",
      "Epoch 288/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 55.2539 - val_loss: 51.0747\n",
      "Epoch 289/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 55.2136 - val_loss: 51.0524\n",
      "Epoch 290/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.1979 - val_loss: 50.9677\n",
      "Epoch 291/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.1288 - val_loss: 50.9390\n",
      "Epoch 292/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.0943 - val_loss: 50.9284\n",
      "Epoch 293/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.0702 - val_loss: 50.9006\n",
      "Epoch 294/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 55.0214 - val_loss: 50.8129\n",
      "Epoch 295/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.9783 - val_loss: 50.8351\n",
      "Epoch 296/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.9535 - val_loss: 50.7647\n",
      "Epoch 297/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.9122 - val_loss: 50.7258\n",
      "Epoch 298/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.8813 - val_loss: 50.8472\n",
      "Epoch 299/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.8422 - val_loss: 50.7988\n",
      "Epoch 300/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.7893 - val_loss: 50.6839\n",
      "Epoch 301/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.7586 - val_loss: 50.6413\n",
      "Epoch 302/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.7261 - val_loss: 50.6217\n",
      "Epoch 303/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.6836 - val_loss: 50.5388\n",
      "Epoch 304/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.6553 - val_loss: 50.5238\n",
      "Epoch 305/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.6137 - val_loss: 50.5770\n",
      "Epoch 306/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.5636 - val_loss: 50.5204\n",
      "Epoch 307/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.5529 - val_loss: 50.5731\n",
      "Epoch 308/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.4979 - val_loss: 50.4289\n",
      "Epoch 309/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.4688 - val_loss: 50.4007\n",
      "Epoch 310/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.4231 - val_loss: 50.3757\n",
      "Epoch 311/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.3916 - val_loss: 50.3944\n",
      "Epoch 312/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.3695 - val_loss: 50.3311\n",
      "Epoch 313/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.3304 - val_loss: 50.3154\n",
      "Epoch 314/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.3144 - val_loss: 50.2406\n",
      "Epoch 315/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 54.2664 - val_loss: 50.1873\n",
      "Epoch 316/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 54.2080 - val_loss: 50.2693\n",
      "Epoch 317/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.1859 - val_loss: 50.2606\n",
      "Epoch 318/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 54.1321 - val_loss: 50.1582\n",
      "Epoch 319/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 54.1066 - val_loss: 50.1423\n",
      "Epoch 320/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 54.0797 - val_loss: 50.0392\n",
      "Epoch 321/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 54.0471 - val_loss: 49.9879\n",
      "Epoch 322/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 54.0057 - val_loss: 49.9579\n",
      "Epoch 323/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.9589 - val_loss: 49.9212\n",
      "Epoch 324/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.9258 - val_loss: 49.9506\n",
      "Epoch 325/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.8817 - val_loss: 49.8863\n",
      "Epoch 326/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 53.8630 - val_loss: 49.8806\n",
      "Epoch 327/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.8181 - val_loss: 49.8687\n",
      "Epoch 328/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7810 - val_loss: 49.8783\n",
      "Epoch 329/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7524 - val_loss: 49.8101\n",
      "Epoch 330/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 53.7267 - val_loss: 49.7968\n",
      "Epoch 331/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.6711 - val_loss: 49.7411\n",
      "Epoch 332/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 53.6443 - val_loss: 49.7690\n",
      "Epoch 333/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 53.6176 - val_loss: 49.6747\n",
      "Epoch 334/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 53.5825 - val_loss: 49.7136\n",
      "Epoch 335/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 53.5449 - val_loss: 49.6700\n",
      "Epoch 336/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 53.5137 - val_loss: 49.6376\n",
      "Epoch 337/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4549 - val_loss: 49.6278\n",
      "Epoch 338/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4329 - val_loss: 49.5865\n",
      "Epoch 339/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.4081 - val_loss: 49.5482\n",
      "Epoch 340/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 53.3637 - val_loss: 49.5641\n",
      "Epoch 341/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.3263 - val_loss: 49.4846\n",
      "Epoch 342/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.3056 - val_loss: 49.4516\n",
      "Epoch 343/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.2493 - val_loss: 49.4016\n",
      "Epoch 344/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 53.2202 - val_loss: 49.3871\n",
      "Epoch 345/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 53.1926 - val_loss: 49.3597\n",
      "Epoch 346/5000\n",
      "1126/1126 [==============================] - 0s 45us/step - loss: 53.1499 - val_loss: 49.4384\n",
      "Epoch 347/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 53.1467 - val_loss: 49.3698\n",
      "Epoch 348/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.0837 - val_loss: 49.3156\n",
      "Epoch 349/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 53.0599 - val_loss: 49.3152\n",
      "Epoch 350/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 53.0407 - val_loss: 49.2300\n",
      "Epoch 351/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.9867 - val_loss: 49.2211\n",
      "Epoch 352/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.9510 - val_loss: 49.2602\n",
      "Epoch 353/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.9252 - val_loss: 49.2210\n",
      "Epoch 354/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.8880 - val_loss: 49.1742\n",
      "Epoch 355/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 52.8531 - val_loss: 49.1197\n",
      "Epoch 356/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 52.8031 - val_loss: 49.0397\n",
      "Epoch 357/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.7807 - val_loss: 49.0050\n",
      "Epoch 358/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.7715 - val_loss: 48.9374\n",
      "Epoch 359/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.7139 - val_loss: 48.9124\n",
      "Epoch 360/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.6862 - val_loss: 48.9644\n",
      "Epoch 361/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.6389 - val_loss: 48.8850\n",
      "Epoch 362/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 52.6129 - val_loss: 48.8606\n",
      "Epoch 363/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 52.5895 - val_loss: 48.8890\n",
      "Epoch 364/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 52.5562 - val_loss: 48.8812\n",
      "Epoch 365/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 52.4946 - val_loss: 48.8119\n",
      "Epoch 366/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 52.4714 - val_loss: 48.7925\n",
      "Epoch 367/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 52.4345 - val_loss: 48.7575\n",
      "Epoch 368/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 52.4076 - val_loss: 48.7373\n",
      "Epoch 369/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 52.3741 - val_loss: 48.8059\n",
      "Epoch 370/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 52.3501 - val_loss: 48.7260\n",
      "Epoch 371/5000\n",
      "1126/1126 [==============================] - 0s 44us/step - loss: 52.3175 - val_loss: 48.6970\n",
      "Epoch 372/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.2932 - val_loss: 48.6351\n",
      "Epoch 373/5000\n",
      "1126/1126 [==============================] - 0s 30us/step - loss: 52.2296 - val_loss: 48.5837\n",
      "Epoch 374/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 52.2164 - val_loss: 48.5353\n",
      "Epoch 375/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 52.1905 - val_loss: 48.4679\n",
      "Epoch 376/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 52.1676 - val_loss: 48.5221\n",
      "Epoch 377/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 52.1160 - val_loss: 48.5035\n",
      "Epoch 378/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 52.1025 - val_loss: 48.4723\n",
      "Epoch 379/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 52.0494 - val_loss: 48.4274\n",
      "Epoch 380/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 52.0182 - val_loss: 48.3803\n",
      "Epoch 381/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.9879 - val_loss: 48.3919\n",
      "Epoch 382/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.9697 - val_loss: 48.3563\n",
      "Epoch 383/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.9426 - val_loss: 48.2957\n",
      "Epoch 384/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.9003 - val_loss: 48.2588\n",
      "Epoch 385/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.8745 - val_loss: 48.2623\n",
      "Epoch 386/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.8381 - val_loss: 48.2761\n",
      "Epoch 387/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.7938 - val_loss: 48.2095\n",
      "Epoch 388/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.7779 - val_loss: 48.1854\n",
      "Epoch 389/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.7354 - val_loss: 48.1732\n",
      "Epoch 390/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.7155 - val_loss: 48.1440\n",
      "Epoch 391/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.6739 - val_loss: 48.1514\n",
      "Epoch 392/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 51.6532 - val_loss: 48.1518\n",
      "Epoch 393/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.6145 - val_loss: 48.0601\n",
      "Epoch 394/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 51.5903 - val_loss: 48.1582\n",
      "Epoch 395/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.5516 - val_loss: 48.1155\n",
      "Epoch 396/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.5252 - val_loss: 48.0859\n",
      "Epoch 397/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 51.4794 - val_loss: 48.1210\n",
      "Epoch 398/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.4709 - val_loss: 47.9990\n",
      "Epoch 399/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 51.4316 - val_loss: 47.9781\n",
      "Epoch 400/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.3997 - val_loss: 47.9119\n",
      "Epoch 401/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 51.3634 - val_loss: 47.9035\n",
      "Epoch 402/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.3339 - val_loss: 47.8364\n",
      "Epoch 403/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 51.3137 - val_loss: 47.8220\n",
      "Epoch 404/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.2705 - val_loss: 47.8122\n",
      "Epoch 405/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 51.2637 - val_loss: 47.7980\n",
      "Epoch 406/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.2148 - val_loss: 47.7748\n",
      "Epoch 407/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.1866 - val_loss: 47.7756\n",
      "Epoch 408/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 51.1446 - val_loss: 47.7517\n",
      "Epoch 409/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 51.1318 - val_loss: 47.6849\n",
      "Epoch 410/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 51.0995 - val_loss: 47.6448\n",
      "Epoch 411/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0633 - val_loss: 47.6176\n",
      "Epoch 412/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 51.0362 - val_loss: 47.6534\n",
      "Epoch 413/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 51.0114 - val_loss: 47.7122\n",
      "Epoch 414/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.9733 - val_loss: 47.6592\n",
      "Epoch 415/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.9441 - val_loss: 47.6334\n",
      "Epoch 416/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.9161 - val_loss: 47.5862\n",
      "Epoch 417/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.8830 - val_loss: 47.5883\n",
      "Epoch 418/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.8688 - val_loss: 47.5425\n",
      "Epoch 419/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.8357 - val_loss: 47.5741\n",
      "Epoch 420/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.7969 - val_loss: 47.5224\n",
      "Epoch 421/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 50.7673 - val_loss: 47.5084\n",
      "Epoch 422/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.7384 - val_loss: 47.4761\n",
      "Epoch 423/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.7225 - val_loss: 47.4578\n",
      "Epoch 424/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6925 - val_loss: 47.4171\n",
      "Epoch 425/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6668 - val_loss: 47.4301\n",
      "Epoch 426/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.6414 - val_loss: 47.4314\n",
      "Epoch 427/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.6142 - val_loss: 47.3955\n",
      "Epoch 428/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 50.5894 - val_loss: 47.3256\n",
      "Epoch 429/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.5449 - val_loss: 47.2921\n",
      "Epoch 430/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.5158 - val_loss: 47.2492\n",
      "Epoch 431/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.5099 - val_loss: 47.2174\n",
      "Epoch 432/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.4712 - val_loss: 47.2147\n",
      "Epoch 433/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.4535 - val_loss: 47.1850\n",
      "Epoch 434/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.4164 - val_loss: 47.1729\n",
      "Epoch 435/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3935 - val_loss: 47.1191\n",
      "Epoch 436/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.3634 - val_loss: 47.0881\n",
      "Epoch 437/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.3484 - val_loss: 47.1539\n",
      "Epoch 438/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2962 - val_loss: 47.1466\n",
      "Epoch 439/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2849 - val_loss: 47.1183\n",
      "Epoch 440/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.2619 - val_loss: 47.0719\n",
      "Epoch 441/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 50.2211 - val_loss: 47.0519\n",
      "Epoch 442/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 50.1944 - val_loss: 47.0157\n",
      "Epoch 443/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1759 - val_loss: 47.0091\n",
      "Epoch 444/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.1555 - val_loss: 47.0262\n",
      "Epoch 445/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.1196 - val_loss: 46.9545\n",
      "Epoch 446/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 50.0788 - val_loss: 46.9843\n",
      "Epoch 447/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0473 - val_loss: 46.8814\n",
      "Epoch 448/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 50.0460 - val_loss: 47.0102\n",
      "Epoch 449/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 50.0104 - val_loss: 46.9416\n",
      "Epoch 450/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.9863 - val_loss: 46.9118\n",
      "Epoch 451/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.9476 - val_loss: 46.9341\n",
      "Epoch 452/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9404 - val_loss: 46.8697\n",
      "Epoch 453/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.9112 - val_loss: 46.8587\n",
      "Epoch 454/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8684 - val_loss: 46.8556\n",
      "Epoch 455/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.8358 - val_loss: 46.8448\n",
      "Epoch 456/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.8242 - val_loss: 46.8059\n",
      "Epoch 457/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.7847 - val_loss: 46.7566\n",
      "Epoch 458/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.7707 - val_loss: 46.7649\n",
      "Epoch 459/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7401 - val_loss: 46.7393\n",
      "Epoch 460/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.7105 - val_loss: 46.7039\n",
      "Epoch 461/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.6876 - val_loss: 46.6939\n",
      "Epoch 462/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.6588 - val_loss: 46.7223\n",
      "Epoch 463/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6288 - val_loss: 46.6697\n",
      "Epoch 464/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.6135 - val_loss: 46.6441\n",
      "Epoch 465/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5854 - val_loss: 46.6605\n",
      "Epoch 466/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.5500 - val_loss: 46.5961\n",
      "Epoch 467/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.5588 - val_loss: 46.5611\n",
      "Epoch 468/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.5204 - val_loss: 46.5644\n",
      "Epoch 469/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4731 - val_loss: 46.5565\n",
      "Epoch 470/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.4550 - val_loss: 46.5274\n",
      "Epoch 471/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.4385 - val_loss: 46.5684\n",
      "Epoch 472/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 49.4232 - val_loss: 46.5326\n",
      "Epoch 473/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.3967 - val_loss: 46.5469\n",
      "Epoch 474/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3729 - val_loss: 46.4779\n",
      "Epoch 475/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.3373 - val_loss: 46.4753\n",
      "Epoch 476/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 49.2937 - val_loss: 46.3763\n",
      "Epoch 477/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2843 - val_loss: 46.3713\n",
      "Epoch 478/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.2530 - val_loss: 46.3083\n",
      "Epoch 479/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2213 - val_loss: 46.3628\n",
      "Epoch 480/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.2208 - val_loss: 46.3235\n",
      "Epoch 481/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.1797 - val_loss: 46.3491\n",
      "Epoch 482/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 49.1643 - val_loss: 46.2984\n",
      "Epoch 483/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1330 - val_loss: 46.2944\n",
      "Epoch 484/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.1185 - val_loss: 46.2520\n",
      "Epoch 485/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 49.0849 - val_loss: 46.2223\n",
      "Epoch 486/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.0398 - val_loss: 46.1738\n",
      "Epoch 487/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 49.0265 - val_loss: 46.1160\n",
      "Epoch 488/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 49.0046 - val_loss: 46.1258\n",
      "Epoch 489/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 48.9843 - val_loss: 46.0420\n",
      "Epoch 490/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 48.9571 - val_loss: 46.0408\n",
      "Epoch 491/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 48.9352 - val_loss: 46.0315\n",
      "Epoch 492/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 48.9006 - val_loss: 46.0482\n",
      "Epoch 493/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.8949 - val_loss: 46.0630\n",
      "Epoch 494/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.8697 - val_loss: 46.0418\n",
      "Epoch 495/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 48.8390 - val_loss: 46.0283\n",
      "Epoch 496/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.8194 - val_loss: 46.0037\n",
      "Epoch 497/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 48.7950 - val_loss: 46.0269\n",
      "Epoch 498/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7790 - val_loss: 45.9628\n",
      "Epoch 499/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7385 - val_loss: 45.9438\n",
      "Epoch 500/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.7039 - val_loss: 45.9423\n",
      "Epoch 501/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.7035 - val_loss: 45.9184\n",
      "Epoch 502/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6765 - val_loss: 45.9509\n",
      "Epoch 503/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.6527 - val_loss: 45.9525\n",
      "Epoch 504/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.6124 - val_loss: 45.9188\n",
      "Epoch 505/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5983 - val_loss: 45.9618\n",
      "Epoch 506/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5814 - val_loss: 45.8191\n",
      "Epoch 507/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.5500 - val_loss: 45.8507\n",
      "Epoch 508/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.5353 - val_loss: 45.8379\n",
      "Epoch 509/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.5149 - val_loss: 45.8264\n",
      "Epoch 510/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.4795 - val_loss: 45.8208\n",
      "Epoch 511/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.4646 - val_loss: 45.8355\n",
      "Epoch 512/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.4550 - val_loss: 45.8344\n",
      "Epoch 513/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.4211 - val_loss: 45.7846\n",
      "Epoch 514/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.3953 - val_loss: 45.7591\n",
      "Epoch 515/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.3757 - val_loss: 45.7102\n",
      "Epoch 516/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.3551 - val_loss: 45.6818\n",
      "Epoch 517/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 48.3398 - val_loss: 45.6689\n",
      "Epoch 518/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2868 - val_loss: 45.6433\n",
      "Epoch 519/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2704 - val_loss: 45.6052\n",
      "Epoch 520/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2687 - val_loss: 45.6518\n",
      "Epoch 521/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.2346 - val_loss: 45.6813\n",
      "Epoch 522/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 48.2236 - val_loss: 45.6312\n",
      "Epoch 523/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 48.1977 - val_loss: 45.6203\n",
      "Epoch 524/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.1835 - val_loss: 45.5408\n",
      "Epoch 525/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 48.1494 - val_loss: 45.5156\n",
      "Epoch 526/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 48.1310 - val_loss: 45.5053\n",
      "Epoch 527/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.1051 - val_loss: 45.5490\n",
      "Epoch 528/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.0867 - val_loss: 45.4937\n",
      "Epoch 529/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.0531 - val_loss: 45.5473\n",
      "Epoch 530/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.0364 - val_loss: 45.5135\n",
      "Epoch 531/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 48.0140 - val_loss: 45.4620\n",
      "Epoch 532/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 48.0063 - val_loss: 45.4106\n",
      "Epoch 533/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.9828 - val_loss: 45.3815\n",
      "Epoch 534/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.9502 - val_loss: 45.3441\n",
      "Epoch 535/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.9432 - val_loss: 45.3290\n",
      "Epoch 536/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.9178 - val_loss: 45.3100\n",
      "Epoch 537/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.9071 - val_loss: 45.3040\n",
      "Epoch 538/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.8704 - val_loss: 45.2456\n",
      "Epoch 539/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.8568 - val_loss: 45.2642\n",
      "Epoch 540/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.8466 - val_loss: 45.2857\n",
      "Epoch 541/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.8170 - val_loss: 45.2736\n",
      "Epoch 542/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7835 - val_loss: 45.2748\n",
      "Epoch 543/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7693 - val_loss: 45.2621\n",
      "Epoch 544/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7554 - val_loss: 45.2648\n",
      "Epoch 545/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.7244 - val_loss: 45.2280\n",
      "Epoch 546/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.6969 - val_loss: 45.2678\n",
      "Epoch 547/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.6914 - val_loss: 45.2233\n",
      "Epoch 548/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6846 - val_loss: 45.1561\n",
      "Epoch 549/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.6461 - val_loss: 45.1457\n",
      "Epoch 550/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.6468 - val_loss: 45.1604\n",
      "Epoch 551/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.6208 - val_loss: 45.1981\n",
      "Epoch 552/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.5884 - val_loss: 45.1443\n",
      "Epoch 553/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.5626 - val_loss: 45.0764\n",
      "Epoch 554/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.5422 - val_loss: 45.0655\n",
      "Epoch 555/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.5223 - val_loss: 45.0683\n",
      "Epoch 556/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.5075 - val_loss: 45.0694\n",
      "Epoch 557/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.4936 - val_loss: 45.0024\n",
      "Epoch 558/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.4718 - val_loss: 45.0603\n",
      "Epoch 559/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.4601 - val_loss: 44.9633\n",
      "Epoch 560/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.4340 - val_loss: 44.9831\n",
      "Epoch 561/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.4165 - val_loss: 44.9554\n",
      "Epoch 562/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.4062 - val_loss: 44.9526\n",
      "Epoch 563/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.3722 - val_loss: 44.9338\n",
      "Epoch 564/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.3497 - val_loss: 44.8961\n",
      "Epoch 565/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.3313 - val_loss: 44.9187\n",
      "Epoch 566/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.3225 - val_loss: 44.9274\n",
      "Epoch 567/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.3094 - val_loss: 44.9178\n",
      "Epoch 568/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.3007 - val_loss: 44.8539\n",
      "Epoch 569/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.2705 - val_loss: 44.8094\n",
      "Epoch 570/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.2507 - val_loss: 44.7522\n",
      "Epoch 571/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.2322 - val_loss: 44.7985\n",
      "Epoch 572/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.2033 - val_loss: 44.8116\n",
      "Epoch 573/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.1927 - val_loss: 44.8401\n",
      "Epoch 574/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.1791 - val_loss: 44.7910\n",
      "Epoch 575/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.1569 - val_loss: 44.8014\n",
      "Epoch 576/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.1301 - val_loss: 44.7550\n",
      "Epoch 577/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 47.1172 - val_loss: 44.7317\n",
      "Epoch 578/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 47.0977 - val_loss: 44.7528\n",
      "Epoch 579/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.0886 - val_loss: 44.7174\n",
      "Epoch 580/5000\n",
      "1126/1126 [==============================] - 0s 43us/step - loss: 47.0680 - val_loss: 44.6863\n",
      "Epoch 581/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 47.0471 - val_loss: 44.6358\n",
      "Epoch 582/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 47.0362 - val_loss: 44.6550\n",
      "Epoch 583/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 47.0131 - val_loss: 44.6475\n",
      "Epoch 584/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.9903 - val_loss: 44.6365\n",
      "Epoch 585/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.9715 - val_loss: 44.6526\n",
      "Epoch 586/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.9701 - val_loss: 44.6679\n",
      "Epoch 587/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.9401 - val_loss: 44.6424\n",
      "Epoch 588/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 46.9286 - val_loss: 44.5435\n",
      "Epoch 589/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 46.8964 - val_loss: 44.5166\n",
      "Epoch 590/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.8820 - val_loss: 44.5644\n",
      "Epoch 591/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.8796 - val_loss: 44.5460\n",
      "Epoch 592/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.8423 - val_loss: 44.5057\n",
      "Epoch 593/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.8276 - val_loss: 44.4888\n",
      "Epoch 594/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.8032 - val_loss: 44.5056\n",
      "Epoch 595/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.7859 - val_loss: 44.4599\n",
      "Epoch 596/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.7783 - val_loss: 44.4633\n",
      "Epoch 597/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.7680 - val_loss: 44.4133\n",
      "Epoch 598/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.7399 - val_loss: 44.4430\n",
      "Epoch 599/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 46.7246 - val_loss: 44.4360\n",
      "Epoch 600/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 46.7073 - val_loss: 44.4485\n",
      "Epoch 601/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 37us/step - loss: 46.6887 - val_loss: 44.4417\n",
      "Epoch 602/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.6757 - val_loss: 44.4596\n",
      "Epoch 603/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.6524 - val_loss: 44.4059\n",
      "Epoch 604/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.6377 - val_loss: 44.3366\n",
      "Epoch 605/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.6247 - val_loss: 44.3537\n",
      "Epoch 606/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.5994 - val_loss: 44.3182\n",
      "Epoch 607/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 46.5904 - val_loss: 44.4019\n",
      "Epoch 608/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.5829 - val_loss: 44.4054\n",
      "Epoch 609/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.5631 - val_loss: 44.3476\n",
      "Epoch 610/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.5378 - val_loss: 44.3206\n",
      "Epoch 611/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 46.5200 - val_loss: 44.3281\n",
      "Epoch 612/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.5075 - val_loss: 44.2777\n",
      "Epoch 613/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.4807 - val_loss: 44.2563\n",
      "Epoch 614/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.4804 - val_loss: 44.2212\n",
      "Epoch 615/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.4475 - val_loss: 44.2325\n",
      "Epoch 616/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.4241 - val_loss: 44.2287\n",
      "Epoch 617/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 46.4221 - val_loss: 44.2019\n",
      "Epoch 618/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.4014 - val_loss: 44.1759\n",
      "Epoch 619/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.4006 - val_loss: 44.1562\n",
      "Epoch 620/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 46.3771 - val_loss: 44.1682\n",
      "Epoch 621/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.3554 - val_loss: 44.1741\n",
      "Epoch 622/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.3416 - val_loss: 44.2468\n",
      "Epoch 623/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.3264 - val_loss: 44.2027\n",
      "Epoch 624/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.3072 - val_loss: 44.1801\n",
      "Epoch 625/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.2940 - val_loss: 44.1358\n",
      "Epoch 626/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 46.2749 - val_loss: 44.1877\n",
      "Epoch 627/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.2620 - val_loss: 44.1353\n",
      "Epoch 628/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 46.2424 - val_loss: 44.1017\n",
      "Epoch 629/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 46.2319 - val_loss: 44.0688\n",
      "Epoch 630/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.2208 - val_loss: 44.0849\n",
      "Epoch 631/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.1919 - val_loss: 44.0569\n",
      "Epoch 632/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.1781 - val_loss: 44.0399\n",
      "Epoch 633/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.1593 - val_loss: 43.9846\n",
      "Epoch 634/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.1526 - val_loss: 44.0035\n",
      "Epoch 635/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.1329 - val_loss: 43.9959\n",
      "Epoch 636/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 46.1095 - val_loss: 43.9844\n",
      "Epoch 637/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.1086 - val_loss: 43.9550\n",
      "Epoch 638/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.0897 - val_loss: 43.9711\n",
      "Epoch 639/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.0817 - val_loss: 43.9609\n",
      "Epoch 640/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.0624 - val_loss: 44.0254\n",
      "Epoch 641/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 46.0417 - val_loss: 43.9650\n",
      "Epoch 642/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.0317 - val_loss: 43.9699\n",
      "Epoch 643/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 46.0165 - val_loss: 43.9081\n",
      "Epoch 644/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.9988 - val_loss: 43.8748\n",
      "Epoch 645/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.9786 - val_loss: 43.8541\n",
      "Epoch 646/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.9566 - val_loss: 43.8578\n",
      "Epoch 647/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.9436 - val_loss: 43.8483\n",
      "Epoch 648/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.9352 - val_loss: 43.8393\n",
      "Epoch 649/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.9149 - val_loss: 43.8457\n",
      "Epoch 650/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.9080 - val_loss: 43.8841\n",
      "Epoch 651/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.8979 - val_loss: 43.8383\n",
      "Epoch 652/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.8735 - val_loss: 43.8358\n",
      "Epoch 653/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.8544 - val_loss: 43.8519\n",
      "Epoch 654/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 45.8567 - val_loss: 43.8266\n",
      "Epoch 655/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.8248 - val_loss: 43.7982\n",
      "Epoch 656/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.8001 - val_loss: 43.7644\n",
      "Epoch 657/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.8089 - val_loss: 43.7823\n",
      "Epoch 658/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.7807 - val_loss: 43.7529\n",
      "Epoch 659/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.7698 - val_loss: 43.7326\n",
      "Epoch 660/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.7643 - val_loss: 43.7047\n",
      "Epoch 661/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.7451 - val_loss: 43.7333\n",
      "Epoch 662/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.7318 - val_loss: 43.7192\n",
      "Epoch 663/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.7219 - val_loss: 43.7123\n",
      "Epoch 664/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.7005 - val_loss: 43.6901\n",
      "Epoch 665/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.6771 - val_loss: 43.6845\n",
      "Epoch 666/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.6874 - val_loss: 43.6893\n",
      "Epoch 667/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.6603 - val_loss: 43.6794\n",
      "Epoch 668/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.6391 - val_loss: 43.6781\n",
      "Epoch 669/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.6378 - val_loss: 43.6886\n",
      "Epoch 670/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.6125 - val_loss: 43.6848\n",
      "Epoch 671/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.5942 - val_loss: 43.6101\n",
      "Epoch 672/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.5810 - val_loss: 43.5654\n",
      "Epoch 673/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.5768 - val_loss: 43.5605\n",
      "Epoch 674/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.5558 - val_loss: 43.5374\n",
      "Epoch 675/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 45.5357 - val_loss: 43.5345\n",
      "Epoch 676/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.5276 - val_loss: 43.5744\n",
      "Epoch 677/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.5138 - val_loss: 43.5557\n",
      "Epoch 678/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.4951 - val_loss: 43.5190\n",
      "Epoch 679/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.4841 - val_loss: 43.5200\n",
      "Epoch 680/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 45.4637 - val_loss: 43.5045\n",
      "Epoch 681/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.4594 - val_loss: 43.4921\n",
      "Epoch 682/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.4462 - val_loss: 43.5481\n",
      "Epoch 683/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.4267 - val_loss: 43.5235\n",
      "Epoch 684/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.4145 - val_loss: 43.4928\n",
      "Epoch 685/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.4057 - val_loss: 43.4511\n",
      "Epoch 686/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.3874 - val_loss: 43.4573\n",
      "Epoch 687/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.3799 - val_loss: 43.4377\n",
      "Epoch 688/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.3616 - val_loss: 43.4465\n",
      "Epoch 689/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.3452 - val_loss: 43.4274\n",
      "Epoch 690/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.3379 - val_loss: 43.3872\n",
      "Epoch 691/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.3173 - val_loss: 43.3608\n",
      "Epoch 692/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.3115 - val_loss: 43.3962\n",
      "Epoch 693/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.2930 - val_loss: 43.3970\n",
      "Epoch 694/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.2768 - val_loss: 43.3536\n",
      "Epoch 695/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.2624 - val_loss: 43.3559\n",
      "Epoch 696/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.2717 - val_loss: 43.3781\n",
      "Epoch 697/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.2318 - val_loss: 43.3519\n",
      "Epoch 698/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.2316 - val_loss: 43.3433\n",
      "Epoch 699/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.2177 - val_loss: 43.3200\n",
      "Epoch 700/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.1980 - val_loss: 43.3005\n",
      "Epoch 701/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.2006 - val_loss: 43.3143\n",
      "Epoch 702/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.1679 - val_loss: 43.2665\n",
      "Epoch 703/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.1601 - val_loss: 43.2512\n",
      "Epoch 704/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.1570 - val_loss: 43.2559\n",
      "Epoch 705/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 45.1359 - val_loss: 43.2747\n",
      "Epoch 706/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.1236 - val_loss: 43.2727\n",
      "Epoch 707/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 45.0970 - val_loss: 43.2911\n",
      "Epoch 708/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.0967 - val_loss: 43.2438\n",
      "Epoch 709/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.0756 - val_loss: 43.2444\n",
      "Epoch 710/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 45.0673 - val_loss: 43.2193\n",
      "Epoch 711/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 45.0485 - val_loss: 43.2405\n",
      "Epoch 712/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.0406 - val_loss: 43.2093\n",
      "Epoch 713/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 45.0241 - val_loss: 43.2048\n",
      "Epoch 714/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 45.0149 - val_loss: 43.1636\n",
      "Epoch 715/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 45.0073 - val_loss: 43.1770\n",
      "Epoch 716/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.9825 - val_loss: 43.1569\n",
      "Epoch 717/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.9810 - val_loss: 43.1400\n",
      "Epoch 718/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.9692 - val_loss: 43.1442\n",
      "Epoch 719/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.9492 - val_loss: 43.1473\n",
      "Epoch 720/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.9431 - val_loss: 43.1215\n",
      "Epoch 721/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.9268 - val_loss: 43.1188\n",
      "Epoch 722/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.9080 - val_loss: 43.1240\n",
      "Epoch 723/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.9020 - val_loss: 43.1691\n",
      "Epoch 724/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.8880 - val_loss: 43.1459\n",
      "Epoch 725/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.8935 - val_loss: 43.1671\n",
      "Epoch 726/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.8611 - val_loss: 43.1597\n",
      "Epoch 727/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.8404 - val_loss: 43.0885\n",
      "Epoch 728/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.8343 - val_loss: 43.0833\n",
      "Epoch 729/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.8206 - val_loss: 43.0900\n",
      "Epoch 730/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.8072 - val_loss: 43.0584\n",
      "Epoch 731/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.8110 - val_loss: 43.0634\n",
      "Epoch 732/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.7888 - val_loss: 43.0564\n",
      "Epoch 733/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.7728 - val_loss: 43.0350\n",
      "Epoch 734/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.7705 - val_loss: 43.0470\n",
      "Epoch 735/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.7476 - val_loss: 43.0675\n",
      "Epoch 736/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.7500 - val_loss: 43.0428\n",
      "Epoch 737/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 44.7187 - val_loss: 43.0292\n",
      "Epoch 738/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.7145 - val_loss: 43.0388\n",
      "Epoch 739/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.6955 - val_loss: 43.0155\n",
      "Epoch 740/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.6868 - val_loss: 42.9921\n",
      "Epoch 741/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.6688 - val_loss: 42.9849\n",
      "Epoch 742/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.6618 - val_loss: 42.9809\n",
      "Epoch 743/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.6462 - val_loss: 43.0217\n",
      "Epoch 744/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.6380 - val_loss: 43.0072\n",
      "Epoch 745/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.6366 - val_loss: 42.9314\n",
      "Epoch 746/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.6098 - val_loss: 43.0092\n",
      "Epoch 747/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.6045 - val_loss: 42.9450\n",
      "Epoch 748/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.5931 - val_loss: 42.9269\n",
      "Epoch 749/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.5772 - val_loss: 42.8902\n",
      "Epoch 750/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.5652 - val_loss: 42.9092\n",
      "Epoch 751/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.5524 - val_loss: 42.9092\n",
      "Epoch 752/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.5399 - val_loss: 42.8926\n",
      "Epoch 753/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.5442 - val_loss: 42.8852\n",
      "Epoch 754/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.5273 - val_loss: 42.8527\n",
      "Epoch 755/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.5144 - val_loss: 42.8423\n",
      "Epoch 756/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.5093 - val_loss: 42.8496\n",
      "Epoch 757/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.4902 - val_loss: 42.9366\n",
      "Epoch 758/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.4875 - val_loss: 42.9138\n",
      "Epoch 759/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.4702 - val_loss: 42.8818\n",
      "Epoch 760/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.4576 - val_loss: 42.8376\n",
      "Epoch 761/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.4510 - val_loss: 42.8356\n",
      "Epoch 762/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.4257 - val_loss: 42.8290\n",
      "Epoch 763/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.4167 - val_loss: 42.8206\n",
      "Epoch 764/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 44.4030 - val_loss: 42.7903\n",
      "Epoch 765/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 44.3855 - val_loss: 42.8134\n",
      "Epoch 766/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.3891 - val_loss: 42.8082\n",
      "Epoch 767/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.3793 - val_loss: 42.8001\n",
      "Epoch 768/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.3525 - val_loss: 42.7833\n",
      "Epoch 769/5000\n",
      "1126/1126 [==============================] - 0s 39us/step - loss: 44.3465 - val_loss: 42.8025\n",
      "Epoch 770/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 44.3461 - val_loss: 42.7578\n",
      "Epoch 771/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 44.3318 - val_loss: 42.7443\n",
      "Epoch 772/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 44.3183 - val_loss: 42.7409\n",
      "Epoch 773/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.3180 - val_loss: 42.7992\n",
      "Epoch 774/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.3034 - val_loss: 42.7620\n",
      "Epoch 775/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.2809 - val_loss: 42.7261\n",
      "Epoch 776/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.2605 - val_loss: 42.6911\n",
      "Epoch 777/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.2581 - val_loss: 42.6706\n",
      "Epoch 778/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.2427 - val_loss: 42.6575\n",
      "Epoch 779/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.2487 - val_loss: 42.6696\n",
      "Epoch 780/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.2244 - val_loss: 42.6561\n",
      "Epoch 781/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.2150 - val_loss: 42.6955\n",
      "Epoch 782/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.2057 - val_loss: 42.7412\n",
      "Epoch 783/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 44.1949 - val_loss: 42.7285\n",
      "Epoch 784/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 44.1873 - val_loss: 42.6987\n",
      "Epoch 785/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 44.1736 - val_loss: 42.6798\n",
      "Epoch 786/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.1548 - val_loss: 42.6578\n",
      "Epoch 787/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.1509 - val_loss: 42.6457\n",
      "Epoch 788/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.1341 - val_loss: 42.6113\n",
      "Epoch 789/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.1229 - val_loss: 42.5930\n",
      "Epoch 790/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.1141 - val_loss: 42.6116\n",
      "Epoch 791/5000\n",
      "1126/1126 [==============================] - 0s 40us/step - loss: 44.0938 - val_loss: 42.5995\n",
      "Epoch 792/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.0869 - val_loss: 42.5779\n",
      "Epoch 793/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.0909 - val_loss: 42.5388\n",
      "Epoch 794/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.0666 - val_loss: 42.5295\n",
      "Epoch 795/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.0596 - val_loss: 42.5289\n",
      "Epoch 796/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 44.0620 - val_loss: 42.5254\n",
      "Epoch 797/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.0456 - val_loss: 42.5407\n",
      "Epoch 798/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.0230 - val_loss: 42.5422\n",
      "Epoch 799/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 44.0145 - val_loss: 42.5602\n",
      "Epoch 800/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 44.0140 - val_loss: 42.5235\n",
      "Epoch 801/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.9931 - val_loss: 42.5180\n",
      "Epoch 802/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.9973 - val_loss: 42.4897\n",
      "Epoch 803/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.9856 - val_loss: 42.5130\n",
      "Epoch 804/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.9569 - val_loss: 42.4514\n",
      "Epoch 805/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.9517 - val_loss: 42.4676\n",
      "Epoch 806/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.9382 - val_loss: 42.4649\n",
      "Epoch 807/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.9293 - val_loss: 42.4644\n",
      "Epoch 808/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.9205 - val_loss: 42.4487\n",
      "Epoch 809/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.9008 - val_loss: 42.4500\n",
      "Epoch 810/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.9013 - val_loss: 42.4124\n",
      "Epoch 811/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.8838 - val_loss: 42.4206\n",
      "Epoch 812/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 43.8757 - val_loss: 42.5086\n",
      "Epoch 813/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.8784 - val_loss: 42.4739\n",
      "Epoch 814/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.8559 - val_loss: 42.4868\n",
      "Epoch 815/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.8423 - val_loss: 42.4430\n",
      "Epoch 816/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.8470 - val_loss: 42.4511\n",
      "Epoch 817/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.8217 - val_loss: 42.4226\n",
      "Epoch 818/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.8098 - val_loss: 42.3928\n",
      "Epoch 819/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.8031 - val_loss: 42.3740\n",
      "Epoch 820/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.7902 - val_loss: 42.3834\n",
      "Epoch 821/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 43.7785 - val_loss: 42.3742\n",
      "Epoch 822/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.7704 - val_loss: 42.3407\n",
      "Epoch 823/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.7665 - val_loss: 42.3073\n",
      "Epoch 824/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 43.7651 - val_loss: 42.3104\n",
      "Epoch 825/5000\n",
      "1126/1126 [==============================] - 0s 41us/step - loss: 43.7295 - val_loss: 42.2946\n",
      "Epoch 826/5000\n",
      "1126/1126 [==============================] - 0s 42us/step - loss: 43.7350 - val_loss: 42.3211\n",
      "Epoch 827/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 43.7109 - val_loss: 42.3060\n",
      "Epoch 828/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.6966 - val_loss: 42.3060\n",
      "Epoch 829/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 43.6898 - val_loss: 42.3138\n",
      "Epoch 830/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.6859 - val_loss: 42.3115\n",
      "Epoch 831/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.6679 - val_loss: 42.2911\n",
      "Epoch 832/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.6646 - val_loss: 42.2785\n",
      "Epoch 833/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.6520 - val_loss: 42.2839\n",
      "Epoch 834/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.6463 - val_loss: 42.2726\n",
      "Epoch 835/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.6276 - val_loss: 42.2841\n",
      "Epoch 836/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.6225 - val_loss: 42.3255\n",
      "Epoch 837/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.6175 - val_loss: 42.2734\n",
      "Epoch 838/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.6039 - val_loss: 42.2346\n",
      "Epoch 839/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.5984 - val_loss: 42.2122\n",
      "Epoch 840/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.5779 - val_loss: 42.1973\n",
      "Epoch 841/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.5695 - val_loss: 42.2059\n",
      "Epoch 842/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.5644 - val_loss: 42.2215\n",
      "Epoch 843/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.5325 - val_loss: 42.2185\n",
      "Epoch 844/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.5331 - val_loss: 42.2229\n",
      "Epoch 845/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.5281 - val_loss: 42.2036\n",
      "Epoch 846/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.5226 - val_loss: 42.1980\n",
      "Epoch 847/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.5039 - val_loss: 42.2385\n",
      "Epoch 848/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.5069 - val_loss: 42.2019\n",
      "Epoch 849/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.4906 - val_loss: 42.1881\n",
      "Epoch 850/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.4787 - val_loss: 42.1492\n",
      "Epoch 851/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.4727 - val_loss: 42.1281\n",
      "Epoch 852/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.4619 - val_loss: 42.1169\n",
      "Epoch 853/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.4515 - val_loss: 42.0963\n",
      "Epoch 854/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.4469 - val_loss: 42.1003\n",
      "Epoch 855/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.4245 - val_loss: 42.0792\n",
      "Epoch 856/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.4184 - val_loss: 42.0864\n",
      "Epoch 857/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.4015 - val_loss: 42.0831\n",
      "Epoch 858/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.3926 - val_loss: 42.1367\n",
      "Epoch 859/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.3819 - val_loss: 42.1269\n",
      "Epoch 860/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.3764 - val_loss: 42.1266\n",
      "Epoch 861/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.3645 - val_loss: 42.0879\n",
      "Epoch 862/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.3615 - val_loss: 42.0709\n",
      "Epoch 863/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.3400 - val_loss: 42.0454\n",
      "Epoch 864/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 43.3404 - val_loss: 42.0629\n",
      "Epoch 865/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 43.3257 - val_loss: 42.0392\n",
      "Epoch 866/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 43.3174 - val_loss: 42.0450\n",
      "Epoch 867/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.3139 - val_loss: 42.0460\n",
      "Epoch 868/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.3006 - val_loss: 42.0089\n",
      "Epoch 869/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.2806 - val_loss: 41.9734\n",
      "Epoch 870/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.2718 - val_loss: 41.9930\n",
      "Epoch 871/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.2673 - val_loss: 42.0080\n",
      "Epoch 872/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.2630 - val_loss: 41.9927\n",
      "Epoch 873/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.2532 - val_loss: 41.9884\n",
      "Epoch 874/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.2422 - val_loss: 41.9999\n",
      "Epoch 875/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.2323 - val_loss: 42.0512\n",
      "Epoch 876/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.2340 - val_loss: 42.0228\n",
      "Epoch 877/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.2131 - val_loss: 41.9899\n",
      "Epoch 878/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.1923 - val_loss: 42.0059\n",
      "Epoch 879/5000\n",
      "1126/1126 [==============================] - 0s 32us/step - loss: 43.1890 - val_loss: 41.9708\n",
      "Epoch 880/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.1870 - val_loss: 41.9189\n",
      "Epoch 881/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.1600 - val_loss: 41.9300\n",
      "Epoch 882/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.1606 - val_loss: 41.9003\n",
      "Epoch 883/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.1496 - val_loss: 41.9213\n",
      "Epoch 884/5000\n",
      "1126/1126 [==============================] - 0s 33us/step - loss: 43.1442 - val_loss: 41.8775\n",
      "Epoch 885/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.1366 - val_loss: 41.9472\n",
      "Epoch 886/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.1225 - val_loss: 41.9204\n",
      "Epoch 887/5000\n",
      "1126/1126 [==============================] - 0s 31us/step - loss: 43.1115 - val_loss: 41.9229\n",
      "Epoch 888/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.1075 - val_loss: 41.9111\n",
      "Epoch 889/5000\n",
      "1126/1126 [==============================] - 0s 34us/step - loss: 43.1079 - val_loss: 42.0370\n",
      "Epoch 890/5000\n",
      "1126/1126 [==============================] - 0s 35us/step - loss: 43.1271 - val_loss: 41.9483\n",
      "Epoch 891/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 43.0978 - val_loss: 41.9133\n",
      "Epoch 892/5000\n",
      "1126/1126 [==============================] - 0s 36us/step - loss: 43.0786 - val_loss: 41.9720\n",
      "Epoch 893/5000\n",
      "1126/1126 [==============================] - 0s 38us/step - loss: 43.0689 - val_loss: 41.8881\n",
      "Epoch 894/5000\n",
      "1126/1126 [==============================] - 0s 37us/step - loss: 43.0495 - val_loss: 41.8868\n",
      "Epoch 00894: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d23c9c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d04521b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.title('Coût et coût de validation')\n",
    "line1,=plt.plot(history.history['loss'], label=\"Loss\", linestyle='-', color='r')\n",
    "line2,=plt.plot(history.history['val_loss'], label=\"Val loss\", linestyle='-', color='b')\n",
    "first_legend = plt.legend(handles=[line1, line2], loc=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.title('y_pred en fonction de y_test')\n",
    "\n",
    "plt.plot(y_pred[:], y_test[:], '+')\n",
    "plt.ylabel('Test')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
