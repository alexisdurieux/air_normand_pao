{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/micro_sud3_normalized.pkl')\n",
    "df = df.reset_index()\n",
    "df.head()\n",
    "\n",
    "def split_dataframe(dataframe, percent):\n",
    "    nb_rows = int(np.floor(percent * len(dataframe)))\n",
    "    return dataframe[:nb_rows], dataframe[nb_rows:]\n",
    "    \"\"\"\n",
    "def dataframe_to_xy(df, sequence_size):\n",
    "\n",
    "    out_X = np.zeros((len(df)//sequence_size, sequence_size, 8))\n",
    "    out_y = np.zeros((len(df)//sequence_size, sequence_size))\n",
    "    i = 0\n",
    "    while i + sequence_size < len(df):\n",
    "        sequence = df.iloc[i:i+sequence_size]\n",
    "        out_X[i//sequence_size] =  np.array(sequence[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                                 'tgrad', 'pressure', 'pluvio']])\n",
    "        out_y[i//sequence_size] = np.array(sequence['NO2_ref'])\n",
    "        i += sequence_size\n",
    "        \n",
    "    return out_X, out_y\n",
    "    \"\"\"\n",
    "\n",
    "def dataframe_to_xy(df):\n",
    "    return (np.array(df[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                         'tgrad', 'pressure', 'pluvio']]),\\\n",
    "            np.array(df['NO2_ref']))\n",
    "\n",
    "        \n",
    "df_train, df_test = split_dataframe(df, 0.5) \n",
    "df_valid, df_test = split_dataframe(df_test, 0.5)\n",
    "\n",
    "X_train, y_train = dataframe_to_xy(df_train)\n",
    "X_valid, y_valid = dataframe_to_xy(df_valid)\n",
    "X_test, y_test = dataframe_to_xy(df_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_valid= X_valid.reshape((X_valid.shape[0], 1,  X_valid.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "#y_train = y_train.reshape((1, len(y_train)))\n",
    "#y_valid = y_valid.reshape((1, len(y_valid)))\n",
    "#y_test = y_test.reshape((1, len(y_test)))\n",
    "\n",
    "def dataframe_to_xy_sequences(df, sequence_size):\n",
    "    out_X = np.zeros((len(df)//sequence_size, sequence_size, 8))\n",
    "    out_y = np.zeros((len(df)//sequence_size, sequence_size))\n",
    "    i = 0\n",
    "    while i + sequence_size < len(df):\n",
    "        sequence = df.iloc[i:i+sequence_size]\n",
    "        out_X[i//sequence_size] =  np.array(sequence[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                                 'tgrad', 'pressure', 'pluvio']])\n",
    "        out_y[i//sequence_size] = np.array(sequence['NO2_ref'])\n",
    "        i += sequence_size\n",
    "        \n",
    "    return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def simple_rnn_model(nb_units, input_dim, loss='mean_squared_error', optimizer='adam'):\n",
    "    print(input_dim)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(nb_units, input_shape=input_dim))#input_dim=input_dim[1], input_length=input_dim[0], return_sequences=True))\n",
    "    #model.add(Dense(nb_units))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 417\n",
      "Trainable params: 417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_rnn_model(16, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1126 samples, validate on 563 samples\n",
      "Epoch 1/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3339.4802 - val_loss: 2992.2122\n",
      "Epoch 2/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3315.5606 - val_loss: 2975.1947\n",
      "Epoch 3/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3294.0302 - val_loss: 2957.9057\n",
      "Epoch 4/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3272.4894 - val_loss: 2940.2850\n",
      "Epoch 5/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3250.9579 - val_loss: 2922.4113\n",
      "Epoch 6/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3229.4468 - val_loss: 2904.40863022.17\n",
      "Epoch 7/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3208.0802 - val_loss: 2886.4580\n",
      "Epoch 8/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3187.0381 - val_loss: 2868.7280\n",
      "Epoch 9/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3166.4243 - val_loss: 2851.2836\n",
      "Epoch 10/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3146.2615 - val_loss: 2834.1547\n",
      "Epoch 11/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3126.5530 - val_loss: 2817.3540\n",
      "Epoch 12/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3107.2808 - val_loss: 2800.8674\n",
      "Epoch 13/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3088.4194 - val_loss: 2784.6809\n",
      "Epoch 14/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3069.9513 - val_loss: 2768.7885\n",
      "Epoch 15/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3051.8616 - val_loss: 2753.1756\n",
      "Epoch 16/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3034.1378 - val_loss: 2737.8425\n",
      "Epoch 17/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3016.7697 - val_loss: 2722.7783\n",
      "Epoch 18/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2999.7445 - val_loss: 2707.9758\n",
      "Epoch 19/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2983.0536 - val_loss: 2693.4304\n",
      "Epoch 20/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2966.6875 - val_loss: 2679.1362\n",
      "Epoch 21/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2950.6374 - val_loss: 2665.0855\n",
      "Epoch 22/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2934.8945 - val_loss: 2651.2742\n",
      "Epoch 23/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2919.4516 - val_loss: 2637.6977\n",
      "Epoch 24/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2904.3005 - val_loss: 2624.3490\n",
      "Epoch 25/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2889.4334 - val_loss: 2611.2244\n",
      "Epoch 26/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2874.8423 - val_loss: 2598.3177\n",
      "Epoch 27/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2860.5203 - val_loss: 2585.6243\n",
      "Epoch 28/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2846.4599 - val_loss: 2573.1383\n",
      "Epoch 29/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2832.6537 - val_loss: 2560.8553\n",
      "Epoch 30/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2819.0950 - val_loss: 2548.7698\n",
      "Epoch 31/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2805.7760 - val_loss: 2536.8765\n",
      "Epoch 32/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2792.6903 - val_loss: 2525.1714\n",
      "Epoch 33/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2779.8314 - val_loss: 2513.6497\n",
      "Epoch 34/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2767.1929 - val_loss: 2502.3067\n",
      "Epoch 35/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2754.7686 - val_loss: 2491.1380\n",
      "Epoch 36/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2742.5523 - val_loss: 2480.1391\n",
      "Epoch 37/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2730.5382 - val_loss: 2469.3063\n",
      "Epoch 38/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2718.7211 - val_loss: 2458.6348\n",
      "Epoch 39/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2707.0947 - val_loss: 2448.1208\n",
      "Epoch 40/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2695.6543 - val_loss: 2437.7607\n",
      "Epoch 41/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2684.3942 - val_loss: 2427.5494\n",
      "Epoch 42/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2673.3095 - val_loss: 2417.4835\n",
      "Epoch 43/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2662.3953 - val_loss: 2407.5597\n",
      "Epoch 44/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2651.6473 - val_loss: 2397.7747\n",
      "Epoch 45/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2641.0612 - val_loss: 2388.1255\n",
      "Epoch 46/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2630.6324 - val_loss: 2378.6071\n",
      "Epoch 47/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2620.3563 - val_loss: 2369.2177\n",
      "Epoch 48/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2610.2295 - val_loss: 2359.9540\n",
      "Epoch 49/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2600.2482 - val_loss: 2350.8129\n",
      "Epoch 50/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2590.4086 - val_loss: 2341.7921\n",
      "Epoch 51/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2580.7072 - val_loss: 2332.8882\n",
      "Epoch 52/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2571.1405 - val_loss: 2324.0985\n",
      "Epoch 53/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2561.7051 - val_loss: 2315.4204\n",
      "Epoch 54/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2552.3977 - val_loss: 2306.8512\n",
      "Epoch 55/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2543.2151 - val_loss: 2298.3885\n",
      "Epoch 56/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2534.1545 - val_loss: 2290.0299\n",
      "Epoch 57/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2525.2128 - val_loss: 2281.7734\n",
      "Epoch 58/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2516.3875 - val_loss: 2273.6166\n",
      "Epoch 59/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2507.6759 - val_loss: 2265.5578\n",
      "Epoch 60/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2499.0753 - val_loss: 2257.5945\n",
      "Epoch 61/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2490.5832 - val_loss: 2249.7246\n",
      "Epoch 62/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2482.1971 - val_loss: 2241.9461\n",
      "Epoch 63/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2473.9146 - val_loss: 2234.2573\n",
      "Epoch 64/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2465.7334 - val_loss: 2226.6561\n",
      "Epoch 65/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2457.6513 - val_loss: 2219.1411\n",
      "Epoch 66/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2449.6662 - val_loss: 2211.7101\n",
      "Epoch 67/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2441.7760 - val_loss: 2204.3619\n",
      "Epoch 68/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2433.9789 - val_loss: 2197.0948\n",
      "Epoch 69/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2426.2727 - val_loss: 2189.9071\n",
      "Epoch 70/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2418.6557 - val_loss: 2182.7975\n",
      "Epoch 71/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2411.1260 - val_loss: 2175.7642\n",
      "Epoch 72/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2403.6820 - val_loss: 2168.8062\n",
      "Epoch 73/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2396.3216 - val_loss: 2161.9217\n",
      "Epoch 74/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2389.0434 - val_loss: 2155.1090\n",
      "Epoch 75/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2381.8454 - val_loss: 2148.3673\n",
      "Epoch 76/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2374.7264 - val_loss: 2141.6952\n",
      "Epoch 77/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2367.6846 - val_loss: 2135.0912\n",
      "Epoch 78/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2360.7188 - val_loss: 2128.5544\n",
      "Epoch 79/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2353.8274 - val_loss: 2122.0838\n",
      "Epoch 80/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 2347.0091 - val_loss: 2115.6777\n",
      "Epoch 81/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2340.2625 - val_loss: 2109.3354\n",
      "Epoch 82/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2333.5861 - val_loss: 2103.0555\n",
      "Epoch 83/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2326.9787 - val_loss: 2096.8372\n",
      "Epoch 84/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2320.4391 - val_loss: 2090.6792\n",
      "Epoch 85/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2313.9659 - val_loss: 2084.5805\n",
      "Epoch 86/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2307.5582 - val_loss: 2078.5403\n",
      "Epoch 87/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2301.2145 - val_loss: 2072.5575\n",
      "Epoch 88/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2294.9339 - val_loss: 2066.6310\n",
      "Epoch 89/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2288.7151 - val_loss: 2060.7601\n",
      "Epoch 90/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2282.5571 - val_loss: 2054.9438\n",
      "Epoch 91/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2276.4590 - val_loss: 2049.1814\n",
      "Epoch 92/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2270.4197 - val_loss: 2043.4720\n",
      "Epoch 93/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2264.4382 - val_loss: 2037.8146\n",
      "Epoch 94/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2258.5134 - val_loss: 2032.2083\n",
      "Epoch 95/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2252.6445 - val_loss: 2026.6528\n",
      "Epoch 96/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2246.8305 - val_loss: 2021.1467\n",
      "Epoch 97/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2241.0705 - val_loss: 2015.6897\n",
      "Epoch 98/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2235.3637 - val_loss: 2010.2808\n",
      "Epoch 99/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2229.7091 - val_loss: 2004.9194\n",
      "Epoch 100/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2224.1061 - val_loss: 1999.6048\n",
      "Epoch 101/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2218.5536 - val_loss: 1994.3361\n",
      "Epoch 102/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2213.0509 - val_loss: 1989.1128\n",
      "Epoch 103/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2207.5973 - val_loss: 1983.9343\n",
      "Epoch 104/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2202.1919 - val_loss: 1978.7997\n",
      "Epoch 105/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2196.8341 - val_loss: 1973.7087\n",
      "Epoch 106/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2191.5231 - val_loss: 1968.6603\n",
      "Epoch 107/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2186.2581 - val_loss: 1963.6543\n",
      "Epoch 108/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2181.0386 - val_loss: 1958.6899\n",
      "Epoch 109/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2175.8638 - val_loss: 1953.7663\n",
      "Epoch 110/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2170.7329 - val_loss: 1948.8831\n",
      "Epoch 111/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2165.6455 - val_loss: 1944.0399\n",
      "Epoch 112/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2160.6009 - val_loss: 1939.2360\n",
      "Epoch 113/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2155.5984 - val_loss: 1934.4710\n",
      "Epoch 114/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2150.6374 - val_loss: 1929.7441\n",
      "Epoch 115/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2145.7173 - val_loss: 1925.0549\n",
      "Epoch 116/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2140.8375 - val_loss: 1920.4030\n",
      "Epoch 117/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2135.9975 - val_loss: 1915.7879\n",
      "Epoch 118/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2131.1967 - val_loss: 1911.2089\n",
      "Epoch 119/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2126.4346 - val_loss: 1906.6658\n",
      "Epoch 120/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2121.7106 - val_loss: 1902.1579\n",
      "Epoch 121/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2117.0242 - val_loss: 1897.6849\n",
      "Epoch 122/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2112.3748 - val_loss: 1893.2462\n",
      "Epoch 123/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2107.7619 - val_loss: 1888.8414\n",
      "Epoch 124/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2103.1852 - val_loss: 1884.4701\n",
      "Epoch 125/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2098.6440 - val_loss: 1880.1320\n",
      "Epoch 126/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2094.1380 - val_loss: 1875.8265\n",
      "Epoch 127/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2089.6666 - val_loss: 1871.5533\n",
      "Epoch 128/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2085.2294 - val_loss: 1867.3119\n",
      "Epoch 129/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2080.8259 - val_loss: 1863.1020\n",
      "Epoch 130/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2076.4558 - val_loss: 1858.9230\n",
      "Epoch 131/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2072.1186 - val_loss: 1854.7749\n",
      "Epoch 132/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2067.8138 - val_loss: 1850.6570\n",
      "Epoch 133/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2063.5409 - val_loss: 1846.5691\n",
      "Epoch 134/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2059.2998 - val_loss: 1842.5107\n",
      "Epoch 135/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2055.0898 - val_loss: 1838.4816\n",
      "Epoch 136/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2050.9107 - val_loss: 1834.4814\n",
      "Epoch 137/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2046.7621 - val_loss: 1830.5095\n",
      "Epoch 138/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2042.6434 - val_loss: 1826.5660\n",
      "Epoch 139/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2038.5543 - val_loss: 1822.6502\n",
      "Epoch 140/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2034.4947 - val_loss: 1818.7621\n",
      "Epoch 141/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2030.4640 - val_loss: 1814.9010\n",
      "Epoch 142/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2026.4619 - val_loss: 1811.0669\n",
      "Epoch 143/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2022.4881 - val_loss: 1807.2594\n",
      "Epoch 144/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2018.5421 - val_loss: 1803.4782\n",
      "Epoch 145/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2014.6238 - val_loss: 1799.7230\n",
      "Epoch 146/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2010.7327 - val_loss: 1795.9934\n",
      "Epoch 147/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2006.8685 - val_loss: 1792.2893\n",
      "Epoch 148/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2003.0310 - val_loss: 1788.6103\n",
      "Epoch 149/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1999.2198 - val_loss: 1784.9562\n",
      "Epoch 150/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1995.4346 - val_loss: 1781.3265\n",
      "Epoch 151/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1991.6751 - val_loss: 1777.7212\n",
      "Epoch 152/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1987.9409 - val_loss: 1774.1399\n",
      "Epoch 153/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1984.2319 - val_loss: 1770.5824\n",
      "Epoch 154/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1980.5477 - val_loss: 1767.0482\n",
      "Epoch 155/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1976.8879 - val_loss: 1763.5374\n",
      "Epoch 156/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1973.2524 - val_loss: 1760.0495\n",
      "Epoch 157/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1969.6410 - val_loss: 1756.5845\n",
      "Epoch 158/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1966.0532 - val_loss: 1753.1417\n",
      "Epoch 159/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1962.4888 - val_loss: 1749.7215\n",
      "Epoch 160/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1958.9476 - val_loss: 1746.3231\n",
      "Epoch 161/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1955.4294 - val_loss: 1742.9465\n",
      "Epoch 162/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1951.9338 - val_loss: 1739.5917\n",
      "Epoch 163/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1948.4607 - val_loss: 1736.2580\n",
      "Epoch 164/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1945.0098 - val_loss: 1732.9455\n",
      "Epoch 165/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1941.5808 - val_loss: 1729.6539\n",
      "Epoch 166/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1938.1734 - val_loss: 1726.3831\n",
      "Epoch 167/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1934.7876 - val_loss: 1723.1325\n",
      "Epoch 168/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1931.4230 - val_loss: 1719.9023\n",
      "Epoch 169/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1928.0795 - val_loss: 1716.6922\n",
      "Epoch 170/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1924.7568 - val_loss: 1713.5021\n",
      "Epoch 171/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1921.4546 - val_loss: 1710.3315\n",
      "Epoch 172/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1918.1728 - val_loss: 1707.1802\n",
      "Epoch 173/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1914.9110 - val_loss: 1704.0483\n",
      "Epoch 174/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1911.6693 - val_loss: 1700.9353\n",
      "Epoch 175/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1908.4473 - val_loss: 1697.8414\n",
      "Epoch 176/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1905.2448 - val_loss: 1694.7659\n",
      "Epoch 177/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1902.0616 - val_loss: 1691.7091\n",
      "Epoch 178/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1898.8976 - val_loss: 1688.6707\n",
      "Epoch 179/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1895.7526 - val_loss: 1685.6502\n",
      "Epoch 180/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1892.6263 - val_loss: 1682.6480\n",
      "Epoch 181/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1889.5186 - val_loss: 1679.6634\n",
      "Epoch 182/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1886.4293 - val_loss: 1676.6963\n",
      "Epoch 183/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1883.3582 - val_loss: 1673.7468\n",
      "Epoch 184/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1880.3051 - val_loss: 1670.8146\n",
      "Epoch 185/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1877.2699 - val_loss: 1667.8994\n",
      "Epoch 186/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1874.2523 - val_loss: 1665.0012\n",
      "Epoch 187/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1871.2524 - val_loss: 1662.1199\n",
      "Epoch 188/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1868.2698 - val_loss: 1659.2552\n",
      "Epoch 189/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1865.3043 - val_loss: 1656.4069\n",
      "Epoch 190/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1862.3558 - val_loss: 1653.5749\n",
      "Epoch 191/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1859.4241 - val_loss: 1650.7591\n",
      "Epoch 192/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1856.5091 - val_loss: 1647.9593\n",
      "Epoch 193/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1853.6106 - val_loss: 1645.1753\n",
      "Epoch 194/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1850.7286 - val_loss: 1642.4070\n",
      "Epoch 195/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1847.8629 - val_loss: 1639.6546\n",
      "Epoch 196/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1845.0132 - val_loss: 1636.9173\n",
      "Epoch 197/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1842.1794 - val_loss: 1634.1955\n",
      "Epoch 198/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1839.3614 - val_loss: 1631.4888\n",
      "Epoch 199/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1836.5590 - val_loss: 1628.7971\n",
      "Epoch 200/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1833.7721 - val_loss: 1626.1203\n",
      "Epoch 201/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1831.0005 - val_loss: 1623.4583\n",
      "Epoch 202/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1828.2442 - val_loss: 1620.8109\n",
      "Epoch 203/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1825.5029 - val_loss: 1618.1779\n",
      "Epoch 204/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1822.7765 - val_loss: 1615.5592\n",
      "Epoch 205/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1820.0649 - val_loss: 1612.9547\n",
      "Epoch 206/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1817.3680 - val_loss: 1610.3645\n",
      "Epoch 207/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1814.6855 - val_loss: 1607.7882\n",
      "Epoch 208/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1812.0175 - val_loss: 1605.2258\n",
      "Epoch 209/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1809.3637 - val_loss: 1602.6770\n",
      "Epoch 210/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1806.7241 - val_loss: 1600.1418\n",
      "Epoch 211/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1804.0984 - val_loss: 1597.6202\n",
      "Epoch 212/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1801.4866 - val_loss: 1595.1120\n",
      "Epoch 213/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1798.8887 - val_loss: 1592.6170\n",
      "Epoch 214/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1796.3042 - val_loss: 1590.1351\n",
      "Epoch 215/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1793.7334 - val_loss: 1587.6664\n",
      "Epoch 216/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1791.1759 - val_loss: 1585.2104\n",
      "Epoch 217/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1788.6316 - val_loss: 1582.7674\n",
      "Epoch 218/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1786.1006 - val_loss: 1580.3370\n",
      "Epoch 219/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1783.5825 - val_loss: 1577.9193\n",
      "Epoch 220/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1781.0776 - val_loss: 1575.5141\n",
      "Epoch 221/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1778.5852 - val_loss: 1573.1212\n",
      "Epoch 222/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1776.1056 - val_loss: 1570.7406\n",
      "Epoch 223/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1773.6386 - val_loss: 1568.3722\n",
      "Epoch 224/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1771.1841 - val_loss: 1566.0158\n",
      "Epoch 225/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1768.7420 - val_loss: 1563.6716\n",
      "Epoch 226/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1766.3121 - val_loss: 1561.3392\n",
      "Epoch 227/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1763.8944 - val_loss: 1559.0186\n",
      "Epoch 228/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1761.4887 - val_loss: 1556.7097\n",
      "Epoch 229/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1759.0950 - val_loss: 1554.4123\n",
      "Epoch 230/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1756.7131 - val_loss: 1552.1265\n",
      "Epoch 231/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1754.3428 - val_loss: 1549.8520\n",
      "Epoch 232/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1751.9843 - val_loss: 1547.5888\n",
      "Epoch 233/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1749.6372 - val_loss: 1545.3370\n",
      "Epoch 234/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1747.3017 - val_loss: 1543.0961\n",
      "Epoch 235/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1744.9775 - val_loss: 1540.8665\n",
      "Epoch 236/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1742.6646 - val_loss: 1538.6478\n",
      "Epoch 237/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1740.3628 - val_loss: 1536.4399\n",
      "Epoch 238/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1738.0720 - val_loss: 1534.2427\n",
      "Epoch 239/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1735.7922 - val_loss: 1532.0564\n",
      "Epoch 240/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1733.5232 - val_loss: 1529.8806\n",
      "Epoch 241/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1731.2651 - val_loss: 1527.7153\n",
      "Epoch 242/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1729.0175 - val_loss: 1525.5605\n",
      "Epoch 243/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1726.7806 - val_loss: 1523.4160\n",
      "Epoch 244/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1724.5542 - val_loss: 1521.2818\n",
      "Epoch 245/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1722.3382 - val_loss: 1519.1579\n",
      "Epoch 246/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1720.1326 - val_loss: 1517.0441\n",
      "Epoch 247/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1717.9372 - val_loss: 1514.9404\n",
      "Epoch 248/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1715.7519 - val_loss: 1512.8464\n",
      "Epoch 249/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1713.5768 - val_loss: 1510.7626\n",
      "Epoch 250/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1711.4116 - val_loss: 1508.6886\n",
      "Epoch 251/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1709.2564 - val_loss: 1506.6242\n",
      "Epoch 252/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1707.1109 - val_loss: 1504.5697\n",
      "Epoch 253/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1704.9753 - val_loss: 1502.5246\n",
      "Epoch 254/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1702.8494 - val_loss: 1500.4892\n",
      "Epoch 255/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1700.7330 - val_loss: 1498.4630\n",
      "Epoch 256/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1698.6260 - val_loss: 1496.4463\n",
      "Epoch 257/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1696.5285 - val_loss: 1494.4390\n",
      "Epoch 258/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1694.4404 - val_loss: 1492.4409\n",
      "Epoch 259/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1692.3616 - val_loss: 1490.4519\n",
      "Epoch 260/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1690.2919 - val_loss: 1488.4720\n",
      "Epoch 261/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1688.2314 - val_loss: 1486.5011\n",
      "Epoch 262/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1686.1799 - val_loss: 1484.5394\n",
      "Epoch 263/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1684.1375 - val_loss: 1482.5864\n",
      "Epoch 264/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1682.1040 - val_loss: 1480.6422\n",
      "Epoch 265/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1680.0793 - val_loss: 1478.7069\n",
      "Epoch 266/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1678.0634 - val_loss: 1476.7803\n",
      "Epoch 267/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1676.0562 - val_loss: 1474.8624\n",
      "Epoch 268/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1674.0577 - val_loss: 1472.9529\n",
      "Epoch 269/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1672.0676 - val_loss: 1471.0521\n",
      "Epoch 270/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1670.0861 - val_loss: 1469.1597\n",
      "Epoch 271/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1668.1129 - val_loss: 1467.2757\n",
      "Epoch 272/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1666.1482 - val_loss: 1465.4000\n",
      "Epoch 273/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1664.1918 - val_loss: 1463.5325\n",
      "Epoch 274/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1662.2435 - val_loss: 1461.6733\n",
      "Epoch 275/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1660.3034 - val_loss: 1459.8222\n",
      "Epoch 276/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1658.3714 - val_loss: 1457.9792\n",
      "Epoch 277/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1656.4474 - val_loss: 1456.1444\n",
      "Epoch 278/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1654.5315 - val_loss: 1454.3173\n",
      "Epoch 279/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1652.6235 - val_loss: 1452.4983\n",
      "Epoch 280/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1650.7233 - val_loss: 1450.6871\n",
      "Epoch 281/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1648.8309 - val_loss: 1448.8838\n",
      "Epoch 282/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1646.9461 - val_loss: 1447.0881\n",
      "Epoch 283/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1645.0691 - val_loss: 1445.3002\n",
      "Epoch 284/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1643.1996 - val_loss: 1443.5199\n",
      "Epoch 285/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1641.3377 - val_loss: 1441.7472\n",
      "Epoch 286/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1639.4833 - val_loss: 1439.9821\n",
      "Epoch 287/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1637.6364 - val_loss: 1438.2244\n",
      "Epoch 288/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1635.7968 - val_loss: 1436.4741\n",
      "Epoch 289/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1633.9645 - val_loss: 1434.7313\n",
      "Epoch 290/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1632.1395 - val_loss: 1432.9957\n",
      "Epoch 291/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1630.3217 - val_loss: 1431.2676\n",
      "Epoch 292/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1628.5111 - val_loss: 1429.5465\n",
      "Epoch 293/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1626.7075 - val_loss: 1427.8327\n",
      "Epoch 294/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1624.9111 - val_loss: 1426.1259\n",
      "Epoch 295/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1623.1216 - val_loss: 1424.4263\n",
      "Epoch 296/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1621.3389 - val_loss: 1422.7337\n",
      "Epoch 297/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1619.5633 - val_loss: 1421.0481\n",
      "Epoch 298/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1617.7943 - val_loss: 1419.3693\n",
      "Epoch 299/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1616.0323 - val_loss: 1417.6975\n",
      "Epoch 300/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1614.2769 - val_loss: 1416.0325\n",
      "Epoch 301/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1612.5282 - val_loss: 1414.3744\n",
      "Epoch 302/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1610.7861 - val_loss: 1412.7228\n",
      "Epoch 303/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1609.0506 - val_loss: 1411.0781\n",
      "Epoch 304/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1607.3216 - val_loss: 1409.4399\n",
      "Epoch 305/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1605.5992 - val_loss: 1407.8085\n",
      "Epoch 306/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1603.8831 - val_loss: 1406.1836\n",
      "Epoch 307/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1602.1734 - val_loss: 1404.5651\n",
      "Epoch 308/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1600.4700 - val_loss: 1402.9531\n",
      "Epoch 309/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1598.7729 - val_loss: 1401.3475\n",
      "Epoch 310/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1597.0820 - val_loss: 1399.7483\n",
      "Epoch 311/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1595.3972 - val_loss: 1398.1554\n",
      "Epoch 312/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1593.7187 - val_loss: 1396.5689\n",
      "Epoch 313/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1592.0463 - val_loss: 1394.9886\n",
      "Epoch 314/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1590.3799 - val_loss: 1393.4146\n",
      "Epoch 315/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1588.7195 - val_loss: 1391.8467\n",
      "Epoch 316/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1587.0651 - val_loss: 1390.2850\n",
      "Epoch 317/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1585.4166 - val_loss: 1388.7294\n",
      "Epoch 318/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1583.7740 - val_loss: 1387.1797\n",
      "Epoch 319/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1582.1371 - val_loss: 1385.6361\n",
      "Epoch 320/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1580.5061 - val_loss: 1384.0985\n",
      "Epoch 321/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1578.8808 - val_loss: 1382.5669\n",
      "Epoch 322/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1577.2613 - val_loss: 1381.0411\n",
      "Epoch 323/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1575.6473 - val_loss: 1379.5211\n",
      "Epoch 324/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1574.0390 - val_loss: 1378.0069\n",
      "Epoch 325/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1572.4362 - val_loss: 1376.4985\n",
      "Epoch 326/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1570.8389 - val_loss: 1374.9959\n",
      "Epoch 327/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1569.2472 - val_loss: 1373.4989\n",
      "Epoch 328/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1567.6609 - val_loss: 1372.0077\n",
      "Epoch 329/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1566.0801 - val_loss: 1370.5221\n",
      "Epoch 330/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1564.5046 - val_loss: 1369.0420\n",
      "Epoch 331/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1562.9344 - val_loss: 1367.5675\n",
      "Epoch 332/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1561.3695 - val_loss: 1366.0985\n",
      "Epoch 333/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1559.8099 - val_loss: 1364.6350\n",
      "Epoch 334/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1558.2556 - val_loss: 1363.1770\n",
      "Epoch 335/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1556.7064 - val_loss: 1361.7244\n",
      "Epoch 336/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1555.1623 - val_loss: 1360.2771\n",
      "Epoch 337/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1553.6233 - val_loss: 1358.8352\n",
      "Epoch 338/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1552.0894 - val_loss: 1357.3987\n",
      "Epoch 339/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1550.5606 - val_loss: 1355.9674\n",
      "Epoch 340/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1549.0367 - val_loss: 1354.5412\n",
      "Epoch 341/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1547.5177 - val_loss: 1353.1203\n",
      "Epoch 342/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1546.0036 - val_loss: 1351.7046\n",
      "Epoch 343/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1544.4945 - val_loss: 1350.2940\n",
      "Epoch 344/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1542.9902 - val_loss: 1348.8886\n",
      "Epoch 345/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1541.4908 - val_loss: 1347.4881\n",
      "Epoch 346/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1539.9960 - val_loss: 1346.0927\n",
      "Epoch 347/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1538.5061 - val_loss: 1344.7025\n",
      "Epoch 348/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1537.0209 - val_loss: 1343.3171\n",
      "Epoch 349/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1535.5403 - val_loss: 1341.9367\n",
      "Epoch 350/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1534.0644 - val_loss: 1340.56121583.57\n",
      "Epoch 351/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1532.5931 - val_loss: 1339.1906\n",
      "Epoch 352/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1531.1263 - val_loss: 1337.8249\n",
      "Epoch 353/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1529.6640 - val_loss: 1336.4639\n",
      "Epoch 354/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1528.2063 - val_loss: 1335.1078\n",
      "Epoch 355/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1526.7530 - val_loss: 1333.7563\n",
      "Epoch 356/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1525.3042 - val_loss: 1332.4096\n",
      "Epoch 357/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1523.8598 - val_loss: 1331.0677\n",
      "Epoch 358/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1522.4198 - val_loss: 1329.7305\n",
      "Epoch 359/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1520.9842 - val_loss: 1328.3979\n",
      "Epoch 360/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1519.5529 - val_loss: 1327.0698\n",
      "Epoch 361/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1518.1258 - val_loss: 1325.7464\n",
      "Epoch 362/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1516.7030 - val_loss: 1324.4276\n",
      "Epoch 363/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1515.2844 - val_loss: 1323.1133\n",
      "Epoch 364/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1513.8700 - val_loss: 1321.8034\n",
      "Epoch 365/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1512.4598 - val_loss: 1320.4980\n",
      "Epoch 366/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1511.0536 - val_loss: 1319.1971\n",
      "Epoch 367/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1509.6517 - val_loss: 1317.9006\n",
      "Epoch 368/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1508.2537 - val_loss: 1316.6086\n",
      "Epoch 369/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1506.8599 - val_loss: 1315.3208\n",
      "Epoch 370/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1505.4701 - val_loss: 1314.0375\n",
      "Epoch 371/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1504.0843 - val_loss: 1312.7585\n",
      "Epoch 372/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1502.7025 - val_loss: 1311.4838\n",
      "Epoch 373/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1501.3245 - val_loss: 1310.2133\n",
      "Epoch 374/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1499.9506 - val_loss: 1308.9471\n",
      "Epoch 375/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1498.5804 - val_loss: 1307.6851\n",
      "Epoch 376/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1497.2141 - val_loss: 1306.4273\n",
      "Epoch 377/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1495.8517 - val_loss: 1305.1736\n",
      "Epoch 378/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1494.4931 - val_loss: 1303.9241\n",
      "Epoch 379/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1493.1383 - val_loss: 1302.6788\n",
      "Epoch 380/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1491.7872 - val_loss: 1301.4374\n",
      "Epoch 381/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1490.4398 - val_loss: 1300.2001\n",
      "Epoch 382/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1489.0960 - val_loss: 1298.9669\n",
      "Epoch 383/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1487.7561 - val_loss: 1297.7377\n",
      "Epoch 384/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1486.4197 - val_loss: 1296.5126\n",
      "Epoch 385/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1485.0870 - val_loss: 1295.2913\n",
      "Epoch 386/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1483.7578 - val_loss: 1294.0740\n",
      "Epoch 387/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1482.4322 - val_loss: 1292.8606\n",
      "Epoch 388/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1481.1103 - val_loss: 1291.6512\n",
      "Epoch 389/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1479.7919 - val_loss: 1290.4457\n",
      "Epoch 390/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1478.4769 - val_loss: 1289.2440\n",
      "Epoch 391/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1477.1653 - val_loss: 1288.0461\n",
      "Epoch 392/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1475.8573 - val_loss: 1286.8520\n",
      "Epoch 393/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1474.5526 - val_loss: 1285.6617\n",
      "Epoch 394/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1473.2515 - val_loss: 1284.4752\n",
      "Epoch 395/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1471.9536 - val_loss: 1283.2924\n",
      "Epoch 396/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1470.6591 - val_loss: 1282.1134\n",
      "Epoch 397/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1469.3680 - val_loss: 1280.9381\n",
      "Epoch 398/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1468.0803 - val_loss: 1279.7665\n",
      "Epoch 399/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1466.7958 - val_loss: 1278.5985\n",
      "Epoch 400/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1465.5145 - val_loss: 1277.4342\n",
      "Epoch 401/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1464.2365 - val_loss: 1276.2735\n",
      "Epoch 402/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1462.9619 - val_loss: 1275.1164\n",
      "Epoch 403/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1461.6903 - val_loss: 1273.9628\n",
      "Epoch 404/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1460.4220 - val_loss: 1272.8129\n",
      "Epoch 405/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1459.1569 - val_loss: 1271.6663\n",
      "Epoch 406/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1457.8948 - val_loss: 1270.5235\n",
      "Epoch 407/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1456.6359 - val_loss: 1269.3840\n",
      "Epoch 408/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1455.3802 - val_loss: 1268.2481\n",
      "Epoch 409/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1454.1275 - val_loss: 1267.1156\n",
      "Epoch 410/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1452.8778 - val_loss: 1265.9866\n",
      "Epoch 411/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1451.6312 - val_loss: 1264.8610\n",
      "Epoch 412/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1450.3878 - val_loss: 1263.7389\n",
      "Epoch 413/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1449.1473 - val_loss: 1262.6200\n",
      "Epoch 414/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1447.9098 - val_loss: 1261.5046\n",
      "Epoch 415/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1446.6753 - val_loss: 1260.3924\n",
      "Epoch 416/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1445.4437 - val_loss: 1259.2837\n",
      "Epoch 417/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1444.2151 - val_loss: 1258.1784\n",
      "Epoch 418/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1442.9894 - val_loss: 1257.0762\n",
      "Epoch 419/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1441.7665 - val_loss: 1255.9774\n",
      "Epoch 420/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1440.5466 - val_loss: 1254.8817\n",
      "Epoch 421/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1439.3295 - val_loss: 1253.7893\n",
      "Epoch 422/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1438.1153 - val_loss: 1252.7001\n",
      "Epoch 423/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1436.9039 - val_loss: 1251.6142\n",
      "Epoch 424/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1435.6953 - val_loss: 1250.5314\n",
      "Epoch 425/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1434.4895 - val_loss: 1249.4518\n",
      "Epoch 426/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1433.2865 - val_loss: 1248.3755\n",
      "Epoch 427/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1432.0862 - val_loss: 1247.3022\n",
      "Epoch 428/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1430.8888 - val_loss: 1246.2320\n",
      "Epoch 429/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1429.6940 - val_loss: 1245.1650\n",
      "Epoch 430/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1428.5020 - val_loss: 1244.1011\n",
      "Epoch 431/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1427.3127 - val_loss: 1243.0402\n",
      "Epoch 432/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1426.1261 - val_loss: 1241.9824\n",
      "Epoch 433/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1424.9422 - val_loss: 1240.9277\n",
      "Epoch 434/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1423.7609 - val_loss: 1239.8760\n",
      "Epoch 435/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1422.5822 - val_loss: 1238.8272\n",
      "Epoch 436/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1421.4060 - val_loss: 1237.7815\n",
      "Epoch 437/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1420.2327 - val_loss: 1236.7388\n",
      "Epoch 438/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1419.0619 - val_loss: 1235.6990\n",
      "Epoch 439/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1417.8936 - val_loss: 1234.6622\n",
      "Epoch 440/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1416.7279 - val_loss: 1233.6283\n",
      "Epoch 441/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1415.5649 - val_loss: 1232.5973\n",
      "Epoch 442/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1414.4042 - val_loss: 1231.5692\n",
      "Epoch 443/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1413.2462 - val_loss: 1230.5440\n",
      "Epoch 444/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1412.0906 - val_loss: 1229.5217\n",
      "Epoch 445/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1410.9376 - val_loss: 1228.5023\n",
      "Epoch 446/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1409.7870 - val_loss: 1227.4858\n",
      "Epoch 447/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1408.6389 - val_loss: 1226.4720\n",
      "Epoch 448/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1407.4933 - val_loss: 1225.4612\n",
      "Epoch 449/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1406.3502 - val_loss: 1224.4532\n",
      "Epoch 450/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1405.2096 - val_loss: 1223.4478\n",
      "Epoch 451/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1404.0713 - val_loss: 1222.4454\n",
      "Epoch 452/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1402.9355 - val_loss: 1221.4456\n",
      "Epoch 453/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1401.8020 - val_loss: 1220.4486\n",
      "Epoch 454/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1400.6709 - val_loss: 1219.4545\n",
      "Epoch 455/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1399.5422 - val_loss: 1218.4629\n",
      "Epoch 456/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1398.4158 - val_loss: 1217.4741\n",
      "Epoch 457/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1397.2918 - val_loss: 1216.4880\n",
      "Epoch 458/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1396.1701 - val_loss: 1215.5046\n",
      "Epoch 459/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1395.0508 - val_loss: 1214.5239\n",
      "Epoch 460/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1393.9338 - val_loss: 1213.5458\n",
      "Epoch 461/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1392.8191 - val_loss: 1212.5704\n",
      "Epoch 462/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1391.7066 - val_loss: 1211.5976\n",
      "Epoch 463/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1390.5966 - val_loss: 1210.6276\n",
      "Epoch 464/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1389.4888 - val_loss: 1209.6600\n",
      "Epoch 465/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1388.3832 - val_loss: 1208.6951\n",
      "Epoch 466/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1387.2799 - val_loss: 1207.7327\n",
      "Epoch 467/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1386.1788 - val_loss: 1206.7730\n",
      "Epoch 468/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1385.0801 - val_loss: 1205.8159\n",
      "Epoch 469/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1383.9835 - val_loss: 1204.8612\n",
      "Epoch 470/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1382.8892 - val_loss: 1203.9092\n",
      "Epoch 471/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1381.7970 - val_loss: 1202.9596\n",
      "Epoch 472/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1380.7071 - val_loss: 1202.0126\n",
      "Epoch 473/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1379.6193 - val_loss: 1201.0681\n",
      "Epoch 474/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1378.5337 - val_loss: 1200.1261\n",
      "Epoch 475/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1377.4502 - val_loss: 1199.1865\n",
      "Epoch 476/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1376.3690 - val_loss: 1198.2495\n",
      "Epoch 477/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1375.2898 - val_loss: 1197.3149\n",
      "Epoch 478/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1374.2129 - val_loss: 1196.3827\n",
      "Epoch 479/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1373.1380 - val_loss: 1195.4530\n",
      "Epoch 480/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1372.0652 - val_loss: 1194.5257\n",
      "Epoch 481/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1370.9946 - val_loss: 1193.6008\n",
      "Epoch 482/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1369.9262 - val_loss: 1192.6784\n",
      "Epoch 483/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1368.8598 - val_loss: 1191.7584\n",
      "Epoch 484/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1367.7955 - val_loss: 1190.8408\n",
      "Epoch 485/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1366.7333 - val_loss: 1189.9255\n",
      "Epoch 486/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1365.6731 - val_loss: 1189.01250.2\n",
      "Epoch 487/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1364.6151 - val_loss: 1188.1020\n",
      "Epoch 488/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1363.5591 - val_loss: 1187.1938\n",
      "Epoch 489/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1362.5051 - val_loss: 1186.2879\n",
      "Epoch 490/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1361.4532 - val_loss: 1185.3844\n",
      "Epoch 491/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1360.4033 - val_loss: 1184.4832\n",
      "Epoch 492/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1359.3554 - val_loss: 1183.5842\n",
      "Epoch 493/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1358.3097 - val_loss: 1182.6877\n",
      "Epoch 494/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1357.2658 - val_loss: 1181.7933\n",
      "Epoch 495/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1356.2240 - val_loss: 1180.9013\n",
      "Epoch 496/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1355.1842 - val_loss: 1180.0116\n",
      "Epoch 497/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1354.1465 - val_loss: 1179.1240\n",
      "Epoch 498/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1353.1106 - val_loss: 1178.2387\n",
      "Epoch 499/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1352.0768 - val_loss: 1177.3557\n",
      "Epoch 500/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1351.0449 - val_loss: 1176.4749\n",
      "Epoch 501/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1350.0150 - val_loss: 1175.5963\n",
      "Epoch 502/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1348.9871 - val_loss: 1174.7200\n",
      "Epoch 503/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1347.9611 - val_loss: 1173.8458\n",
      "Epoch 504/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1346.9371 - val_loss: 1172.9737\n",
      "Epoch 505/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1345.9150 - val_loss: 1172.1040\n",
      "Epoch 506/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1344.8948 - val_loss: 1171.2364\n",
      "Epoch 507/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1343.8766 - val_loss: 1170.3709\n",
      "Epoch 508/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1342.8603 - val_loss: 1169.5076\n",
      "Epoch 509/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1341.8459 - val_loss: 1168.6465\n",
      "Epoch 510/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1340.8333 - val_loss: 1167.7874\n",
      "Epoch 511/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1339.8227 - val_loss: 1166.9305\n",
      "Epoch 512/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1338.8141 - val_loss: 1166.0758\n",
      "Epoch 513/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1337.8073 - val_loss: 1165.2232\n",
      "Epoch 514/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1336.8024 - val_loss: 1164.3727\n",
      "Epoch 515/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1335.7993 - val_loss: 1163.5241\n",
      "Epoch 516/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1334.7981 - val_loss: 1162.6778\n",
      "Epoch 517/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1333.7989 - val_loss: 1161.8335\n",
      "Epoch 518/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1332.8015 - val_loss: 1160.9912\n",
      "Epoch 519/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1331.8059 - val_loss: 1160.1511\n",
      "Epoch 520/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1330.8122 - val_loss: 1159.3130\n",
      "Epoch 521/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1329.8204 - val_loss: 1158.4770\n",
      "Epoch 522/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1328.8303 - val_loss: 1157.6429\n",
      "Epoch 523/10000\n",
      " 800/1126 [====================>.........] - ETA: 0s - loss: 1492.7092"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2373c51e407d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1195\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1196\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1198\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1337\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.plot(history.history['loss'], 'r-')\n",
    "plt.plot(history.history['val_loss'], 'b-')\n",
    "plt.show()\n",
    "\n",
    "plt.title('y_pred, y_test')\n",
    "\n",
    "plt.plot(y_pred[:], y_test[:], '+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataframe_to_xy_sequences(df_train, 4)\n",
    "X_valid, y_valid = dataframe_to_xy_sequences(df_valid, 4)\n",
    "X_test, y_test = dataframe_to_xy_sequences(df_test, 4)\n",
    "\n",
    "def simple_rnn_model(nb_units, input_dim, loss='mean_squared_error', optimizer='adagrad'):\n",
    "    print(input_dim)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(nb_units, batch_input_shape=(96, 4, 8)))#input_dim=input_dim[1], input_length=input_dim[0], return_sequences=True))\n",
    "    #model.add(Dense(nb_units))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = simple_rnn_model(16, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
