{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/micro_sud3_normalized.pkl')\n",
    "df = df.reset_index()\n",
    "df.head()\n",
    "\n",
    "def split_dataframe(dataframe, percent):\n",
    "    nb_rows = int(np.floor(percent * len(dataframe)))\n",
    "    return dataframe[:nb_rows], dataframe[nb_rows:]\n",
    "    \"\"\"\n",
    "def dataframe_to_xy(df, sequence_size):\n",
    "\n",
    "    out_X = np.zeros((len(df)//sequence_size, sequence_size, 8))\n",
    "    out_y = np.zeros((len(df)//sequence_size, sequence_size))\n",
    "    i = 0\n",
    "    while i + sequence_size < len(df):\n",
    "        sequence = df.iloc[i:i+sequence_size]\n",
    "        out_X[i//sequence_size] =  np.array(sequence[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                                 'tgrad', 'pressure', 'pluvio']])\n",
    "        out_y[i//sequence_size] = np.array(sequence['NO2_ref'])\n",
    "        i += sequence_size\n",
    "        \n",
    "    return out_X, out_y\n",
    "    \"\"\"\n",
    "\n",
    "def dataframe_to_xy(df):\n",
    "    return (np.array(df[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                         'tgrad', 'pressure', 'pluvio']]),\\\n",
    "            np.array(df['NO2_ref']))\n",
    "\n",
    "        \n",
    "df_train, df_test = split_dataframe(df, 0.5) \n",
    "df_valid, df_test = split_dataframe(df_test, 0.5)\n",
    "\n",
    "X_train, y_train = dataframe_to_xy(df_train)\n",
    "X_valid, y_valid = dataframe_to_xy(df_valid)\n",
    "X_test, y_test = dataframe_to_xy(df_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_valid= X_valid.reshape((X_valid.shape[0], 1,  X_valid.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "#y_train = y_train.reshape((1, len(y_train)))\n",
    "#y_valid = y_valid.reshape((1, len(y_valid)))\n",
    "#y_test = y_test.reshape((1, len(y_test)))\n",
    "\n",
    "def dataframe_to_xy_sequences(df, sequence_size):\n",
    "    out_X = np.zeros((len(df)//sequence_size, sequence_size, 8))\n",
    "    out_y = np.zeros((len(df)//sequence_size, sequence_size))\n",
    "    i = 0\n",
    "    while i + sequence_size < len(df):\n",
    "        sequence = df.iloc[i:i+sequence_size]\n",
    "        out_X[i//sequence_size] =  np.array(sequence[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                                 'tgrad', 'pressure', 'pluvio']])\n",
    "        out_y[i//sequence_size] = np.array(sequence['NO2_ref'])\n",
    "        i += sequence_size\n",
    "        \n",
    "    return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def simple_rnn_model(nb_units, input_dim, loss='mean_squared_error', optimizer='adam'):\n",
    "    print(input_dim)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(nb_units, input_shape=input_dim))#input_dim=input_dim[1], input_length=input_dim[0], return_sequences=True))\n",
    "    #model.add(Dense(nb_units))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 417\n",
      "Trainable params: 417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_rnn_model(16, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1126 samples, validate on 563 samples\n",
      "Epoch 1/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3352.5525 - val_loss: 3015.8167\n",
      "Epoch 2/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3342.8907 - val_loss: 3007.8665\n",
      "Epoch 3/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3331.3117 - val_loss: 2998.1805\n",
      "Epoch 4/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3316.6519 - val_loss: 2986.1466\n",
      "Epoch 5/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3298.6062 - val_loss: 2971.3558\n",
      "Epoch 6/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3277.0715 - val_loss: 2953.5259\n",
      "Epoch 7/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3251.8953 - val_loss: 2932.4591\n",
      "Epoch 8/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3222.9662 - val_loss: 2907.9869\n",
      "Epoch 9/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3190.3740 - val_loss: 2880.1756\n",
      "Epoch 10/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3154.5252 - val_loss: 2849.3402\n",
      "Epoch 11/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3115.9611 - val_loss: 2815.9012\n",
      "Epoch 12/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3075.2092 - val_loss: 2780.2580\n",
      "Epoch 13/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3032.7709 - val_loss: 2742.8475\n",
      "Epoch 14/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2989.1090 - val_loss: 2704.0676\n",
      "Epoch 15/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2944.6258 - val_loss: 2664.2989\n",
      "Epoch 16/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2899.6652 - val_loss: 2623.8668\n",
      "Epoch 17/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2854.5096 - val_loss: 2583.0409\n",
      "Epoch 18/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2809.4039 - val_loss: 2542.0776\n",
      "Epoch 19/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2764.5602 - val_loss: 2501.1598\n",
      "Epoch 20/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2720.1379 - val_loss: 2460.4527\n",
      "Epoch 21/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2676.2737 - val_loss: 2420.0873\n",
      "Epoch 22/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2633.0691 - val_loss: 2380.1669\n",
      "Epoch 23/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2590.6080 - val_loss: 2340.7849\n",
      "Epoch 24/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2548.9529 - val_loss: 2301.9984\n",
      "Epoch 25/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2508.1394 - val_loss: 2263.8592\n",
      "Epoch 26/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2468.1954 - val_loss: 2226.4079\n",
      "Epoch 27/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2429.1406 - val_loss: 2189.6751\n",
      "Epoch 28/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2390.9821 - val_loss: 2153.6795\n",
      "Epoch 29/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2353.7224 - val_loss: 2118.4383\n",
      "Epoch 30/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2317.3571 - val_loss: 2083.9597\n",
      "Epoch 31/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2281.8803 - val_loss: 2050.2519\n",
      "Epoch 32/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2247.2819 - val_loss: 2017.3110\n",
      "Epoch 33/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2213.5472 - val_loss: 1985.1357\n",
      "Epoch 34/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2180.6637 - val_loss: 1953.7241\n",
      "Epoch 35/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2148.6182 - val_loss: 1923.0657\n",
      "Epoch 36/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2117.3927 - val_loss: 1893.1504\n",
      "Epoch 37/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2086.9699 - val_loss: 1863.9658\n",
      "Epoch 38/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2057.3334 - val_loss: 1835.4998\n",
      "Epoch 39/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2028.4651 - val_loss: 1807.7424\n",
      "Epoch 40/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2000.3478 - val_loss: 1780.6781\n",
      "Epoch 41/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1972.9624 - val_loss: 1754.2944\n",
      "Epoch 42/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1946.2908 - val_loss: 1728.5775\n",
      "Epoch 43/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1920.3130 - val_loss: 1703.5115\n",
      "Epoch 44/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1895.0093 - val_loss: 1679.0830\n",
      "Epoch 45/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1870.3605 - val_loss: 1655.2764\n",
      "Epoch 46/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1846.3464 - val_loss: 1632.0786\n",
      "Epoch 47/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1822.9467 - val_loss: 1609.4737\n",
      "Epoch 48/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1800.1408 - val_loss: 1587.4488\n",
      "Epoch 49/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1777.9091 - val_loss: 1565.9894\n",
      "Epoch 50/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1756.2314 - val_loss: 1545.0798\n",
      "Epoch 51/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1735.0874 - val_loss: 1524.7055\n",
      "Epoch 52/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1714.4575 - val_loss: 1504.8520\n",
      "Epoch 53/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1694.3217 - val_loss: 1485.5038\n",
      "Epoch 54/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1674.6603 - val_loss: 1466.6458\n",
      "Epoch 55/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1655.4534 - val_loss: 1448.2633\n",
      "Epoch 56/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1636.6813 - val_loss: 1430.3412\n",
      "Epoch 57/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1618.3237 - val_loss: 1412.8638\n",
      "Epoch 58/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1600.3608 - val_loss: 1395.8166\n",
      "Epoch 59/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1582.7720 - val_loss: 1379.1841\n",
      "Epoch 60/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1565.5369 - val_loss: 1362.9520\n",
      "Epoch 61/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1548.6347 - val_loss: 1347.1050\n",
      "Epoch 62/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1532.0448 - val_loss: 1331.628356.\n",
      "Epoch 63/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1515.7467 - val_loss: 1316.5081\n",
      "Epoch 64/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1499.7202 - val_loss: 1301.7299\n",
      "Epoch 65/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1483.9456 - val_loss: 1287.2800\n",
      "Epoch 66/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1468.4039 - val_loss: 1273.1456\n",
      "Epoch 67/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1453.0776 - val_loss: 1259.3145\n",
      "Epoch 68/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1437.9508 - val_loss: 1245.7752\n",
      "Epoch 69/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1423.0096 - val_loss: 1232.5174\n",
      "Epoch 70/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1408.2421 - val_loss: 1219.5316\n",
      "Epoch 71/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1393.6394 - val_loss: 1206.8088\n",
      "Epoch 72/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1379.1950 - val_loss: 1194.3410\n",
      "Epoch 73/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1364.9054 - val_loss: 1182.1213\n",
      "Epoch 74/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1350.7691 - val_loss: 1170.1427\n",
      "Epoch 75/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1336.7869 - val_loss: 1158.3989\n",
      "Epoch 76/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1322.9604 - val_loss: 1146.8836\n",
      "Epoch 77/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1309.2922 - val_loss: 1135.5903\n",
      "Epoch 78/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1295.7852 - val_loss: 1124.5125\n",
      "Epoch 79/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1282.4418 - val_loss: 1113.6443\n",
      "Epoch 80/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1269.2637 - val_loss: 1102.9789\n",
      "Epoch 81/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1256.2512 - val_loss: 1092.5091\n",
      "Epoch 82/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1243.4040 - val_loss: 1082.2287\n",
      "Epoch 83/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1230.7200 - val_loss: 1072.1304\n",
      "Epoch 84/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1218.1967 - val_loss: 1062.2074\n",
      "Epoch 85/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1205.8309 - val_loss: 1052.4530\n",
      "Epoch 86/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1193.6185 - val_loss: 1042.8603\n",
      "Epoch 87/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1181.5554 - val_loss: 1033.4218\n",
      "Epoch 88/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1169.6370 - val_loss: 1024.1309\n",
      "Epoch 89/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1157.8586 - val_loss: 1014.9809\n",
      "Epoch 90/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1146.2153 - val_loss: 1005.9640\n",
      "Epoch 91/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1134.7025 - val_loss: 997.0733\n",
      "Epoch 92/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1123.3156 - val_loss: 988.3018\n",
      "Epoch 93/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1112.0499 - val_loss: 979.6425\n",
      "Epoch 94/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1100.9010 - val_loss: 971.0881\n",
      "Epoch 95/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1089.8647 - val_loss: 962.6316\n",
      "Epoch 96/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1078.9366 - val_loss: 954.2660\n",
      "Epoch 97/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1068.1125 - val_loss: 945.9843\n",
      "Epoch 98/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1057.3886 - val_loss: 937.7799\n",
      "Epoch 99/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1046.7611 - val_loss: 929.6453\n",
      "Epoch 100/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1036.2259 - val_loss: 921.5742\n",
      "Epoch 101/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1025.7797 - val_loss: 913.5600\n",
      "Epoch 102/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1015.4188 - val_loss: 905.5959\n",
      "Epoch 103/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1005.1400 - val_loss: 897.6757\n",
      "Epoch 104/10000\n",
      "1126/1126 [==============================] - 0s - loss: 994.9403 - val_loss: 889.7937\n",
      "Epoch 105/10000\n",
      "1126/1126 [==============================] - 0s - loss: 984.8164 - val_loss: 881.9428\n",
      "Epoch 106/10000\n",
      "1126/1126 [==============================] - 0s - loss: 974.7653 - val_loss: 874.1176\n",
      "Epoch 107/10000\n",
      "1126/1126 [==============================] - 0s - loss: 964.7847 - val_loss: 866.3128\n",
      "Epoch 108/10000\n",
      "1126/1126 [==============================] - 0s - loss: 954.8719 - val_loss: 858.5221\n",
      "Epoch 109/10000\n",
      "1126/1126 [==============================] - 0s - loss: 945.0244 - val_loss: 850.7408\n",
      "Epoch 110/10000\n",
      "1126/1126 [==============================] - 0s - loss: 935.2406 - val_loss: 842.9642\n",
      "Epoch 111/10000\n",
      "1126/1126 [==============================] - 0s - loss: 925.5179 - val_loss: 835.1872\n",
      "Epoch 112/10000\n",
      "1126/1126 [==============================] - 0s - loss: 915.8551 - val_loss: 827.4062\n",
      "Epoch 113/10000\n",
      "1126/1126 [==============================] - 0s - loss: 906.2505 - val_loss: 819.6170\n",
      "Epoch 114/10000\n",
      "1126/1126 [==============================] - 0s - loss: 896.7033 - val_loss: 811.8174\n",
      "Epoch 115/10000\n",
      "1126/1126 [==============================] - 0s - loss: 887.2123 - val_loss: 804.0038\n",
      "Epoch 116/10000\n",
      "1126/1126 [==============================] - 0s - loss: 877.7769 - val_loss: 796.1744\n",
      "Epoch 117/10000\n",
      "1126/1126 [==============================] - 0s - loss: 868.3969 - val_loss: 788.3282\n",
      "Epoch 118/10000\n",
      "1126/1126 [==============================] - 0s - loss: 859.0722 - val_loss: 780.4642\n",
      "Epoch 119/10000\n",
      "1126/1126 [==============================] - 0s - loss: 849.8028 - val_loss: 772.5824\n",
      "Epoch 120/10000\n",
      "1126/1126 [==============================] - 0s - loss: 840.5896 - val_loss: 764.6837\n",
      "Epoch 121/10000\n",
      "1126/1126 [==============================] - 0s - loss: 831.4333 - val_loss: 756.7696\n",
      "Epoch 122/10000\n",
      "1126/1126 [==============================] - 0s - loss: 822.3348 - val_loss: 748.8425\n",
      "Epoch 123/10000\n",
      "1126/1126 [==============================] - 0s - loss: 813.2955 - val_loss: 740.9052\n",
      "Epoch 124/10000\n",
      "1126/1126 [==============================] - 0s - loss: 804.3170 - val_loss: 732.9618\n",
      "Epoch 125/10000\n",
      "1126/1126 [==============================] - 0s - loss: 795.4011 - val_loss: 725.0164\n",
      "Epoch 126/10000\n",
      "1126/1126 [==============================] - 0s - loss: 786.5494 - val_loss: 717.0744\n",
      "Epoch 127/10000\n",
      "1126/1126 [==============================] - 0s - loss: 777.7639 - val_loss: 709.1408\n",
      "Epoch 128/10000\n",
      "1126/1126 [==============================] - 0s - loss: 769.0465 - val_loss: 701.2216\n",
      "Epoch 129/10000\n",
      "1126/1126 [==============================] - 0s - loss: 760.3994 - val_loss: 693.3236\n",
      "Epoch 130/10000\n",
      "1126/1126 [==============================] - 0s - loss: 751.8244 - val_loss: 685.4528\n",
      "Epoch 131/10000\n",
      "1126/1126 [==============================] - 0s - loss: 743.3232 - val_loss: 677.6159\n",
      "Epoch 132/10000\n",
      "1126/1126 [==============================] - 0s - loss: 734.8978 - val_loss: 669.8195\n",
      "Epoch 133/10000\n",
      "1126/1126 [==============================] - 0s - loss: 726.5497 - val_loss: 662.0697\n",
      "Epoch 134/10000\n",
      "1126/1126 [==============================] - 0s - loss: 718.2804 - val_loss: 654.3728\n",
      "Epoch 135/10000\n",
      "1126/1126 [==============================] - 0s - loss: 710.0914 - val_loss: 646.7354\n",
      "Epoch 136/10000\n",
      "1126/1126 [==============================] - 0s - loss: 701.9836 - val_loss: 639.1622\n",
      "Epoch 137/10000\n",
      "1126/1126 [==============================] - 0s - loss: 693.9584 - val_loss: 631.6585\n",
      "Epoch 138/10000\n",
      "1126/1126 [==============================] - 0s - loss: 686.0169 - val_loss: 624.2300\n",
      "Epoch 139/10000\n",
      "1126/1126 [==============================] - 0s - loss: 678.1595 - val_loss: 616.8808\n",
      "Epoch 140/10000\n",
      "1126/1126 [==============================] - 0s - loss: 670.3871 - val_loss: 609.6149\n",
      "Epoch 141/10000\n",
      "1126/1126 [==============================] - 0s - loss: 662.7001 - val_loss: 602.4354\n",
      "Epoch 142/10000\n",
      "1126/1126 [==============================] - 0s - loss: 655.0989 - val_loss: 595.3451\n",
      "Epoch 143/10000\n",
      "1126/1126 [==============================] - 0s - loss: 647.5841 - val_loss: 588.3475\n",
      "Epoch 144/10000\n",
      "1126/1126 [==============================] - 0s - loss: 640.1558 - val_loss: 581.4444\n",
      "Epoch 145/10000\n",
      "1126/1126 [==============================] - 0s - loss: 632.8143 - val_loss: 574.6377\n",
      "Epoch 146/10000\n",
      "1126/1126 [==============================] - 0s - loss: 625.5596 - val_loss: 567.9289\n",
      "Epoch 147/10000\n",
      "1126/1126 [==============================] - 0s - loss: 618.3921 - val_loss: 561.3198\n",
      "Epoch 148/10000\n",
      "1126/1126 [==============================] - 0s - loss: 611.3116 - val_loss: 554.8109\n",
      "Epoch 149/10000\n",
      "1126/1126 [==============================] - 0s - loss: 604.3180 - val_loss: 548.4031\n",
      "Epoch 150/10000\n",
      "1126/1126 [==============================] - 0s - loss: 597.4115 - val_loss: 542.0967\n",
      "Epoch 151/10000\n",
      "1126/1126 [==============================] - 0s - loss: 590.5919 - val_loss: 535.8924\n",
      "Epoch 152/10000\n",
      "1126/1126 [==============================] - 0s - loss: 583.8589 - val_loss: 529.7897\n",
      "Epoch 153/10000\n",
      "1126/1126 [==============================] - 0s - loss: 577.2127 - val_loss: 523.7892\n",
      "Epoch 154/10000\n",
      "1126/1126 [==============================] - 0s - loss: 570.6526 - val_loss: 517.8904\n",
      "Epoch 155/10000\n",
      "1126/1126 [==============================] - 0s - loss: 564.1787 - val_loss: 512.0931\n",
      "Epoch 156/10000\n",
      "1126/1126 [==============================] - 0s - loss: 557.7907 - val_loss: 506.3967\n",
      "Epoch 157/10000\n",
      "1126/1126 [==============================] - 0s - loss: 551.4881 - val_loss: 500.8007\n",
      "Epoch 158/10000\n",
      "1126/1126 [==============================] - 0s - loss: 545.2708 - val_loss: 495.3043\n",
      "Epoch 159/10000\n",
      "1126/1126 [==============================] - 0s - loss: 539.1382 - val_loss: 489.9068\n",
      "Epoch 160/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 533.0903 - val_loss: 484.6076\n",
      "Epoch 161/10000\n",
      "1126/1126 [==============================] - 0s - loss: 527.1262 - val_loss: 479.4060\n",
      "Epoch 162/10000\n",
      "1126/1126 [==============================] - 0s - loss: 521.2456 - val_loss: 474.3009\n",
      "Epoch 163/10000\n",
      "1126/1126 [==============================] - 0s - loss: 515.4482 - val_loss: 469.2914\n",
      "Epoch 164/10000\n",
      "1126/1126 [==============================] - 0s - loss: 509.7335 - val_loss: 464.3766\n",
      "Epoch 165/10000\n",
      "1126/1126 [==============================] - 0s - loss: 504.1009 - val_loss: 459.5552\n",
      "Epoch 166/10000\n",
      "1126/1126 [==============================] - 0s - loss: 498.5500 - val_loss: 454.8266\n",
      "Epoch 167/10000\n",
      "1126/1126 [==============================] - 0s - loss: 493.0800 - val_loss: 450.1896\n",
      "Epoch 168/10000\n",
      "1126/1126 [==============================] - 0s - loss: 487.6906 - val_loss: 445.6429\n",
      "Epoch 169/10000\n",
      "1126/1126 [==============================] - 0s - loss: 482.3811 - val_loss: 441.1856\n",
      "Epoch 170/10000\n",
      "1126/1126 [==============================] - 0s - loss: 477.1510 - val_loss: 436.8165\n",
      "Epoch 171/10000\n",
      "1126/1126 [==============================] - 0s - loss: 471.9996 - val_loss: 432.5346\n",
      "Epoch 172/10000\n",
      "1126/1126 [==============================] - 0s - loss: 466.9262 - val_loss: 428.3387\n",
      "Epoch 173/10000\n",
      "1126/1126 [==============================] - 0s - loss: 461.9304 - val_loss: 424.2274\n",
      "Epoch 174/10000\n",
      "1126/1126 [==============================] - 0s - loss: 457.0114 - val_loss: 420.1997\n",
      "Epoch 175/10000\n",
      "1126/1126 [==============================] - 0s - loss: 452.1685 - val_loss: 416.2543\n",
      "Epoch 176/10000\n",
      "1126/1126 [==============================] - 0s - loss: 447.4012 - val_loss: 412.3900\n",
      "Epoch 177/10000\n",
      "1126/1126 [==============================] - 0s - loss: 442.7085 - val_loss: 408.6058\n",
      "Epoch 178/10000\n",
      "1126/1126 [==============================] - 0s - loss: 438.0900 - val_loss: 404.9002\n",
      "Epoch 179/10000\n",
      "1126/1126 [==============================] - 0s - loss: 433.5448 - val_loss: 401.2719\n",
      "Epoch 180/10000\n",
      "1126/1126 [==============================] - 0s - loss: 429.0722 - val_loss: 397.7197\n",
      "Epoch 181/10000\n",
      "1126/1126 [==============================] - 0s - loss: 424.6715 - val_loss: 394.2424\n",
      "Epoch 182/10000\n",
      "1126/1126 [==============================] - 0s - loss: 420.3419 - val_loss: 390.8386\n",
      "Epoch 183/10000\n",
      "1126/1126 [==============================] - 0s - loss: 416.0826 - val_loss: 387.5068\n",
      "Epoch 184/10000\n",
      "1126/1126 [==============================] - 0s - loss: 411.8929 - val_loss: 384.2460\n",
      "Epoch 185/10000\n",
      "1126/1126 [==============================] - 0s - loss: 407.7720 - val_loss: 381.0547\n",
      "Epoch 186/10000\n",
      "1126/1126 [==============================] - 0s - loss: 403.7190 - val_loss: 377.9315\n",
      "Epoch 187/10000\n",
      "1126/1126 [==============================] - 0s - loss: 399.7332 - val_loss: 374.8751\n",
      "Epoch 188/10000\n",
      "1126/1126 [==============================] - 0s - loss: 395.8138 - val_loss: 371.8840\n",
      "Epoch 189/10000\n",
      "1126/1126 [==============================] - 0s - loss: 391.9597 - val_loss: 368.9570\n",
      "Epoch 190/10000\n",
      "1126/1126 [==============================] - 0s - loss: 388.1704 - val_loss: 366.0926\n",
      "Epoch 191/10000\n",
      "1126/1126 [==============================] - 0s - loss: 384.4448 - val_loss: 363.2894\n",
      "Epoch 192/10000\n",
      "1126/1126 [==============================] - 0s - loss: 380.7822 - val_loss: 360.5460\n",
      "Epoch 193/10000\n",
      "1126/1126 [==============================] - 0s - loss: 377.1816 - val_loss: 357.8609\n",
      "Epoch 194/10000\n",
      "1126/1126 [==============================] - 0s - loss: 373.6422 - val_loss: 355.2327\n",
      "Epoch 195/10000\n",
      "1126/1126 [==============================] - 0s - loss: 370.1631 - val_loss: 352.6601\n",
      "Epoch 196/10000\n",
      "1126/1126 [==============================] - 0s - loss: 366.7434 - val_loss: 350.1414\n",
      "Epoch 197/10000\n",
      "1126/1126 [==============================] - 0s - loss: 363.3821 - val_loss: 347.6754\n",
      "Epoch 198/10000\n",
      "1126/1126 [==============================] - 0s - loss: 360.0784 - val_loss: 345.2604\n",
      "Epoch 199/10000\n",
      "1126/1126 [==============================] - 0s - loss: 356.8313 - val_loss: 342.8950\n",
      "Epoch 200/10000\n",
      "1126/1126 [==============================] - 0s - loss: 353.6398 - val_loss: 340.5778\n",
      "Epoch 201/10000\n",
      "1126/1126 [==============================] - 0s - loss: 350.5031 - val_loss: 338.3073\n",
      "Epoch 202/10000\n",
      "1126/1126 [==============================] - 0s - loss: 347.4200 - val_loss: 336.0819\n",
      "Epoch 203/10000\n",
      "1126/1126 [==============================] - 0s - loss: 344.3898 - val_loss: 333.9002\n",
      "Epoch 204/10000\n",
      "1126/1126 [==============================] - 0s - loss: 341.4111 - val_loss: 331.7606\n",
      "Epoch 205/10000\n",
      "1126/1126 [==============================] - 0s - loss: 338.4834 - val_loss: 329.6618\n",
      "Epoch 206/10000\n",
      "1126/1126 [==============================] - 0s - loss: 335.6056 - val_loss: 327.6025\n",
      "Epoch 207/10000\n",
      "1126/1126 [==============================] - 0s - loss: 332.7763 - val_loss: 325.5808\n",
      "Epoch 208/10000\n",
      "1126/1126 [==============================] - 0s - loss: 329.9950 - val_loss: 323.5954\n",
      "Epoch 209/10000\n",
      "1126/1126 [==============================] - 0s - loss: 327.2603 - val_loss: 321.6448\n",
      "Epoch 210/10000\n",
      "1126/1126 [==============================] - 0s - loss: 324.5714 - val_loss: 319.7278\n",
      "Epoch 211/10000\n",
      "1126/1126 [==============================] - 0s - loss: 321.9271 - val_loss: 317.8426\n",
      "Epoch 212/10000\n",
      "1126/1126 [==============================] - 0s - loss: 319.3263 - val_loss: 315.9880\n",
      "Epoch 213/10000\n",
      "1126/1126 [==============================] - 0s - loss: 316.7682 - val_loss: 314.1625\n",
      "Epoch 214/10000\n",
      "1126/1126 [==============================] - 0s - loss: 314.2516 - val_loss: 312.3646\n",
      "Epoch 215/10000\n",
      "1126/1126 [==============================] - 0s - loss: 311.7754 - val_loss: 310.5931\n",
      "Epoch 216/10000\n",
      "1126/1126 [==============================] - 0s - loss: 309.3388 - val_loss: 308.8468\n",
      "Epoch 217/10000\n",
      "1126/1126 [==============================] - 0s - loss: 306.9402 - val_loss: 307.1242\n",
      "Epoch 218/10000\n",
      "1126/1126 [==============================] - 0s - loss: 304.5790 - val_loss: 305.4241\n",
      "Epoch 219/10000\n",
      "1126/1126 [==============================] - 0s - loss: 302.2540 - val_loss: 303.7452\n",
      "Epoch 220/10000\n",
      "1126/1126 [==============================] - 0s - loss: 299.9642 - val_loss: 302.0863\n",
      "Epoch 221/10000\n",
      "1126/1126 [==============================] - 0s - loss: 297.7086 - val_loss: 300.4462\n",
      "Epoch 222/10000\n",
      "1126/1126 [==============================] - 0s - loss: 295.4860 - val_loss: 298.8239\n",
      "Epoch 223/10000\n",
      "1126/1126 [==============================] - 0s - loss: 293.2956 - val_loss: 297.2183\n",
      "Epoch 224/10000\n",
      "1126/1126 [==============================] - 0s - loss: 291.1361 - val_loss: 295.6283\n",
      "Epoch 225/10000\n",
      "1126/1126 [==============================] - 0s - loss: 289.0067 - val_loss: 294.0530\n",
      "Epoch 226/10000\n",
      "1126/1126 [==============================] - 0s - loss: 286.9063 - val_loss: 292.4914\n",
      "Epoch 227/10000\n",
      "1126/1126 [==============================] - 0s - loss: 284.8342 - val_loss: 290.9429\n",
      "Epoch 228/10000\n",
      "1126/1126 [==============================] - 0s - loss: 282.7891 - val_loss: 289.4064\n",
      "Epoch 229/10000\n",
      "1126/1126 [==============================] - 0s - loss: 280.7702 - val_loss: 287.8813\n",
      "Epoch 230/10000\n",
      "1126/1126 [==============================] - 0s - loss: 278.7768 - val_loss: 286.3671\n",
      "Epoch 231/10000\n",
      "1126/1126 [==============================] - 0s - loss: 276.8080 - val_loss: 284.8631\n",
      "Epoch 232/10000\n",
      "1126/1126 [==============================] - 0s - loss: 274.8627 - val_loss: 283.3688\n",
      "Epoch 233/10000\n",
      "1126/1126 [==============================] - 0s - loss: 272.9403 - val_loss: 281.8837\n",
      "Epoch 234/10000\n",
      "1126/1126 [==============================] - 0s - loss: 271.0401 - val_loss: 280.4075\n",
      "Epoch 235/10000\n",
      "1126/1126 [==============================] - 0s - loss: 269.1614 - val_loss: 278.9398\n",
      "Epoch 236/10000\n",
      "1126/1126 [==============================] - 0s - loss: 267.3036 - val_loss: 277.4804\n",
      "Epoch 237/10000\n",
      "1126/1126 [==============================] - 0s - loss: 265.4658 - val_loss: 276.0291\n",
      "Epoch 238/10000\n",
      "1126/1126 [==============================] - 0s - loss: 263.6478 - val_loss: 274.5860\n",
      "Epoch 239/10000\n",
      "1126/1126 [==============================] - 0s - loss: 261.8487 - val_loss: 273.1508\n",
      "Epoch 240/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 260.0683 - val_loss: 271.7236\n",
      "Epoch 241/10000\n",
      "1126/1126 [==============================] - 0s - loss: 258.3060 - val_loss: 270.3046\n",
      "Epoch 242/10000\n",
      "1126/1126 [==============================] - 0s - loss: 256.5614 - val_loss: 268.8939\n",
      "Epoch 243/10000\n",
      "1126/1126 [==============================] - 0s - loss: 254.8342 - val_loss: 267.4916\n",
      "Epoch 244/10000\n",
      "1126/1126 [==============================] - 0s - loss: 253.1240 - val_loss: 266.0981\n",
      "Epoch 245/10000\n",
      "1126/1126 [==============================] - 0s - loss: 251.4307 - val_loss: 264.7134\n",
      "Epoch 246/10000\n",
      "1126/1126 [==============================] - 0s - loss: 249.7537 - val_loss: 263.3381\n",
      "Epoch 247/10000\n",
      "1126/1126 [==============================] - 0s - loss: 248.0930 - val_loss: 261.9722\n",
      "Epoch 248/10000\n",
      "1126/1126 [==============================] - 0s - loss: 246.4483 - val_loss: 260.6163\n",
      "Epoch 249/10000\n",
      "1126/1126 [==============================] - 0s - loss: 244.8194 - val_loss: 259.2707\n",
      "Epoch 250/10000\n",
      "1126/1126 [==============================] - 0s - loss: 243.2062 - val_loss: 257.9357\n",
      "Epoch 251/10000\n",
      "1126/1126 [==============================] - 0s - loss: 241.6085 - val_loss: 256.6116\n",
      "Epoch 252/10000\n",
      "1126/1126 [==============================] - 0s - loss: 240.0261 - val_loss: 255.2991\n",
      "Epoch 253/10000\n",
      "1126/1126 [==============================] - 0s - loss: 238.4589 - val_loss: 253.9984\n",
      "Epoch 254/10000\n",
      "1126/1126 [==============================] - 0s - loss: 236.9067 - val_loss: 252.7098\n",
      "Epoch 255/10000\n",
      "1126/1126 [==============================] - 0s - loss: 235.3694 - val_loss: 251.4337\n",
      "Epoch 256/10000\n",
      "1126/1126 [==============================] - 0s - loss: 233.8469 - val_loss: 250.1707\n",
      "Epoch 257/10000\n",
      "1126/1126 [==============================] - 0s - loss: 232.3391 - val_loss: 248.9209\n",
      "Epoch 258/10000\n",
      "1126/1126 [==============================] - 0s - loss: 230.8458 - val_loss: 247.6848\n",
      "Epoch 259/10000\n",
      "1126/1126 [==============================] - 0s - loss: 229.3669 - val_loss: 246.4628\n",
      "Epoch 260/10000\n",
      "1126/1126 [==============================] - 0s - loss: 227.9024 - val_loss: 245.2551\n",
      "Epoch 261/10000\n",
      "1126/1126 [==============================] - 0s - loss: 226.4521 - val_loss: 244.0623\n",
      "Epoch 262/10000\n",
      "1126/1126 [==============================] - 0s - loss: 225.0160 - val_loss: 242.8845\n",
      "Epoch 263/10000\n",
      "1126/1126 [==============================] - 0s - loss: 223.5940 - val_loss: 241.7224\n",
      "Epoch 264/10000\n",
      "1126/1126 [==============================] - 0s - loss: 222.1861 - val_loss: 240.5760\n",
      "Epoch 265/10000\n",
      "1126/1126 [==============================] - 0s - loss: 220.7923 - val_loss: 239.4460\n",
      "Epoch 266/10000\n",
      "1126/1126 [==============================] - 0s - loss: 219.4127 - val_loss: 238.3327\n",
      "Epoch 267/10000\n",
      "1126/1126 [==============================] - 0s - loss: 218.0473 - val_loss: 237.2363\n",
      "Epoch 268/10000\n",
      "1126/1126 [==============================] - 0s - loss: 216.6963 - val_loss: 236.1574\n",
      "Epoch 269/10000\n",
      "1126/1126 [==============================] - 0s - loss: 215.3597 - val_loss: 235.0961\n",
      "Epoch 270/10000\n",
      "1126/1126 [==============================] - 0s - loss: 214.0377 - val_loss: 234.0529\n",
      "Epoch 271/10000\n",
      "1126/1126 [==============================] - 0s - loss: 212.7306 - val_loss: 233.0280\n",
      "Epoch 272/10000\n",
      "1126/1126 [==============================] - 0s - loss: 211.4384 - val_loss: 232.0215\n",
      "Epoch 273/10000\n",
      "1126/1126 [==============================] - 0s - loss: 210.1616 - val_loss: 231.0337\n",
      "Epoch 274/10000\n",
      "1126/1126 [==============================] - 0s - loss: 208.9003 - val_loss: 230.0646\n",
      "Epoch 275/10000\n",
      "1126/1126 [==============================] - 0s - loss: 207.6547 - val_loss: 229.1142\n",
      "Epoch 276/10000\n",
      "1126/1126 [==============================] - 0s - loss: 206.4251 - val_loss: 228.1825\n",
      "Epoch 277/10000\n",
      "1126/1126 [==============================] - 0s - loss: 205.2118 - val_loss: 227.2695\n",
      "Epoch 278/10000\n",
      "1126/1126 [==============================] - 0s - loss: 204.0149 - val_loss: 226.3750\n",
      "Epoch 279/10000\n",
      "1126/1126 [==============================] - 0s - loss: 202.8346 - val_loss: 225.4987\n",
      "Epoch 280/10000\n",
      "1126/1126 [==============================] - 0s - loss: 201.6712 - val_loss: 224.6402\n",
      "Epoch 281/10000\n",
      "1126/1126 [==============================] - 0s - loss: 200.5248 - val_loss: 223.7995\n",
      "Epoch 282/10000\n",
      "1126/1126 [==============================] - 0s - loss: 199.3956 - val_loss: 222.9762\n",
      "Epoch 283/10000\n",
      "1126/1126 [==============================] - 0s - loss: 198.2835 - val_loss: 222.1697\n",
      "Epoch 284/10000\n",
      "1126/1126 [==============================] - 0s - loss: 197.1888 - val_loss: 221.3798\n",
      "Epoch 285/10000\n",
      "1126/1126 [==============================] - 0s - loss: 196.1113 - val_loss: 220.6061\n",
      "Epoch 286/10000\n",
      "1126/1126 [==============================] - 0s - loss: 195.0512 - val_loss: 219.8480\n",
      "Epoch 287/10000\n",
      "1126/1126 [==============================] - 0s - loss: 194.0083 - val_loss: 219.1052\n",
      "Epoch 288/10000\n",
      "1126/1126 [==============================] - 0s - loss: 192.9827 - val_loss: 218.3772\n",
      "Epoch 289/10000\n",
      "1126/1126 [==============================] - 0s - loss: 191.9741 - val_loss: 217.6637\n",
      "Epoch 290/10000\n",
      "1126/1126 [==============================] - 0s - loss: 190.9825 - val_loss: 216.9641\n",
      "Epoch 291/10000\n",
      "1126/1126 [==============================] - 0s - loss: 190.0077 - val_loss: 216.2780\n",
      "Epoch 292/10000\n",
      "1126/1126 [==============================] - 0s - loss: 189.0495 - val_loss: 215.6051\n",
      "Epoch 293/10000\n",
      "1126/1126 [==============================] - 0s - loss: 188.1078 - val_loss: 214.9449\n",
      "Epoch 294/10000\n",
      "1126/1126 [==============================] - 0s - loss: 187.1822 - val_loss: 214.2971\n",
      "Epoch 295/10000\n",
      "1126/1126 [==============================] - 0s - loss: 186.2725 - val_loss: 213.6611\n",
      "Epoch 296/10000\n",
      "1126/1126 [==============================] - 0s - loss: 185.3786 - val_loss: 213.0368\n",
      "Epoch 297/10000\n",
      "1126/1126 [==============================] - 0s - loss: 184.4999 - val_loss: 212.4236\n",
      "Epoch 298/10000\n",
      "1126/1126 [==============================] - 0s - loss: 183.6364 - val_loss: 211.8212\n",
      "Epoch 299/10000\n",
      "1126/1126 [==============================] - 0s - loss: 182.7876 - val_loss: 211.2293\n",
      "Epoch 300/10000\n",
      "1126/1126 [==============================] - 0s - loss: 181.9532 - val_loss: 210.6474\n",
      "Epoch 301/10000\n",
      "1126/1126 [==============================] - 0s - loss: 181.1330 - val_loss: 210.0753\n",
      "Epoch 302/10000\n",
      "1126/1126 [==============================] - 0s - loss: 180.3265 - val_loss: 209.5126\n",
      "Epoch 303/10000\n",
      "1126/1126 [==============================] - 0s - loss: 179.5334 - val_loss: 208.9589\n",
      "Epoch 304/10000\n",
      "1126/1126 [==============================] - 0s - loss: 178.7533 - val_loss: 208.4139\n",
      "Epoch 305/10000\n",
      "1126/1126 [==============================] - 0s - loss: 177.9859 - val_loss: 207.8773\n",
      "Epoch 306/10000\n",
      "1126/1126 [==============================] - 0s - loss: 177.2308 - val_loss: 207.3486\n",
      "Epoch 307/10000\n",
      "1126/1126 [==============================] - 0s - loss: 176.4877 - val_loss: 206.8277\n",
      "Epoch 308/10000\n",
      "1126/1126 [==============================] - 0s - loss: 175.7561 - val_loss: 206.3141\n",
      "Epoch 309/10000\n",
      "1126/1126 [==============================] - 0s - loss: 175.0358 - val_loss: 205.8074\n",
      "Epoch 310/10000\n",
      "1126/1126 [==============================] - 0s - loss: 174.3262 - val_loss: 205.3073\n",
      "Epoch 311/10000\n",
      "  32/1126 [..............................] - ETA: 0s - loss: 32.3098"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.plot(history.history['loss'], 'r-')\n",
    "plt.plot(history.history['val_loss'], 'b-')\n",
    "plt.show()\n",
    "\n",
    "plt.title('y_pred, y_test')\n",
    "\n",
    "plt.plot(y_pred[:], y_test[:], '+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataframe_to_xy_sequences(df_train, 4)\n",
    "X_valid, y_valid = dataframe_to_xy_sequences(df_valid, 4)\n",
    "X_test, y_test = dataframe_to_xy_sequences(df_test, 4)\n",
    "\n",
    "def simple_rnn_model(nb_units, input_dim, loss='mean_squared_error', optimizer='adagrad'):\n",
    "    print(input_dim)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(nb_units, batch_input_shape=(96, 4, 8)))#input_dim=input_dim[1], input_length=input_dim[0], return_sequences=True))\n",
    "    #model.add(Dense(nb_units))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = simple_rnn_model(16, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
