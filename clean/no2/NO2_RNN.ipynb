{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/micro_sud3_normalized.pkl')\n",
    "df = df.reset_index()\n",
    "df.head()\n",
    "\n",
    "def split_dataframe(dataframe, percent):\n",
    "    nb_rows = int(np.floor(percent * len(dataframe)))\n",
    "    return dataframe[:nb_rows], dataframe[nb_rows:]\n",
    "    \"\"\"\n",
    "def dataframe_to_xy(df, sequence_size):\n",
    "\n",
    "    out_X = np.zeros((len(df)//sequence_size, sequence_size, 8))\n",
    "    out_y = np.zeros((len(df)//sequence_size, sequence_size))\n",
    "    i = 0\n",
    "    while i + sequence_size < len(df):\n",
    "        sequence = df.iloc[i:i+sequence_size]\n",
    "        out_X[i//sequence_size] =  np.array(sequence[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                                 'tgrad', 'pressure', 'pluvio']])\n",
    "        out_y[i//sequence_size] = np.array(sequence['NO2_ref'])\n",
    "        i += sequence_size\n",
    "        \n",
    "    return out_X, out_y\n",
    "    \"\"\"\n",
    "\n",
    "def dataframe_to_xy(df):\n",
    "    return (np.array(df[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                         'tgrad', 'pressure', 'pluvio']]),\\\n",
    "            np.array(df['NO2_ref']))\n",
    "\n",
    "        \n",
    "df_train, df_test = split_dataframe(df, 0.5) \n",
    "df_valid, df_test = split_dataframe(df_test, 0.5)\n",
    "\n",
    "X_train, y_train = dataframe_to_xy(df_train)\n",
    "X_valid, y_valid = dataframe_to_xy(df_valid)\n",
    "X_test, y_test = dataframe_to_xy(df_test)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_valid= X_valid.reshape((X_valid.shape[0], 1,  X_valid.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "#y_train = y_train.reshape((1, len(y_train)))\n",
    "#y_valid = y_valid.reshape((1, len(y_valid)))\n",
    "#y_test = y_test.reshape((1, len(y_test)))\n",
    "\n",
    "def dataframe_to_xy_sequences(df, sequence_size):\n",
    "    out_X = np.zeros((len(df)//sequence_size, sequence_size, 8))\n",
    "    out_y = np.zeros((len(df)//sequence_size, sequence_size))\n",
    "    i = 0\n",
    "    while i + sequence_size < len(df):\n",
    "        sequence = df.iloc[i:i+sequence_size]\n",
    "        out_X[i//sequence_size] =  np.array(sequence[['NO2_61FD', 'NO2_61F0', 'NO2_61EF', 'temp', 'rh',\\\n",
    "                                 'tgrad', 'pressure', 'pluvio']])\n",
    "        out_y[i//sequence_size] = np.array(sequence['NO2_ref'])\n",
    "        i += sequence_size\n",
    "        \n",
    "    return out_X, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def simple_rnn_model(nb_units, input_dim, loss='mean_squared_error', optimizer='adam'):\n",
    "    print(input_dim)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(nb_units, input_shape=input_dim))#input_dim=input_dim[1], input_length=input_dim[0], return_sequences=True))\n",
    "    #model.add(Dense(nb_units))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 417\n",
      "Trainable params: 417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_rnn_model(16, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1126 samples, validate on 563 samples\n",
      "Epoch 1/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3352.5525 - val_loss: 3015.8167\n",
      "Epoch 2/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3342.8907 - val_loss: 3007.8665\n",
      "Epoch 3/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3331.3117 - val_loss: 2998.1805\n",
      "Epoch 4/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3316.6519 - val_loss: 2986.1466\n",
      "Epoch 5/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3298.6062 - val_loss: 2971.3558\n",
      "Epoch 6/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3277.0715 - val_loss: 2953.5259\n",
      "Epoch 7/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3251.8953 - val_loss: 2932.4591\n",
      "Epoch 8/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3222.9662 - val_loss: 2907.9869\n",
      "Epoch 9/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3190.3740 - val_loss: 2880.1756\n",
      "Epoch 10/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3154.5252 - val_loss: 2849.3402\n",
      "Epoch 11/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3115.9611 - val_loss: 2815.9012\n",
      "Epoch 12/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3075.2092 - val_loss: 2780.2580\n",
      "Epoch 13/10000\n",
      "1126/1126 [==============================] - 0s - loss: 3032.7709 - val_loss: 2742.8475\n",
      "Epoch 14/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2989.1090 - val_loss: 2704.0676\n",
      "Epoch 15/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2944.6258 - val_loss: 2664.2989\n",
      "Epoch 16/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2899.6652 - val_loss: 2623.8668\n",
      "Epoch 17/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2854.5096 - val_loss: 2583.0409\n",
      "Epoch 18/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2809.4039 - val_loss: 2542.0776\n",
      "Epoch 19/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2764.5602 - val_loss: 2501.1598\n",
      "Epoch 20/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2720.1379 - val_loss: 2460.4527\n",
      "Epoch 21/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2676.2737 - val_loss: 2420.0873\n",
      "Epoch 22/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2633.0691 - val_loss: 2380.1669\n",
      "Epoch 23/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2590.6080 - val_loss: 2340.7849\n",
      "Epoch 24/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2548.9529 - val_loss: 2301.9984\n",
      "Epoch 25/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2508.1394 - val_loss: 2263.8592\n",
      "Epoch 26/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2468.1954 - val_loss: 2226.4079\n",
      "Epoch 27/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2429.1406 - val_loss: 2189.6751\n",
      "Epoch 28/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2390.9821 - val_loss: 2153.6795\n",
      "Epoch 29/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2353.7224 - val_loss: 2118.4383\n",
      "Epoch 30/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2317.3571 - val_loss: 2083.9597\n",
      "Epoch 31/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2281.8803 - val_loss: 2050.2519\n",
      "Epoch 32/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2247.2819 - val_loss: 2017.3110\n",
      "Epoch 33/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2213.5472 - val_loss: 1985.1357\n",
      "Epoch 34/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2180.6637 - val_loss: 1953.7241\n",
      "Epoch 35/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2148.6182 - val_loss: 1923.0657\n",
      "Epoch 36/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2117.3927 - val_loss: 1893.1504\n",
      "Epoch 37/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2086.9699 - val_loss: 1863.9658\n",
      "Epoch 38/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2057.3334 - val_loss: 1835.4998\n",
      "Epoch 39/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2028.4651 - val_loss: 1807.7424\n",
      "Epoch 40/10000\n",
      "1126/1126 [==============================] - 0s - loss: 2000.3478 - val_loss: 1780.6781\n",
      "Epoch 41/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1972.9624 - val_loss: 1754.2944\n",
      "Epoch 42/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1946.2908 - val_loss: 1728.5775\n",
      "Epoch 43/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1920.3130 - val_loss: 1703.5115\n",
      "Epoch 44/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1895.0093 - val_loss: 1679.0830\n",
      "Epoch 45/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1870.3605 - val_loss: 1655.2764\n",
      "Epoch 46/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1846.3464 - val_loss: 1632.0786\n",
      "Epoch 47/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1822.9467 - val_loss: 1609.4737\n",
      "Epoch 48/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1800.1408 - val_loss: 1587.4488\n",
      "Epoch 49/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1777.9091 - val_loss: 1565.9894\n",
      "Epoch 50/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1756.2314 - val_loss: 1545.0798\n",
      "Epoch 51/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1735.0874 - val_loss: 1524.7055\n",
      "Epoch 52/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1714.4575 - val_loss: 1504.8520\n",
      "Epoch 53/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1694.3217 - val_loss: 1485.5038\n",
      "Epoch 54/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1674.6603 - val_loss: 1466.6458\n",
      "Epoch 55/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1655.4534 - val_loss: 1448.2633\n",
      "Epoch 56/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1636.6813 - val_loss: 1430.3412\n",
      "Epoch 57/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1618.3237 - val_loss: 1412.8638\n",
      "Epoch 58/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1600.3608 - val_loss: 1395.8166\n",
      "Epoch 59/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1582.7720 - val_loss: 1379.1841\n",
      "Epoch 60/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1565.5369 - val_loss: 1362.9520\n",
      "Epoch 61/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1548.6347 - val_loss: 1347.1050\n",
      "Epoch 62/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1532.0448 - val_loss: 1331.628356.\n",
      "Epoch 63/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1515.7467 - val_loss: 1316.5081\n",
      "Epoch 64/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1499.7202 - val_loss: 1301.7299\n",
      "Epoch 65/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1483.9456 - val_loss: 1287.2800\n",
      "Epoch 66/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1468.4039 - val_loss: 1273.1456\n",
      "Epoch 67/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1453.0776 - val_loss: 1259.3145\n",
      "Epoch 68/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1437.9508 - val_loss: 1245.7752\n",
      "Epoch 69/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1423.0096 - val_loss: 1232.5174\n",
      "Epoch 70/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1408.2421 - val_loss: 1219.5316\n",
      "Epoch 71/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1393.6394 - val_loss: 1206.8088\n",
      "Epoch 72/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1379.1950 - val_loss: 1194.3410\n",
      "Epoch 73/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1364.9054 - val_loss: 1182.1213\n",
      "Epoch 74/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1350.7691 - val_loss: 1170.1427\n",
      "Epoch 75/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1336.7869 - val_loss: 1158.3989\n",
      "Epoch 76/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1322.9604 - val_loss: 1146.8836\n",
      "Epoch 77/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1309.2922 - val_loss: 1135.5903\n",
      "Epoch 78/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1295.7852 - val_loss: 1124.5125\n",
      "Epoch 79/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1282.4418 - val_loss: 1113.6443\n",
      "Epoch 80/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 1269.2637 - val_loss: 1102.9789\n",
      "Epoch 81/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1256.2512 - val_loss: 1092.5091\n",
      "Epoch 82/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1243.4040 - val_loss: 1082.2287\n",
      "Epoch 83/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1230.7200 - val_loss: 1072.1304\n",
      "Epoch 84/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1218.1967 - val_loss: 1062.2074\n",
      "Epoch 85/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1205.8309 - val_loss: 1052.4530\n",
      "Epoch 86/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1193.6185 - val_loss: 1042.8603\n",
      "Epoch 87/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1181.5554 - val_loss: 1033.4218\n",
      "Epoch 88/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1169.6370 - val_loss: 1024.1309\n",
      "Epoch 89/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1157.8586 - val_loss: 1014.9809\n",
      "Epoch 90/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1146.2153 - val_loss: 1005.9640\n",
      "Epoch 91/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1134.7025 - val_loss: 997.0733\n",
      "Epoch 92/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1123.3156 - val_loss: 988.3018\n",
      "Epoch 93/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1112.0499 - val_loss: 979.6425\n",
      "Epoch 94/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1100.9010 - val_loss: 971.0881\n",
      "Epoch 95/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1089.8647 - val_loss: 962.6316\n",
      "Epoch 96/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1078.9366 - val_loss: 954.2660\n",
      "Epoch 97/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1068.1125 - val_loss: 945.9843\n",
      "Epoch 98/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1057.3886 - val_loss: 937.7799\n",
      "Epoch 99/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1046.7611 - val_loss: 929.6453\n",
      "Epoch 100/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1036.2259 - val_loss: 921.5742\n",
      "Epoch 101/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1025.7797 - val_loss: 913.5600\n",
      "Epoch 102/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1015.4188 - val_loss: 905.5959\n",
      "Epoch 103/10000\n",
      "1126/1126 [==============================] - 0s - loss: 1005.1400 - val_loss: 897.6757\n",
      "Epoch 104/10000\n",
      "1126/1126 [==============================] - 0s - loss: 994.9403 - val_loss: 889.7937\n",
      "Epoch 105/10000\n",
      "1126/1126 [==============================] - 0s - loss: 984.8164 - val_loss: 881.9428\n",
      "Epoch 106/10000\n",
      "1126/1126 [==============================] - 0s - loss: 974.7653 - val_loss: 874.1176\n",
      "Epoch 107/10000\n",
      "1126/1126 [==============================] - 0s - loss: 964.7847 - val_loss: 866.3128\n",
      "Epoch 108/10000\n",
      "1126/1126 [==============================] - 0s - loss: 954.8719 - val_loss: 858.5221\n",
      "Epoch 109/10000\n",
      "1126/1126 [==============================] - 0s - loss: 945.0244 - val_loss: 850.7408\n",
      "Epoch 110/10000\n",
      "1126/1126 [==============================] - 0s - loss: 935.2406 - val_loss: 842.9642\n",
      "Epoch 111/10000\n",
      "1126/1126 [==============================] - 0s - loss: 925.5179 - val_loss: 835.1872\n",
      "Epoch 112/10000\n",
      "1126/1126 [==============================] - 0s - loss: 915.8551 - val_loss: 827.4062\n",
      "Epoch 113/10000\n",
      "1126/1126 [==============================] - 0s - loss: 906.2505 - val_loss: 819.6170\n",
      "Epoch 114/10000\n",
      "1126/1126 [==============================] - 0s - loss: 896.7033 - val_loss: 811.8174\n",
      "Epoch 115/10000\n",
      "1126/1126 [==============================] - 0s - loss: 887.2123 - val_loss: 804.0038\n",
      "Epoch 116/10000\n",
      "1126/1126 [==============================] - 0s - loss: 877.7769 - val_loss: 796.1744\n",
      "Epoch 117/10000\n",
      "1126/1126 [==============================] - 0s - loss: 868.3969 - val_loss: 788.3282\n",
      "Epoch 118/10000\n",
      "1126/1126 [==============================] - 0s - loss: 859.0722 - val_loss: 780.4642\n",
      "Epoch 119/10000\n",
      "1126/1126 [==============================] - 0s - loss: 849.8028 - val_loss: 772.5824\n",
      "Epoch 120/10000\n",
      "1126/1126 [==============================] - 0s - loss: 840.5896 - val_loss: 764.6837\n",
      "Epoch 121/10000\n",
      "1126/1126 [==============================] - 0s - loss: 831.4333 - val_loss: 756.7696\n",
      "Epoch 122/10000\n",
      "1126/1126 [==============================] - 0s - loss: 822.3348 - val_loss: 748.8425\n",
      "Epoch 123/10000\n",
      "1126/1126 [==============================] - 0s - loss: 813.2955 - val_loss: 740.9052\n",
      "Epoch 124/10000\n",
      "1126/1126 [==============================] - 0s - loss: 804.3170 - val_loss: 732.9618\n",
      "Epoch 125/10000\n",
      "1126/1126 [==============================] - 0s - loss: 795.4011 - val_loss: 725.0164\n",
      "Epoch 126/10000\n",
      "1126/1126 [==============================] - 0s - loss: 786.5494 - val_loss: 717.0744\n",
      "Epoch 127/10000\n",
      "1126/1126 [==============================] - 0s - loss: 777.7639 - val_loss: 709.1408\n",
      "Epoch 128/10000\n",
      "1126/1126 [==============================] - 0s - loss: 769.0465 - val_loss: 701.2216\n",
      "Epoch 129/10000\n",
      "1126/1126 [==============================] - 0s - loss: 760.3994 - val_loss: 693.3236\n",
      "Epoch 130/10000\n",
      "1126/1126 [==============================] - 0s - loss: 751.8244 - val_loss: 685.4528\n",
      "Epoch 131/10000\n",
      "1126/1126 [==============================] - 0s - loss: 743.3232 - val_loss: 677.6159\n",
      "Epoch 132/10000\n",
      "1126/1126 [==============================] - 0s - loss: 734.8978 - val_loss: 669.8195\n",
      "Epoch 133/10000\n",
      "1126/1126 [==============================] - 0s - loss: 726.5497 - val_loss: 662.0697\n",
      "Epoch 134/10000\n",
      "1126/1126 [==============================] - 0s - loss: 718.2804 - val_loss: 654.3728\n",
      "Epoch 135/10000\n",
      "1126/1126 [==============================] - 0s - loss: 710.0914 - val_loss: 646.7354\n",
      "Epoch 136/10000\n",
      "1126/1126 [==============================] - 0s - loss: 701.9836 - val_loss: 639.1622\n",
      "Epoch 137/10000\n",
      "1126/1126 [==============================] - 0s - loss: 693.9584 - val_loss: 631.6585\n",
      "Epoch 138/10000\n",
      "1126/1126 [==============================] - 0s - loss: 686.0169 - val_loss: 624.2300\n",
      "Epoch 139/10000\n",
      "1126/1126 [==============================] - 0s - loss: 678.1595 - val_loss: 616.8808\n",
      "Epoch 140/10000\n",
      "1126/1126 [==============================] - 0s - loss: 670.3871 - val_loss: 609.6149\n",
      "Epoch 141/10000\n",
      "1126/1126 [==============================] - 0s - loss: 662.7001 - val_loss: 602.4354\n",
      "Epoch 142/10000\n",
      "1126/1126 [==============================] - 0s - loss: 655.0989 - val_loss: 595.3451\n",
      "Epoch 143/10000\n",
      "1126/1126 [==============================] - 0s - loss: 647.5841 - val_loss: 588.3475\n",
      "Epoch 144/10000\n",
      "1126/1126 [==============================] - 0s - loss: 640.1558 - val_loss: 581.4444\n",
      "Epoch 145/10000\n",
      "1126/1126 [==============================] - 0s - loss: 632.8143 - val_loss: 574.6377\n",
      "Epoch 146/10000\n",
      "1126/1126 [==============================] - 0s - loss: 625.5596 - val_loss: 567.9289\n",
      "Epoch 147/10000\n",
      "1126/1126 [==============================] - 0s - loss: 618.3921 - val_loss: 561.3198\n",
      "Epoch 148/10000\n",
      "1126/1126 [==============================] - 0s - loss: 611.3116 - val_loss: 554.8109\n",
      "Epoch 149/10000\n",
      "1126/1126 [==============================] - 0s - loss: 604.3180 - val_loss: 548.4031\n",
      "Epoch 150/10000\n",
      "1126/1126 [==============================] - 0s - loss: 597.4115 - val_loss: 542.0967\n",
      "Epoch 151/10000\n",
      "1126/1126 [==============================] - 0s - loss: 590.5919 - val_loss: 535.8924\n",
      "Epoch 152/10000\n",
      "1126/1126 [==============================] - 0s - loss: 583.8589 - val_loss: 529.7897\n",
      "Epoch 153/10000\n",
      "1126/1126 [==============================] - 0s - loss: 577.2127 - val_loss: 523.7892\n",
      "Epoch 154/10000\n",
      "1126/1126 [==============================] - 0s - loss: 570.6526 - val_loss: 517.8904\n",
      "Epoch 155/10000\n",
      "1126/1126 [==============================] - 0s - loss: 564.1787 - val_loss: 512.0931\n",
      "Epoch 156/10000\n",
      "1126/1126 [==============================] - 0s - loss: 557.7907 - val_loss: 506.3967\n",
      "Epoch 157/10000\n",
      "1126/1126 [==============================] - 0s - loss: 551.4881 - val_loss: 500.8007\n",
      "Epoch 158/10000\n",
      "1126/1126 [==============================] - 0s - loss: 545.2708 - val_loss: 495.3043\n",
      "Epoch 159/10000\n",
      "1126/1126 [==============================] - 0s - loss: 539.1382 - val_loss: 489.9068\n",
      "Epoch 160/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 533.0903 - val_loss: 484.6076\n",
      "Epoch 161/10000\n",
      "1126/1126 [==============================] - 0s - loss: 527.1262 - val_loss: 479.4060\n",
      "Epoch 162/10000\n",
      "1126/1126 [==============================] - 0s - loss: 521.2456 - val_loss: 474.3009\n",
      "Epoch 163/10000\n",
      "1126/1126 [==============================] - 0s - loss: 515.4482 - val_loss: 469.2914\n",
      "Epoch 164/10000\n",
      "1126/1126 [==============================] - 0s - loss: 509.7335 - val_loss: 464.3766\n",
      "Epoch 165/10000\n",
      "1126/1126 [==============================] - 0s - loss: 504.1009 - val_loss: 459.5552\n",
      "Epoch 166/10000\n",
      "1126/1126 [==============================] - 0s - loss: 498.5500 - val_loss: 454.8266\n",
      "Epoch 167/10000\n",
      "1126/1126 [==============================] - 0s - loss: 493.0800 - val_loss: 450.1896\n",
      "Epoch 168/10000\n",
      "1126/1126 [==============================] - 0s - loss: 487.6906 - val_loss: 445.6429\n",
      "Epoch 169/10000\n",
      "1126/1126 [==============================] - 0s - loss: 482.3811 - val_loss: 441.1856\n",
      "Epoch 170/10000\n",
      "1126/1126 [==============================] - 0s - loss: 477.1510 - val_loss: 436.8165\n",
      "Epoch 171/10000\n",
      "1126/1126 [==============================] - 0s - loss: 471.9996 - val_loss: 432.5346\n",
      "Epoch 172/10000\n",
      "1126/1126 [==============================] - 0s - loss: 466.9262 - val_loss: 428.3387\n",
      "Epoch 173/10000\n",
      "1126/1126 [==============================] - 0s - loss: 461.9304 - val_loss: 424.2274\n",
      "Epoch 174/10000\n",
      "1126/1126 [==============================] - 0s - loss: 457.0114 - val_loss: 420.1997\n",
      "Epoch 175/10000\n",
      "1126/1126 [==============================] - 0s - loss: 452.1685 - val_loss: 416.2543\n",
      "Epoch 176/10000\n",
      "1126/1126 [==============================] - 0s - loss: 447.4012 - val_loss: 412.3900\n",
      "Epoch 177/10000\n",
      "1126/1126 [==============================] - 0s - loss: 442.7085 - val_loss: 408.6058\n",
      "Epoch 178/10000\n",
      "1126/1126 [==============================] - 0s - loss: 438.0900 - val_loss: 404.9002\n",
      "Epoch 179/10000\n",
      "1126/1126 [==============================] - 0s - loss: 433.5448 - val_loss: 401.2719\n",
      "Epoch 180/10000\n",
      "1126/1126 [==============================] - 0s - loss: 429.0722 - val_loss: 397.7197\n",
      "Epoch 181/10000\n",
      "1126/1126 [==============================] - 0s - loss: 424.6715 - val_loss: 394.2424\n",
      "Epoch 182/10000\n",
      "1126/1126 [==============================] - 0s - loss: 420.3419 - val_loss: 390.8386\n",
      "Epoch 183/10000\n",
      "1126/1126 [==============================] - 0s - loss: 416.0826 - val_loss: 387.5068\n",
      "Epoch 184/10000\n",
      "1126/1126 [==============================] - 0s - loss: 411.8929 - val_loss: 384.2460\n",
      "Epoch 185/10000\n",
      "1126/1126 [==============================] - 0s - loss: 407.7720 - val_loss: 381.0547\n",
      "Epoch 186/10000\n",
      "1126/1126 [==============================] - 0s - loss: 403.7190 - val_loss: 377.9315\n",
      "Epoch 187/10000\n",
      "1126/1126 [==============================] - 0s - loss: 399.7332 - val_loss: 374.8751\n",
      "Epoch 188/10000\n",
      "1126/1126 [==============================] - 0s - loss: 395.8138 - val_loss: 371.8840\n",
      "Epoch 189/10000\n",
      "1126/1126 [==============================] - 0s - loss: 391.9597 - val_loss: 368.9570\n",
      "Epoch 190/10000\n",
      "1126/1126 [==============================] - 0s - loss: 388.1704 - val_loss: 366.0926\n",
      "Epoch 191/10000\n",
      "1126/1126 [==============================] - 0s - loss: 384.4448 - val_loss: 363.2894\n",
      "Epoch 192/10000\n",
      "1126/1126 [==============================] - 0s - loss: 380.7822 - val_loss: 360.5460\n",
      "Epoch 193/10000\n",
      "1126/1126 [==============================] - 0s - loss: 377.1816 - val_loss: 357.8609\n",
      "Epoch 194/10000\n",
      "1126/1126 [==============================] - 0s - loss: 373.6422 - val_loss: 355.2327\n",
      "Epoch 195/10000\n",
      "1126/1126 [==============================] - 0s - loss: 370.1631 - val_loss: 352.6601\n",
      "Epoch 196/10000\n",
      "1126/1126 [==============================] - 0s - loss: 366.7434 - val_loss: 350.1414\n",
      "Epoch 197/10000\n",
      "1126/1126 [==============================] - 0s - loss: 363.3821 - val_loss: 347.6754\n",
      "Epoch 198/10000\n",
      "1126/1126 [==============================] - 0s - loss: 360.0784 - val_loss: 345.2604\n",
      "Epoch 199/10000\n",
      "1126/1126 [==============================] - 0s - loss: 356.8313 - val_loss: 342.8950\n",
      "Epoch 200/10000\n",
      "1126/1126 [==============================] - 0s - loss: 353.6398 - val_loss: 340.5778\n",
      "Epoch 201/10000\n",
      "1126/1126 [==============================] - 0s - loss: 350.5031 - val_loss: 338.3073\n",
      "Epoch 202/10000\n",
      "1126/1126 [==============================] - 0s - loss: 347.4200 - val_loss: 336.0819\n",
      "Epoch 203/10000\n",
      "1126/1126 [==============================] - 0s - loss: 344.3898 - val_loss: 333.9002\n",
      "Epoch 204/10000\n",
      "1126/1126 [==============================] - 0s - loss: 341.4111 - val_loss: 331.7606\n",
      "Epoch 205/10000\n",
      "1126/1126 [==============================] - 0s - loss: 338.4834 - val_loss: 329.6618\n",
      "Epoch 206/10000\n",
      "1126/1126 [==============================] - 0s - loss: 335.6056 - val_loss: 327.6025\n",
      "Epoch 207/10000\n",
      "1126/1126 [==============================] - 0s - loss: 332.7763 - val_loss: 325.5808\n",
      "Epoch 208/10000\n",
      "1126/1126 [==============================] - 0s - loss: 329.9950 - val_loss: 323.5954\n",
      "Epoch 209/10000\n",
      "1126/1126 [==============================] - 0s - loss: 327.2603 - val_loss: 321.6448\n",
      "Epoch 210/10000\n",
      "1126/1126 [==============================] - 0s - loss: 324.5714 - val_loss: 319.7278\n",
      "Epoch 211/10000\n",
      "1126/1126 [==============================] - 0s - loss: 321.9271 - val_loss: 317.8426\n",
      "Epoch 212/10000\n",
      "1126/1126 [==============================] - 0s - loss: 319.3263 - val_loss: 315.9880\n",
      "Epoch 213/10000\n",
      "1126/1126 [==============================] - 0s - loss: 316.7682 - val_loss: 314.1625\n",
      "Epoch 214/10000\n",
      "1126/1126 [==============================] - 0s - loss: 314.2516 - val_loss: 312.3646\n",
      "Epoch 215/10000\n",
      "1126/1126 [==============================] - 0s - loss: 311.7754 - val_loss: 310.5931\n",
      "Epoch 216/10000\n",
      "1126/1126 [==============================] - 0s - loss: 309.3388 - val_loss: 308.8468\n",
      "Epoch 217/10000\n",
      "1126/1126 [==============================] - 0s - loss: 306.9402 - val_loss: 307.1242\n",
      "Epoch 218/10000\n",
      "1126/1126 [==============================] - 0s - loss: 304.5790 - val_loss: 305.4241\n",
      "Epoch 219/10000\n",
      "1126/1126 [==============================] - 0s - loss: 302.2540 - val_loss: 303.7452\n",
      "Epoch 220/10000\n",
      "1126/1126 [==============================] - 0s - loss: 299.9642 - val_loss: 302.0863\n",
      "Epoch 221/10000\n",
      "1126/1126 [==============================] - 0s - loss: 297.7086 - val_loss: 300.4462\n",
      "Epoch 222/10000\n",
      "1126/1126 [==============================] - 0s - loss: 295.4860 - val_loss: 298.8239\n",
      "Epoch 223/10000\n",
      "1126/1126 [==============================] - 0s - loss: 293.2956 - val_loss: 297.2183\n",
      "Epoch 224/10000\n",
      "1126/1126 [==============================] - 0s - loss: 291.1361 - val_loss: 295.6283\n",
      "Epoch 225/10000\n",
      "1126/1126 [==============================] - 0s - loss: 289.0067 - val_loss: 294.0530\n",
      "Epoch 226/10000\n",
      "1126/1126 [==============================] - 0s - loss: 286.9063 - val_loss: 292.4914\n",
      "Epoch 227/10000\n",
      "1126/1126 [==============================] - 0s - loss: 284.8342 - val_loss: 290.9429\n",
      "Epoch 228/10000\n",
      "1126/1126 [==============================] - 0s - loss: 282.7891 - val_loss: 289.4064\n",
      "Epoch 229/10000\n",
      "1126/1126 [==============================] - 0s - loss: 280.7702 - val_loss: 287.8813\n",
      "Epoch 230/10000\n",
      "1126/1126 [==============================] - 0s - loss: 278.7768 - val_loss: 286.3671\n",
      "Epoch 231/10000\n",
      "1126/1126 [==============================] - 0s - loss: 276.8080 - val_loss: 284.8631\n",
      "Epoch 232/10000\n",
      "1126/1126 [==============================] - 0s - loss: 274.8627 - val_loss: 283.3688\n",
      "Epoch 233/10000\n",
      "1126/1126 [==============================] - 0s - loss: 272.9403 - val_loss: 281.8837\n",
      "Epoch 234/10000\n",
      "1126/1126 [==============================] - 0s - loss: 271.0401 - val_loss: 280.4075\n",
      "Epoch 235/10000\n",
      "1126/1126 [==============================] - 0s - loss: 269.1614 - val_loss: 278.9398\n",
      "Epoch 236/10000\n",
      "1126/1126 [==============================] - 0s - loss: 267.3036 - val_loss: 277.4804\n",
      "Epoch 237/10000\n",
      "1126/1126 [==============================] - 0s - loss: 265.4658 - val_loss: 276.0291\n",
      "Epoch 238/10000\n",
      "1126/1126 [==============================] - 0s - loss: 263.6478 - val_loss: 274.5860\n",
      "Epoch 239/10000\n",
      "1126/1126 [==============================] - 0s - loss: 261.8487 - val_loss: 273.1508\n",
      "Epoch 240/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 260.0683 - val_loss: 271.7236\n",
      "Epoch 241/10000\n",
      "1126/1126 [==============================] - 0s - loss: 258.3060 - val_loss: 270.3046\n",
      "Epoch 242/10000\n",
      "1126/1126 [==============================] - 0s - loss: 256.5614 - val_loss: 268.8939\n",
      "Epoch 243/10000\n",
      "1126/1126 [==============================] - 0s - loss: 254.8342 - val_loss: 267.4916\n",
      "Epoch 244/10000\n",
      "1126/1126 [==============================] - 0s - loss: 253.1240 - val_loss: 266.0981\n",
      "Epoch 245/10000\n",
      "1126/1126 [==============================] - 0s - loss: 251.4307 - val_loss: 264.7134\n",
      "Epoch 246/10000\n",
      "1126/1126 [==============================] - 0s - loss: 249.7537 - val_loss: 263.3381\n",
      "Epoch 247/10000\n",
      "1126/1126 [==============================] - 0s - loss: 248.0930 - val_loss: 261.9722\n",
      "Epoch 248/10000\n",
      "1126/1126 [==============================] - 0s - loss: 246.4483 - val_loss: 260.6163\n",
      "Epoch 249/10000\n",
      "1126/1126 [==============================] - 0s - loss: 244.8194 - val_loss: 259.2707\n",
      "Epoch 250/10000\n",
      "1126/1126 [==============================] - 0s - loss: 243.2062 - val_loss: 257.9357\n",
      "Epoch 251/10000\n",
      "1126/1126 [==============================] - 0s - loss: 241.6085 - val_loss: 256.6116\n",
      "Epoch 252/10000\n",
      "1126/1126 [==============================] - 0s - loss: 240.0261 - val_loss: 255.2991\n",
      "Epoch 253/10000\n",
      "1126/1126 [==============================] - 0s - loss: 238.4589 - val_loss: 253.9984\n",
      "Epoch 254/10000\n",
      "1126/1126 [==============================] - 0s - loss: 236.9067 - val_loss: 252.7098\n",
      "Epoch 255/10000\n",
      "1126/1126 [==============================] - 0s - loss: 235.3694 - val_loss: 251.4337\n",
      "Epoch 256/10000\n",
      "1126/1126 [==============================] - 0s - loss: 233.8469 - val_loss: 250.1707\n",
      "Epoch 257/10000\n",
      "1126/1126 [==============================] - 0s - loss: 232.3391 - val_loss: 248.9209\n",
      "Epoch 258/10000\n",
      "1126/1126 [==============================] - 0s - loss: 230.8458 - val_loss: 247.6848\n",
      "Epoch 259/10000\n",
      "1126/1126 [==============================] - 0s - loss: 229.3669 - val_loss: 246.4628\n",
      "Epoch 260/10000\n",
      "1126/1126 [==============================] - 0s - loss: 227.9024 - val_loss: 245.2551\n",
      "Epoch 261/10000\n",
      "1126/1126 [==============================] - 0s - loss: 226.4521 - val_loss: 244.0623\n",
      "Epoch 262/10000\n",
      "1126/1126 [==============================] - 0s - loss: 225.0160 - val_loss: 242.8845\n",
      "Epoch 263/10000\n",
      "1126/1126 [==============================] - 0s - loss: 223.5940 - val_loss: 241.7224\n",
      "Epoch 264/10000\n",
      "1126/1126 [==============================] - 0s - loss: 222.1861 - val_loss: 240.5760\n",
      "Epoch 265/10000\n",
      "1126/1126 [==============================] - 0s - loss: 220.7923 - val_loss: 239.4460\n",
      "Epoch 266/10000\n",
      "1126/1126 [==============================] - 0s - loss: 219.4127 - val_loss: 238.3327\n",
      "Epoch 267/10000\n",
      "1126/1126 [==============================] - 0s - loss: 218.0473 - val_loss: 237.2363\n",
      "Epoch 268/10000\n",
      "1126/1126 [==============================] - 0s - loss: 216.6963 - val_loss: 236.1574\n",
      "Epoch 269/10000\n",
      "1126/1126 [==============================] - 0s - loss: 215.3597 - val_loss: 235.0961\n",
      "Epoch 270/10000\n",
      "1126/1126 [==============================] - 0s - loss: 214.0377 - val_loss: 234.0529\n",
      "Epoch 271/10000\n",
      "1126/1126 [==============================] - 0s - loss: 212.7306 - val_loss: 233.0280\n",
      "Epoch 272/10000\n",
      "1126/1126 [==============================] - 0s - loss: 211.4384 - val_loss: 232.0215\n",
      "Epoch 273/10000\n",
      "1126/1126 [==============================] - 0s - loss: 210.1616 - val_loss: 231.0337\n",
      "Epoch 274/10000\n",
      "1126/1126 [==============================] - 0s - loss: 208.9003 - val_loss: 230.0646\n",
      "Epoch 275/10000\n",
      "1126/1126 [==============================] - 0s - loss: 207.6547 - val_loss: 229.1142\n",
      "Epoch 276/10000\n",
      "1126/1126 [==============================] - 0s - loss: 206.4251 - val_loss: 228.1825\n",
      "Epoch 277/10000\n",
      "1126/1126 [==============================] - 0s - loss: 205.2118 - val_loss: 227.2695\n",
      "Epoch 278/10000\n",
      "1126/1126 [==============================] - 0s - loss: 204.0149 - val_loss: 226.3750\n",
      "Epoch 279/10000\n",
      "1126/1126 [==============================] - 0s - loss: 202.8346 - val_loss: 225.4987\n",
      "Epoch 280/10000\n",
      "1126/1126 [==============================] - 0s - loss: 201.6712 - val_loss: 224.6402\n",
      "Epoch 281/10000\n",
      "1126/1126 [==============================] - 0s - loss: 200.5248 - val_loss: 223.7995\n",
      "Epoch 282/10000\n",
      "1126/1126 [==============================] - 0s - loss: 199.3956 - val_loss: 222.9762\n",
      "Epoch 283/10000\n",
      "1126/1126 [==============================] - 0s - loss: 198.2835 - val_loss: 222.1697\n",
      "Epoch 284/10000\n",
      "1126/1126 [==============================] - 0s - loss: 197.1888 - val_loss: 221.3798\n",
      "Epoch 285/10000\n",
      "1126/1126 [==============================] - 0s - loss: 196.1113 - val_loss: 220.6061\n",
      "Epoch 286/10000\n",
      "1126/1126 [==============================] - 0s - loss: 195.0512 - val_loss: 219.8480\n",
      "Epoch 287/10000\n",
      "1126/1126 [==============================] - 0s - loss: 194.0083 - val_loss: 219.1052\n",
      "Epoch 288/10000\n",
      "1126/1126 [==============================] - 0s - loss: 192.9827 - val_loss: 218.3772\n",
      "Epoch 289/10000\n",
      "1126/1126 [==============================] - 0s - loss: 191.9741 - val_loss: 217.6637\n",
      "Epoch 290/10000\n",
      "1126/1126 [==============================] - 0s - loss: 190.9825 - val_loss: 216.9641\n",
      "Epoch 291/10000\n",
      "1126/1126 [==============================] - 0s - loss: 190.0077 - val_loss: 216.2780\n",
      "Epoch 292/10000\n",
      "1126/1126 [==============================] - 0s - loss: 189.0495 - val_loss: 215.6051\n",
      "Epoch 293/10000\n",
      "1126/1126 [==============================] - 0s - loss: 188.1078 - val_loss: 214.9449\n",
      "Epoch 294/10000\n",
      "1126/1126 [==============================] - 0s - loss: 187.1822 - val_loss: 214.2971\n",
      "Epoch 295/10000\n",
      "1126/1126 [==============================] - 0s - loss: 186.2725 - val_loss: 213.6611\n",
      "Epoch 296/10000\n",
      "1126/1126 [==============================] - 0s - loss: 185.3786 - val_loss: 213.0368\n",
      "Epoch 297/10000\n",
      "1126/1126 [==============================] - 0s - loss: 184.4999 - val_loss: 212.4236\n",
      "Epoch 298/10000\n",
      "1126/1126 [==============================] - 0s - loss: 183.6364 - val_loss: 211.8212\n",
      "Epoch 299/10000\n",
      "1126/1126 [==============================] - 0s - loss: 182.7876 - val_loss: 211.2293\n",
      "Epoch 300/10000\n",
      "1126/1126 [==============================] - 0s - loss: 181.9532 - val_loss: 210.6474\n",
      "Epoch 301/10000\n",
      "1126/1126 [==============================] - 0s - loss: 181.1330 - val_loss: 210.0753\n",
      "Epoch 302/10000\n",
      "1126/1126 [==============================] - 0s - loss: 180.3265 - val_loss: 209.5126\n",
      "Epoch 303/10000\n",
      "1126/1126 [==============================] - 0s - loss: 179.5334 - val_loss: 208.9589\n",
      "Epoch 304/10000\n",
      "1126/1126 [==============================] - 0s - loss: 178.7533 - val_loss: 208.4139\n",
      "Epoch 305/10000\n",
      "1126/1126 [==============================] - 0s - loss: 177.9859 - val_loss: 207.8773\n",
      "Epoch 306/10000\n",
      "1126/1126 [==============================] - 0s - loss: 177.2308 - val_loss: 207.3486\n",
      "Epoch 307/10000\n",
      "1126/1126 [==============================] - 0s - loss: 176.4877 - val_loss: 206.8277\n",
      "Epoch 308/10000\n",
      "1126/1126 [==============================] - 0s - loss: 175.7561 - val_loss: 206.3141\n",
      "Epoch 309/10000\n",
      "1126/1126 [==============================] - 0s - loss: 175.0358 - val_loss: 205.8074\n",
      "Epoch 310/10000\n",
      "1126/1126 [==============================] - 0s - loss: 174.3262 - val_loss: 205.3073\n",
      "Epoch 311/10000\n",
      "1126/1126 [==============================] - 0s - loss: 173.6271 - val_loss: 204.8135\n",
      "Epoch 312/10000\n",
      "1126/1126 [==============================] - 0s - loss: 172.9380 - val_loss: 204.3256\n",
      "Epoch 313/10000\n",
      "1126/1126 [==============================] - 0s - loss: 172.2586 - val_loss: 203.8433\n",
      "Epoch 314/10000\n",
      "1126/1126 [==============================] - 0s - loss: 171.5884 - val_loss: 203.3660\n",
      "Epoch 315/10000\n",
      "1126/1126 [==============================] - 0s - loss: 170.9272 - val_loss: 202.8936\n",
      "Epoch 316/10000\n",
      "1126/1126 [==============================] - 0s - loss: 170.2746 - val_loss: 202.4256\n",
      "Epoch 317/10000\n",
      "1126/1126 [==============================] - 0s - loss: 169.6301 - val_loss: 201.9615\n",
      "Epoch 318/10000\n",
      "1126/1126 [==============================] - 0s - loss: 168.9933 - val_loss: 201.5011\n",
      "Epoch 319/10000\n",
      "1126/1126 [==============================] - 0s - loss: 168.3641 - val_loss: 201.0438\n",
      "Epoch 320/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 167.7418 - val_loss: 200.5895\n",
      "Epoch 321/10000\n",
      "1126/1126 [==============================] - 0s - loss: 167.1263 - val_loss: 200.1374\n",
      "Epoch 322/10000\n",
      "1126/1126 [==============================] - 0s - loss: 166.5171 - val_loss: 199.6875\n",
      "Epoch 323/10000\n",
      "1126/1126 [==============================] - 0s - loss: 165.9139 - val_loss: 199.2392\n",
      "Epoch 324/10000\n",
      "1126/1126 [==============================] - 0s - loss: 165.3164 - val_loss: 198.7921\n",
      "Epoch 325/10000\n",
      "1126/1126 [==============================] - 0s - loss: 164.7242 - val_loss: 198.3460\n",
      "Epoch 326/10000\n",
      "1126/1126 [==============================] - 0s - loss: 164.1369 - val_loss: 197.9003\n",
      "Epoch 327/10000\n",
      "1126/1126 [==============================] - 0s - loss: 163.5544 - val_loss: 197.4549\n",
      "Epoch 328/10000\n",
      "1126/1126 [==============================] - 0s - loss: 162.9761 - val_loss: 197.0093\n",
      "Epoch 329/10000\n",
      "1126/1126 [==============================] - 0s - loss: 162.4019 - val_loss: 196.5633\n",
      "Epoch 330/10000\n",
      "1126/1126 [==============================] - 0s - loss: 161.8314 - val_loss: 196.1166\n",
      "Epoch 331/10000\n",
      "1126/1126 [==============================] - 0s - loss: 161.2643 - val_loss: 195.6690\n",
      "Epoch 332/10000\n",
      "1126/1126 [==============================] - 0s - loss: 160.7004 - val_loss: 195.2202\n",
      "Epoch 333/10000\n",
      "1126/1126 [==============================] - 0s - loss: 160.1392 - val_loss: 194.7701\n",
      "Epoch 334/10000\n",
      "1126/1126 [==============================] - 0s - loss: 159.5807 - val_loss: 194.3185\n",
      "Epoch 335/10000\n",
      "1126/1126 [==============================] - 0s - loss: 159.0245 - val_loss: 193.8654\n",
      "Epoch 336/10000\n",
      "1126/1126 [==============================] - 0s - loss: 158.4702 - val_loss: 193.4106\n",
      "Epoch 337/10000\n",
      "1126/1126 [==============================] - 0s - loss: 157.9178 - val_loss: 192.9542\n",
      "Epoch 338/10000\n",
      "1126/1126 [==============================] - 0s - loss: 157.3669 - val_loss: 192.4961\n",
      "Epoch 339/10000\n",
      "1126/1126 [==============================] - 0s - loss: 156.8172 - val_loss: 192.0365\n",
      "Epoch 340/10000\n",
      "1126/1126 [==============================] - 0s - loss: 156.2686 - val_loss: 191.5755\n",
      "Epoch 341/10000\n",
      "1126/1126 [==============================] - 0s - loss: 155.7208 - val_loss: 191.1131\n",
      "Epoch 342/10000\n",
      "1126/1126 [==============================] - 0s - loss: 155.1736 - val_loss: 190.6496\n",
      "Epoch 343/10000\n",
      "1126/1126 [==============================] - 0s - loss: 154.6267 - val_loss: 190.1851\n",
      "Epoch 344/10000\n",
      "1126/1126 [==============================] - 0s - loss: 154.0800 - val_loss: 189.7198\n",
      "Epoch 345/10000\n",
      "1126/1126 [==============================] - 0s - loss: 153.5332 - val_loss: 189.2541\n",
      "Epoch 346/10000\n",
      "1126/1126 [==============================] - 0s - loss: 152.9861 - val_loss: 188.7881\n",
      "Epoch 347/10000\n",
      "1126/1126 [==============================] - 0s - loss: 152.4386 - val_loss: 188.3221\n",
      "Epoch 348/10000\n",
      "1126/1126 [==============================] - 0s - loss: 151.8904 - val_loss: 187.8564\n",
      "Epoch 349/10000\n",
      "1126/1126 [==============================] - 0s - loss: 151.3414 - val_loss: 187.3912\n",
      "Epoch 350/10000\n",
      "1126/1126 [==============================] - 0s - loss: 150.7915 - val_loss: 186.9268\n",
      "Epoch 351/10000\n",
      "1126/1126 [==============================] - 0s - loss: 150.2405 - val_loss: 186.4635\n",
      "Epoch 352/10000\n",
      "1126/1126 [==============================] - 0s - loss: 149.6883 - val_loss: 186.0016\n",
      "Epoch 353/10000\n",
      "1126/1126 [==============================] - 0s - loss: 149.1348 - val_loss: 185.5411\n",
      "Epoch 354/10000\n",
      "1126/1126 [==============================] - 0s - loss: 148.5800 - val_loss: 185.0824\n",
      "Epoch 355/10000\n",
      "1126/1126 [==============================] - 0s - loss: 148.0237 - val_loss: 184.6255\n",
      "Epoch 356/10000\n",
      "1126/1126 [==============================] - 0s - loss: 147.4661 - val_loss: 184.1706\n",
      "Epoch 357/10000\n",
      "1126/1126 [==============================] - 0s - loss: 146.9071 - val_loss: 183.7179\n",
      "Epoch 358/10000\n",
      "1126/1126 [==============================] - 0s - loss: 146.3467 - val_loss: 183.2675\n",
      "Epoch 359/10000\n",
      "1126/1126 [==============================] - 0s - loss: 145.7851 - val_loss: 182.8194\n",
      "Epoch 360/10000\n",
      "1126/1126 [==============================] - 0s - loss: 145.2223 - val_loss: 182.3737\n",
      "Epoch 361/10000\n",
      "1126/1126 [==============================] - 0s - loss: 144.6586 - val_loss: 181.9304\n",
      "Epoch 362/10000\n",
      "1126/1126 [==============================] - 0s - loss: 144.0940 - val_loss: 181.4897\n",
      "Epoch 363/10000\n",
      "1126/1126 [==============================] - 0s - loss: 143.5288 - val_loss: 181.0514\n",
      "Epoch 364/10000\n",
      "1126/1126 [==============================] - 0s - loss: 142.9632 - val_loss: 180.6156\n",
      "Epoch 365/10000\n",
      "1126/1126 [==============================] - 0s - loss: 142.3973 - val_loss: 180.1823\n",
      "Epoch 366/10000\n",
      "1126/1126 [==============================] - 0s - loss: 141.8316 - val_loss: 179.7515\n",
      "Epoch 367/10000\n",
      "1126/1126 [==============================] - 0s - loss: 141.2661 - val_loss: 179.3231\n",
      "Epoch 368/10000\n",
      "1126/1126 [==============================] - 0s - loss: 140.7013 - val_loss: 178.8972\n",
      "Epoch 369/10000\n",
      "1126/1126 [==============================] - 0s - loss: 140.1374 - val_loss: 178.4737\n",
      "Epoch 370/10000\n",
      "1126/1126 [==============================] - 0s - loss: 139.5746 - val_loss: 178.0527\n",
      "Epoch 371/10000\n",
      "1126/1126 [==============================] - 0s - loss: 139.0133 - val_loss: 177.6339\n",
      "Epoch 372/10000\n",
      "1126/1126 [==============================] - 0s - loss: 138.4538 - val_loss: 177.2175\n",
      "Epoch 373/10000\n",
      "1126/1126 [==============================] - 0s - loss: 137.8963 - val_loss: 176.8033\n",
      "Epoch 374/10000\n",
      "1126/1126 [==============================] - 0s - loss: 137.3412 - val_loss: 176.3913\n",
      "Epoch 375/10000\n",
      "1126/1126 [==============================] - 0s - loss: 136.7887 - val_loss: 175.9813\n",
      "Epoch 376/10000\n",
      "1126/1126 [==============================] - 0s - loss: 136.2392 - val_loss: 175.5734\n",
      "Epoch 377/10000\n",
      "1126/1126 [==============================] - 0s - loss: 135.6928 - val_loss: 175.1673\n",
      "Epoch 378/10000\n",
      "1126/1126 [==============================] - 0s - loss: 135.1498 - val_loss: 174.7629\n",
      "Epoch 379/10000\n",
      "1126/1126 [==============================] - 0s - loss: 134.6105 - val_loss: 174.3602\n",
      "Epoch 380/10000\n",
      "1126/1126 [==============================] - 0s - loss: 134.0751 - val_loss: 173.9590\n",
      "Epoch 381/10000\n",
      "1126/1126 [==============================] - 0s - loss: 133.5436 - val_loss: 173.5589\n",
      "Epoch 382/10000\n",
      "1126/1126 [==============================] - 0s - loss: 133.0162 - val_loss: 173.1600\n",
      "Epoch 383/10000\n",
      "1126/1126 [==============================] - 0s - loss: 132.4930 - val_loss: 172.7622\n",
      "Epoch 384/10000\n",
      "1126/1126 [==============================] - 0s - loss: 131.9740 - val_loss: 172.3653\n",
      "Epoch 385/10000\n",
      "1126/1126 [==============================] - 0s - loss: 131.4592 - val_loss: 171.9690\n",
      "Epoch 386/10000\n",
      "1126/1126 [==============================] - 0s - loss: 130.9486 - val_loss: 171.5733\n",
      "Epoch 387/10000\n",
      "1126/1126 [==============================] - 0s - loss: 130.4422 - val_loss: 171.1782\n",
      "Epoch 388/10000\n",
      "1126/1126 [==============================] - 0s - loss: 129.9397 - val_loss: 170.7834\n",
      "Epoch 389/10000\n",
      "1126/1126 [==============================] - 0s - loss: 129.4413 - val_loss: 170.3889\n",
      "Epoch 390/10000\n",
      "1126/1126 [==============================] - 0s - loss: 128.9466 - val_loss: 169.9947\n",
      "Epoch 391/10000\n",
      "1126/1126 [==============================] - 0s - loss: 128.4557 - val_loss: 169.6008\n",
      "Epoch 392/10000\n",
      "1126/1126 [==============================] - 0s - loss: 127.9683 - val_loss: 169.2071\n",
      "Epoch 393/10000\n",
      "1126/1126 [==============================] - 0s - loss: 127.4845 - val_loss: 168.8136\n",
      "Epoch 394/10000\n",
      "1126/1126 [==============================] - 0s - loss: 127.0041 - val_loss: 168.4205\n",
      "Epoch 395/10000\n",
      "1126/1126 [==============================] - 0s - loss: 126.5270 - val_loss: 168.0276\n",
      "Epoch 396/10000\n",
      "1126/1126 [==============================] - 0s - loss: 126.0532 - val_loss: 167.6351\n",
      "Epoch 397/10000\n",
      "1126/1126 [==============================] - 0s - loss: 125.5827 - val_loss: 167.2431\n",
      "Epoch 398/10000\n",
      "1126/1126 [==============================] - 0s - loss: 125.1153 - val_loss: 166.8516\n",
      "Epoch 399/10000\n",
      "1126/1126 [==============================] - 0s - loss: 124.6512 - val_loss: 166.4607\n",
      "Epoch 400/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 124.1901 - val_loss: 166.0704\n",
      "Epoch 401/10000\n",
      "1126/1126 [==============================] - 0s - loss: 123.7323 - val_loss: 165.6809\n",
      "Epoch 402/10000\n",
      "1126/1126 [==============================] - 0s - loss: 123.2776 - val_loss: 165.2923\n",
      "Epoch 403/10000\n",
      "1126/1126 [==============================] - 0s - loss: 122.8261 - val_loss: 164.9045\n",
      "Epoch 404/10000\n",
      "1126/1126 [==============================] - 0s - loss: 122.3777 - val_loss: 164.5177\n",
      "Epoch 405/10000\n",
      "1126/1126 [==============================] - 0s - loss: 121.9323 - val_loss: 164.1318\n",
      "Epoch 406/10000\n",
      "1126/1126 [==============================] - 0s - loss: 121.4900 - val_loss: 163.7469\n",
      "Epoch 407/10000\n",
      "1126/1126 [==============================] - 0s - loss: 121.0507 - val_loss: 163.3631\n",
      "Epoch 408/10000\n",
      "1126/1126 [==============================] - 0s - loss: 120.6143 - val_loss: 162.9803\n",
      "Epoch 409/10000\n",
      "1126/1126 [==============================] - 0s - loss: 120.1806 - val_loss: 162.5985\n",
      "Epoch 410/10000\n",
      "1126/1126 [==============================] - 0s - loss: 119.7497 - val_loss: 162.2177\n",
      "Epoch 411/10000\n",
      "1126/1126 [==============================] - 0s - loss: 119.3214 - val_loss: 161.8379\n",
      "Epoch 412/10000\n",
      "1126/1126 [==============================] - 0s - loss: 118.8956 - val_loss: 161.4591\n",
      "Epoch 413/10000\n",
      "1126/1126 [==============================] - 0s - loss: 118.4721 - val_loss: 161.0812\n",
      "Epoch 414/10000\n",
      "1126/1126 [==============================] - 0s - loss: 118.0508 - val_loss: 160.7041\n",
      "Epoch 415/10000\n",
      "1126/1126 [==============================] - 0s - loss: 117.6315 - val_loss: 160.3280\n",
      "Epoch 416/10000\n",
      "1126/1126 [==============================] - 0s - loss: 117.2143 - val_loss: 159.9526\n",
      "Epoch 417/10000\n",
      "1126/1126 [==============================] - 0s - loss: 116.7988 - val_loss: 159.5780\n",
      "Epoch 418/10000\n",
      "1126/1126 [==============================] - 0s - loss: 116.3850 - val_loss: 159.2042\n",
      "Epoch 419/10000\n",
      "1126/1126 [==============================] - 0s - loss: 115.9728 - val_loss: 158.8310\n",
      "Epoch 420/10000\n",
      "1126/1126 [==============================] - 0s - loss: 115.5621 - val_loss: 158.4586\n",
      "Epoch 421/10000\n",
      "1126/1126 [==============================] - 0s - loss: 115.1528 - val_loss: 158.0868\n",
      "Epoch 422/10000\n",
      "1126/1126 [==============================] - 0s - loss: 114.7447 - val_loss: 157.7158\n",
      "Epoch 423/10000\n",
      "1126/1126 [==============================] - 0s - loss: 114.3378 - val_loss: 157.3453\n",
      "Epoch 424/10000\n",
      "1126/1126 [==============================] - 0s - loss: 113.9320 - val_loss: 156.9755\n",
      "Epoch 425/10000\n",
      "1126/1126 [==============================] - 0s - loss: 113.5273 - val_loss: 156.6063\n",
      "Epoch 426/10000\n",
      "1126/1126 [==============================] - 0s - loss: 113.1235 - val_loss: 156.2378\n",
      "Epoch 427/10000\n",
      "1126/1126 [==============================] - 0s - loss: 112.7207 - val_loss: 155.8700\n",
      "Epoch 428/10000\n",
      "1126/1126 [==============================] - 0s - loss: 112.3189 - val_loss: 155.5029\n",
      "Epoch 429/10000\n",
      "1126/1126 [==============================] - 0s - loss: 111.9179 - val_loss: 155.1365\n",
      "Epoch 430/10000\n",
      "1126/1126 [==============================] - 0s - loss: 111.5177 - val_loss: 154.7709\n",
      "Epoch 431/10000\n",
      "1126/1126 [==============================] - 0s - loss: 111.1184 - val_loss: 154.4062\n",
      "Epoch 432/10000\n",
      "1126/1126 [==============================] - 0s - loss: 110.7200 - val_loss: 154.0423\n",
      "Epoch 433/10000\n",
      "1126/1126 [==============================] - 0s - loss: 110.3225 - val_loss: 153.6793\n",
      "Epoch 434/10000\n",
      "1126/1126 [==============================] - 0s - loss: 109.9259 - val_loss: 153.3173\n",
      "Epoch 435/10000\n",
      "1126/1126 [==============================] - 0s - loss: 109.5302 - val_loss: 152.9564\n",
      "Epoch 436/10000\n",
      "1126/1126 [==============================] - 0s - loss: 109.1354 - val_loss: 152.5966\n",
      "Epoch 437/10000\n",
      "1126/1126 [==============================] - 0s - loss: 108.7417 - val_loss: 152.2379\n",
      "Epoch 438/10000\n",
      "1126/1126 [==============================] - 0s - loss: 108.3491 - val_loss: 151.8805\n",
      "Epoch 439/10000\n",
      "1126/1126 [==============================] - 0s - loss: 107.9576 - val_loss: 151.5244\n",
      "Epoch 440/10000\n",
      "1126/1126 [==============================] - 0s - loss: 107.5673 - val_loss: 151.1696\n",
      "Epoch 441/10000\n",
      "1126/1126 [==============================] - 0s - loss: 107.1782 - val_loss: 150.8163\n",
      "Epoch 442/10000\n",
      "1126/1126 [==============================] - 0s - loss: 106.7905 - val_loss: 150.4643\n",
      "Epoch 443/10000\n",
      "1126/1126 [==============================] - 0s - loss: 106.4042 - val_loss: 150.1139\n",
      "Epoch 444/10000\n",
      "1126/1126 [==============================] - 0s - loss: 106.0194 - val_loss: 149.7651\n",
      "Epoch 445/10000\n",
      "1126/1126 [==============================] - 0s - loss: 105.6362 - val_loss: 149.4178\n",
      "Epoch 446/10000\n",
      "1126/1126 [==============================] - 0s - loss: 105.2546 - val_loss: 149.0721\n",
      "Epoch 447/10000\n",
      "1126/1126 [==============================] - 0s - loss: 104.8746 - val_loss: 148.7280\n",
      "Epoch 448/10000\n",
      "1126/1126 [==============================] - 0s - loss: 104.4965 - val_loss: 148.3856\n",
      "Epoch 449/10000\n",
      "1126/1126 [==============================] - 0s - loss: 104.1201 - val_loss: 148.0448\n",
      "Epoch 450/10000\n",
      "1126/1126 [==============================] - 0s - loss: 103.7457 - val_loss: 147.7057\n",
      "Epoch 451/10000\n",
      "1126/1126 [==============================] - 0s - loss: 103.3731 - val_loss: 147.3681\n",
      "Epoch 452/10000\n",
      "1126/1126 [==============================] - 0s - loss: 103.0025 - val_loss: 147.0323\n",
      "Epoch 453/10000\n",
      "1126/1126 [==============================] - 0s - loss: 102.6340 - val_loss: 146.6980\n",
      "Epoch 454/10000\n",
      "1126/1126 [==============================] - 0s - loss: 102.2674 - val_loss: 146.3654\n",
      "Epoch 455/10000\n",
      "1126/1126 [==============================] - 0s - loss: 101.9029 - val_loss: 146.0343\n",
      "Epoch 456/10000\n",
      "1126/1126 [==============================] - 0s - loss: 101.5405 - val_loss: 145.7048\n",
      "Epoch 457/10000\n",
      "1126/1126 [==============================] - 0s - loss: 101.1802 - val_loss: 145.3768\n",
      "Epoch 458/10000\n",
      "1126/1126 [==============================] - 0s - loss: 100.8221 - val_loss: 145.0504\n",
      "Epoch 459/10000\n",
      "1126/1126 [==============================] - 0s - loss: 100.4660 - val_loss: 144.7254\n",
      "Epoch 460/10000\n",
      "1126/1126 [==============================] - 0s - loss: 100.1120 - val_loss: 144.4019\n",
      "Epoch 461/10000\n",
      "1126/1126 [==============================] - 0s - loss: 99.7602 - val_loss: 144.0799\n",
      "Epoch 462/10000\n",
      "1126/1126 [==============================] - 0s - loss: 99.4105 - val_loss: 143.7592\n",
      "Epoch 463/10000\n",
      "1126/1126 [==============================] - 0s - loss: 99.0628 - val_loss: 143.4400\n",
      "Epoch 464/10000\n",
      "1126/1126 [==============================] - 0s - loss: 98.7173 - val_loss: 143.1221\n",
      "Epoch 465/10000\n",
      "1126/1126 [==============================] - 0s - loss: 98.3739 - val_loss: 142.8056\n",
      "Epoch 466/10000\n",
      "1126/1126 [==============================] - 0s - loss: 98.0326 - val_loss: 142.4904\n",
      "Epoch 467/10000\n",
      "1126/1126 [==============================] - 0s - loss: 97.6932 - val_loss: 142.1765\n",
      "Epoch 468/10000\n",
      "1126/1126 [==============================] - 0s - loss: 97.3560 - val_loss: 141.8640\n",
      "Epoch 469/10000\n",
      "1126/1126 [==============================] - 0s - loss: 97.0208 - val_loss: 141.5527\n",
      "Epoch 470/10000\n",
      "1126/1126 [==============================] - 0s - loss: 96.6875 - val_loss: 141.2427\n",
      "Epoch 471/10000\n",
      "1126/1126 [==============================] - 0s - loss: 96.3563 - val_loss: 140.9340\n",
      "Epoch 472/10000\n",
      "1126/1126 [==============================] - 0s - loss: 96.0271 - val_loss: 140.6266\n",
      "Epoch 473/10000\n",
      "1126/1126 [==============================] - 0s - loss: 95.6998 - val_loss: 140.3205\n",
      "Epoch 474/10000\n",
      "1126/1126 [==============================] - 0s - loss: 95.3744 - val_loss: 140.0156\n",
      "Epoch 475/10000\n",
      "1126/1126 [==============================] - 0s - loss: 95.0509 - val_loss: 139.7120\n",
      "Epoch 476/10000\n",
      "1126/1126 [==============================] - 0s - loss: 94.7294 - val_loss: 139.4098\n",
      "Epoch 477/10000\n",
      "1126/1126 [==============================] - 0s - loss: 94.4097 - val_loss: 139.1088\n",
      "Epoch 478/10000\n",
      "1126/1126 [==============================] - 0s - loss: 94.0919 - val_loss: 138.8091\n",
      "Epoch 479/10000\n",
      "1126/1126 [==============================] - 0s - loss: 93.7759 - val_loss: 138.5108\n",
      "Epoch 480/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 93.4618 - val_loss: 138.2137\n",
      "Epoch 481/10000\n",
      "1126/1126 [==============================] - 0s - loss: 93.1495 - val_loss: 137.9181\n",
      "Epoch 482/10000\n",
      "1126/1126 [==============================] - 0s - loss: 92.8389 - val_loss: 137.6238\n",
      "Epoch 483/10000\n",
      "1126/1126 [==============================] - 0s - loss: 92.5301 - val_loss: 137.3308\n",
      "Epoch 484/10000\n",
      "1126/1126 [==============================] - 0s - loss: 92.2230 - val_loss: 137.0392\n",
      "Epoch 485/10000\n",
      "1126/1126 [==============================] - 0s - loss: 91.9178 - val_loss: 136.7491\n",
      "Epoch 486/10000\n",
      "1126/1126 [==============================] - 0s - loss: 91.6142 - val_loss: 136.4603\n",
      "Epoch 487/10000\n",
      "1126/1126 [==============================] - 0s - loss: 91.3123 - val_loss: 136.1730\n",
      "Epoch 488/10000\n",
      "1126/1126 [==============================] - 0s - loss: 91.0121 - val_loss: 135.8871\n",
      "Epoch 489/10000\n",
      "1126/1126 [==============================] - 0s - loss: 90.7135 - val_loss: 135.6027\n",
      "Epoch 490/10000\n",
      "1126/1126 [==============================] - 0s - loss: 90.4166 - val_loss: 135.3197\n",
      "Epoch 491/10000\n",
      "1126/1126 [==============================] - 0s - loss: 90.1214 - val_loss: 135.0382\n",
      "Epoch 492/10000\n",
      "1126/1126 [==============================] - 0s - loss: 89.8277 - val_loss: 134.7582\n",
      "Epoch 493/10000\n",
      "1126/1126 [==============================] - 0s - loss: 89.5357 - val_loss: 134.4797\n",
      "Epoch 494/10000\n",
      "1126/1126 [==============================] - 0s - loss: 89.2453 - val_loss: 134.2027\n",
      "Epoch 495/10000\n",
      "1126/1126 [==============================] - 0s - loss: 88.9564 - val_loss: 133.9272\n",
      "Epoch 496/10000\n",
      "1126/1126 [==============================] - 0s - loss: 88.6692 - val_loss: 133.6533\n",
      "Epoch 497/10000\n",
      "1126/1126 [==============================] - 0s - loss: 88.3835 - val_loss: 133.3808\n",
      "Epoch 498/10000\n",
      "1126/1126 [==============================] - 0s - loss: 88.0993 - val_loss: 133.1099\n",
      "Epoch 499/10000\n",
      "1126/1126 [==============================] - 0s - loss: 87.8167 - val_loss: 132.8404\n",
      "Epoch 500/10000\n",
      "1126/1126 [==============================] - 0s - loss: 87.5357 - val_loss: 132.5726\n",
      "Epoch 501/10000\n",
      "1126/1126 [==============================] - 0s - loss: 87.2561 - val_loss: 132.3062\n",
      "Epoch 502/10000\n",
      "1126/1126 [==============================] - 0s - loss: 86.9781 - val_loss: 132.0413\n",
      "Epoch 503/10000\n",
      "1126/1126 [==============================] - 0s - loss: 86.7015 - val_loss: 131.7779\n",
      "Epoch 504/10000\n",
      "1126/1126 [==============================] - 0s - loss: 86.4265 - val_loss: 131.5159\n",
      "Epoch 505/10000\n",
      "1126/1126 [==============================] - 0s - loss: 86.1530 - val_loss: 131.2554\n",
      "Epoch 506/10000\n",
      "1126/1126 [==============================] - 0s - loss: 85.8811 - val_loss: 130.9964\n",
      "Epoch 507/10000\n",
      "1126/1126 [==============================] - 0s - loss: 85.6106 - val_loss: 130.7387\n",
      "Epoch 508/10000\n",
      "1126/1126 [==============================] - 0s - loss: 85.3416 - val_loss: 130.4824\n",
      "Epoch 509/10000\n",
      "1126/1126 [==============================] - 0s - loss: 85.0741 - val_loss: 130.2275\n",
      "Epoch 510/10000\n",
      "1126/1126 [==============================] - 0s - loss: 84.8082 - val_loss: 129.9740\n",
      "Epoch 511/10000\n",
      "1126/1126 [==============================] - 0s - loss: 84.5437 - val_loss: 129.7217\n",
      "Epoch 512/10000\n",
      "1126/1126 [==============================] - 0s - loss: 84.2807 - val_loss: 129.4708\n",
      "Epoch 513/10000\n",
      "1126/1126 [==============================] - 0s - loss: 84.0193 - val_loss: 129.2211\n",
      "Epoch 514/10000\n",
      "1126/1126 [==============================] - 0s - loss: 83.7594 - val_loss: 128.9726\n",
      "Epoch 515/10000\n",
      "1126/1126 [==============================] - 0s - loss: 83.5010 - val_loss: 128.7253\n",
      "Epoch 516/10000\n",
      "1126/1126 [==============================] - 0s - loss: 83.2442 - val_loss: 128.4791\n",
      "Epoch 517/10000\n",
      "1126/1126 [==============================] - 0s - loss: 82.9889 - val_loss: 128.2340\n",
      "Epoch 518/10000\n",
      "1126/1126 [==============================] - 0s - loss: 82.7351 - val_loss: 127.9900\n",
      "Epoch 519/10000\n",
      "1126/1126 [==============================] - 0s - loss: 82.4829 - val_loss: 127.7469\n",
      "Epoch 520/10000\n",
      "1126/1126 [==============================] - 0s - loss: 82.2323 - val_loss: 127.5049\n",
      "Epoch 521/10000\n",
      "1126/1126 [==============================] - 0s - loss: 81.9833 - val_loss: 127.2638\n",
      "Epoch 522/10000\n",
      "1126/1126 [==============================] - 0s - loss: 81.7358 - val_loss: 127.0235\n",
      "Epoch 523/10000\n",
      "1126/1126 [==============================] - 0s - loss: 81.4899 - val_loss: 126.7841\n",
      "Epoch 524/10000\n",
      "1126/1126 [==============================] - 0s - loss: 81.2457 - val_loss: 126.5454\n",
      "Epoch 525/10000\n",
      "1126/1126 [==============================] - 0s - loss: 81.0030 - val_loss: 126.3076\n",
      "Epoch 526/10000\n",
      "1126/1126 [==============================] - 0s - loss: 80.7620 - val_loss: 126.0704\n",
      "Epoch 527/10000\n",
      "1126/1126 [==============================] - 0s - loss: 80.5226 - val_loss: 125.8339\n",
      "Epoch 528/10000\n",
      "1126/1126 [==============================] - 0s - loss: 80.2849 - val_loss: 125.5979\n",
      "Epoch 529/10000\n",
      "1126/1126 [==============================] - 0s - loss: 80.0488 - val_loss: 125.3626\n",
      "Epoch 530/10000\n",
      "1126/1126 [==============================] - 0s - loss: 79.8143 - val_loss: 125.1278\n",
      "Epoch 531/10000\n",
      "1126/1126 [==============================] - 0s - loss: 79.5816 - val_loss: 124.8934\n",
      "Epoch 532/10000\n",
      "1126/1126 [==============================] - 0s - loss: 79.3505 - val_loss: 124.6595\n",
      "Epoch 533/10000\n",
      "1126/1126 [==============================] - 0s - loss: 79.1211 - val_loss: 124.4260\n",
      "Epoch 534/10000\n",
      "1126/1126 [==============================] - 0s - loss: 78.8933 - val_loss: 124.1929\n",
      "Epoch 535/10000\n",
      "1126/1126 [==============================] - 0s - loss: 78.6673 - val_loss: 123.9600\n",
      "Epoch 536/10000\n",
      "1126/1126 [==============================] - 0s - loss: 78.4430 - val_loss: 123.7275\n",
      "Epoch 537/10000\n",
      "1126/1126 [==============================] - 0s - loss: 78.2203 - val_loss: 123.4952\n",
      "Epoch 538/10000\n",
      "1126/1126 [==============================] - 0s - loss: 77.9994 - val_loss: 123.2632\n",
      "Epoch 539/10000\n",
      "1126/1126 [==============================] - 0s - loss: 77.7801 - val_loss: 123.0313\n",
      "Epoch 540/10000\n",
      "1126/1126 [==============================] - 0s - loss: 77.5626 - val_loss: 122.7996\n",
      "Epoch 541/10000\n",
      "1126/1126 [==============================] - 0s - loss: 77.3468 - val_loss: 122.5680\n",
      "Epoch 542/10000\n",
      "1126/1126 [==============================] - 0s - loss: 77.1326 - val_loss: 122.3365\n",
      "Epoch 543/10000\n",
      "1126/1126 [==============================] - 0s - loss: 76.9202 - val_loss: 122.1052\n",
      "Epoch 544/10000\n",
      "1126/1126 [==============================] - 0s - loss: 76.7095 - val_loss: 121.8739\n",
      "Epoch 545/10000\n",
      "1126/1126 [==============================] - 0s - loss: 76.5005 - val_loss: 121.6427\n",
      "Epoch 546/10000\n",
      "1126/1126 [==============================] - 0s - loss: 76.2931 - val_loss: 121.4114\n",
      "Epoch 547/10000\n",
      "1126/1126 [==============================] - 0s - loss: 76.0875 - val_loss: 121.1802\n",
      "Epoch 548/10000\n",
      "1126/1126 [==============================] - 0s - loss: 75.8836 - val_loss: 120.9490\n",
      "Epoch 549/10000\n",
      "1126/1126 [==============================] - 0s - loss: 75.6813 - val_loss: 120.7177\n",
      "Epoch 550/10000\n",
      "1126/1126 [==============================] - 0s - loss: 75.4807 - val_loss: 120.4865\n",
      "Epoch 551/10000\n",
      "1126/1126 [==============================] - 0s - loss: 75.2818 - val_loss: 120.2551\n",
      "Epoch 552/10000\n",
      "1126/1126 [==============================] - 0s - loss: 75.0845 - val_loss: 120.0238\n",
      "Epoch 553/10000\n",
      "1126/1126 [==============================] - 0s - loss: 74.8888 - val_loss: 119.7923\n",
      "Epoch 554/10000\n",
      "1126/1126 [==============================] - 0s - loss: 74.6948 - val_loss: 119.5608\n",
      "Epoch 555/10000\n",
      "1126/1126 [==============================] - 0s - loss: 74.5024 - val_loss: 119.3292\n",
      "Epoch 556/10000\n",
      "1126/1126 [==============================] - 0s - loss: 74.3117 - val_loss: 119.0975\n",
      "Epoch 557/10000\n",
      "1126/1126 [==============================] - 0s - loss: 74.1225 - val_loss: 118.8657\n",
      "Epoch 558/10000\n",
      "1126/1126 [==============================] - 0s - loss: 73.9349 - val_loss: 118.6337\n",
      "Epoch 559/10000\n",
      "1126/1126 [==============================] - 0s - loss: 73.7489 - val_loss: 118.4017\n",
      "Epoch 560/10000\n",
      "1126/1126 [==============================] - 0s - loss: 73.5644 - val_loss: 118.1695\n",
      "Epoch 561/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 73.3815 - val_loss: 117.9372\n",
      "Epoch 562/10000\n",
      "1126/1126 [==============================] - 0s - loss: 73.2001 - val_loss: 117.7048\n",
      "Epoch 563/10000\n",
      "1126/1126 [==============================] - 0s - loss: 73.0203 - val_loss: 117.4722\n",
      "Epoch 564/10000\n",
      "1126/1126 [==============================] - 0s - loss: 72.8419 - val_loss: 117.2395\n",
      "Epoch 565/10000\n",
      "1126/1126 [==============================] - 0s - loss: 72.6650 - val_loss: 117.0065\n",
      "Epoch 566/10000\n",
      "1126/1126 [==============================] - 0s - loss: 72.4896 - val_loss: 116.7735\n",
      "Epoch 567/10000\n",
      "1126/1126 [==============================] - 0s - loss: 72.3157 - val_loss: 116.5404\n",
      "Epoch 568/10000\n",
      "1126/1126 [==============================] - 0s - loss: 72.1432 - val_loss: 116.3070\n",
      "Epoch 569/10000\n",
      "1126/1126 [==============================] - 0s - loss: 71.9721 - val_loss: 116.0736\n",
      "Epoch 570/10000\n",
      "1126/1126 [==============================] - 0s - loss: 71.8024 - val_loss: 115.8400\n",
      "Epoch 571/10000\n",
      "1126/1126 [==============================] - 0s - loss: 71.6342 - val_loss: 115.6062\n",
      "Epoch 572/10000\n",
      "1126/1126 [==============================] - 0s - loss: 71.4673 - val_loss: 115.3724\n",
      "Epoch 573/10000\n",
      "1126/1126 [==============================] - 0s - loss: 71.3018 - val_loss: 115.1384\n",
      "Epoch 574/10000\n",
      "1126/1126 [==============================] - 0s - loss: 71.1377 - val_loss: 114.9043\n",
      "Epoch 575/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.9749 - val_loss: 114.6701\n",
      "Epoch 576/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.8135 - val_loss: 114.4359\n",
      "Epoch 577/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.6534 - val_loss: 114.2016\n",
      "Epoch 578/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.4947 - val_loss: 113.9673\n",
      "Epoch 579/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.3372 - val_loss: 113.7330\n",
      "Epoch 580/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.1811 - val_loss: 113.4986\n",
      "Epoch 581/10000\n",
      "1126/1126 [==============================] - 0s - loss: 70.0262 - val_loss: 113.2644\n",
      "Epoch 582/10000\n",
      "1126/1126 [==============================] - 0s - loss: 69.8727 - val_loss: 113.0302\n",
      "Epoch 583/10000\n",
      "1126/1126 [==============================] - 0s - loss: 69.7205 - val_loss: 112.7961\n",
      "Epoch 584/10000\n",
      "1126/1126 [==============================] - 0s - loss: 69.5695 - val_loss: 112.5622\n",
      "Epoch 585/10000\n",
      "1126/1126 [==============================] - 0s - loss: 69.4198 - val_loss: 112.3285\n",
      "Epoch 586/10000\n",
      "1126/1126 [==============================] - 0s - loss: 69.2714 - val_loss: 112.0951\n",
      "Epoch 587/10000\n",
      "1126/1126 [==============================] - 0s - loss: 69.1243 - val_loss: 111.8618\n",
      "Epoch 588/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.9785 - val_loss: 111.6290\n",
      "Epoch 589/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.8339 - val_loss: 111.3965\n",
      "Epoch 590/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.6906 - val_loss: 111.1643\n",
      "Epoch 591/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.5485 - val_loss: 110.9327\n",
      "Epoch 592/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.4077 - val_loss: 110.7016\n",
      "Epoch 593/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.2682 - val_loss: 110.4710\n",
      "Epoch 594/10000\n",
      "1126/1126 [==============================] - 0s - loss: 68.1299 - val_loss: 110.2410\n",
      "Epoch 595/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.9929 - val_loss: 110.0117\n",
      "Epoch 596/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.8571 - val_loss: 109.7831\n",
      "Epoch 597/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.7226 - val_loss: 109.5553\n",
      "Epoch 598/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.5893 - val_loss: 109.3283\n",
      "Epoch 599/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.4573 - val_loss: 109.1021\n",
      "Epoch 600/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.3265 - val_loss: 108.8768\n",
      "Epoch 601/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.1969 - val_loss: 108.6524\n",
      "Epoch 602/10000\n",
      "1126/1126 [==============================] - 0s - loss: 67.0686 - val_loss: 108.4291\n",
      "Epoch 603/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.9415 - val_loss: 108.2068\n",
      "Epoch 604/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.8156 - val_loss: 107.9855\n",
      "Epoch 605/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.6909 - val_loss: 107.7654\n",
      "Epoch 606/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.5674 - val_loss: 107.5464\n",
      "Epoch 607/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.4451 - val_loss: 107.3286\n",
      "Epoch 608/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.3240 - val_loss: 107.1120\n",
      "Epoch 609/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.2041 - val_loss: 106.8966\n",
      "Epoch 610/10000\n",
      "1126/1126 [==============================] - 0s - loss: 66.0853 - val_loss: 106.6825\n",
      "Epoch 611/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.9677 - val_loss: 106.4697\n",
      "Epoch 612/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.8513 - val_loss: 106.2582\n",
      "Epoch 613/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.7360 - val_loss: 106.0481\n",
      "Epoch 614/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.6219 - val_loss: 105.8393\n",
      "Epoch 615/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.5088 - val_loss: 105.6318\n",
      "Epoch 616/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.3969 - val_loss: 105.4258\n",
      "Epoch 617/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.2861 - val_loss: 105.2212\n",
      "Epoch 618/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.1763 - val_loss: 105.0180\n",
      "Epoch 619/10000\n",
      "1126/1126 [==============================] - 0s - loss: 65.0677 - val_loss: 104.8162\n",
      "Epoch 620/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.9601 - val_loss: 104.6158\n",
      "Epoch 621/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.8536 - val_loss: 104.4169\n",
      "Epoch 622/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.7481 - val_loss: 104.2194\n",
      "Epoch 623/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.6437 - val_loss: 104.0234\n",
      "Epoch 624/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.5403 - val_loss: 103.8289\n",
      "Epoch 625/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.4379 - val_loss: 103.6358\n",
      "Epoch 626/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.3365 - val_loss: 103.4441\n",
      "Epoch 627/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.2362 - val_loss: 103.2538\n",
      "Epoch 628/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.1368 - val_loss: 103.0651\n",
      "Epoch 629/10000\n",
      "1126/1126 [==============================] - 0s - loss: 64.0383 - val_loss: 102.8777\n",
      "Epoch 630/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.9409 - val_loss: 102.6918\n",
      "Epoch 631/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.8443 - val_loss: 102.5074\n",
      "Epoch 632/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.7488 - val_loss: 102.3244\n",
      "Epoch 633/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.6541 - val_loss: 102.1428\n",
      "Epoch 634/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.5604 - val_loss: 101.9626\n",
      "Epoch 635/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.4676 - val_loss: 101.7839\n",
      "Epoch 636/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.3756 - val_loss: 101.6065\n",
      "Epoch 637/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.2846 - val_loss: 101.4306\n",
      "Epoch 638/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.1945 - val_loss: 101.2561\n",
      "Epoch 639/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.1052 - val_loss: 101.0830\n",
      "Epoch 640/10000\n",
      "1126/1126 [==============================] - 0s - loss: 63.0168 - val_loss: 100.9112\n",
      "Epoch 641/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.9293 - val_loss: 100.7408\n",
      "Epoch 642/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 62.8425 - val_loss: 100.5718\n",
      "Epoch 643/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.7567 - val_loss: 100.4042\n",
      "Epoch 644/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.6716 - val_loss: 100.2379\n",
      "Epoch 645/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.5874 - val_loss: 100.0730\n",
      "Epoch 646/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.5040 - val_loss: 99.9093\n",
      "Epoch 647/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.4214 - val_loss: 99.7470\n",
      "Epoch 648/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.3396 - val_loss: 99.5861\n",
      "Epoch 649/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.2586 - val_loss: 99.4265\n",
      "Epoch 650/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.1784 - val_loss: 99.2682\n",
      "Epoch 651/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.0989 - val_loss: 99.1112\n",
      "Epoch 652/10000\n",
      "1126/1126 [==============================] - 0s - loss: 62.0202 - val_loss: 98.9555\n",
      "Epoch 653/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.9422 - val_loss: 98.8012\n",
      "Epoch 654/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.8650 - val_loss: 98.6480\n",
      "Epoch 655/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.7886 - val_loss: 98.4961\n",
      "Epoch 656/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.7129 - val_loss: 98.3455\n",
      "Epoch 657/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.6379 - val_loss: 98.1962\n",
      "Epoch 658/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.5637 - val_loss: 98.0481\n",
      "Epoch 659/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.4901 - val_loss: 97.9012\n",
      "Epoch 660/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.4173 - val_loss: 97.7556\n",
      "Epoch 661/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.3452 - val_loss: 97.6111\n",
      "Epoch 662/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.2738 - val_loss: 97.4679\n",
      "Epoch 663/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.2030 - val_loss: 97.3259\n",
      "Epoch 664/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.1329 - val_loss: 97.1850\n",
      "Epoch 665/10000\n",
      "1126/1126 [==============================] - 0s - loss: 61.0636 - val_loss: 97.0454\n",
      "Epoch 666/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.9948 - val_loss: 96.9069\n",
      "Epoch 667/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.9268 - val_loss: 96.7695\n",
      "Epoch 668/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.8594 - val_loss: 96.6334\n",
      "Epoch 669/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.7926 - val_loss: 96.4983\n",
      "Epoch 670/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.7265 - val_loss: 96.3644\n",
      "Epoch 671/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.6610 - val_loss: 96.2316\n",
      "Epoch 672/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.5961 - val_loss: 96.0999\n",
      "Epoch 673/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.5319 - val_loss: 95.9693\n",
      "Epoch 674/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.4682 - val_loss: 95.8397\n",
      "Epoch 675/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.4052 - val_loss: 95.7113\n",
      "Epoch 676/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.3427 - val_loss: 95.5838\n",
      "Epoch 677/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.2809 - val_loss: 95.4575\n",
      "Epoch 678/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.2196 - val_loss: 95.3321\n",
      "Epoch 679/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.1589 - val_loss: 95.2078\n",
      "Epoch 680/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.0988 - val_loss: 95.0845\n",
      "Epoch 681/10000\n",
      "1126/1126 [==============================] - 0s - loss: 60.0392 - val_loss: 94.9622\n",
      "Epoch 682/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.9801 - val_loss: 94.8408\n",
      "Epoch 683/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.9216 - val_loss: 94.7205\n",
      "Epoch 684/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.8637 - val_loss: 94.6011\n",
      "Epoch 685/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.8063 - val_loss: 94.4826\n",
      "Epoch 686/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.7494 - val_loss: 94.3652\n",
      "Epoch 687/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.6930 - val_loss: 94.2486\n",
      "Epoch 688/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.6371 - val_loss: 94.1329\n",
      "Epoch 689/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.5817 - val_loss: 94.0181\n",
      "Epoch 690/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.5268 - val_loss: 93.9043\n",
      "Epoch 691/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.4724 - val_loss: 93.7913\n",
      "Epoch 692/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.4184 - val_loss: 93.6792\n",
      "Epoch 693/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.3649 - val_loss: 93.5679\n",
      "Epoch 694/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.3119 - val_loss: 93.4574\n",
      "Epoch 695/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.2594 - val_loss: 93.3479\n",
      "Epoch 696/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.2073 - val_loss: 93.2391\n",
      "Epoch 697/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.1556 - val_loss: 93.1312\n",
      "Epoch 698/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.1044 - val_loss: 93.0241\n",
      "Epoch 699/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.0536 - val_loss: 92.9177\n",
      "Epoch 700/10000\n",
      "1126/1126 [==============================] - 0s - loss: 59.0032 - val_loss: 92.8122\n",
      "Epoch 701/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.9532 - val_loss: 92.7074\n",
      "Epoch 702/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.9036 - val_loss: 92.6034\n",
      "Epoch 703/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.8545 - val_loss: 92.5001\n",
      "Epoch 704/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.8057 - val_loss: 92.3977\n",
      "Epoch 705/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.7573 - val_loss: 92.2959\n",
      "Epoch 706/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.7093 - val_loss: 92.1949\n",
      "Epoch 707/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.6617 - val_loss: 92.0946\n",
      "Epoch 708/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.6145 - val_loss: 91.9950\n",
      "Epoch 709/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.5676 - val_loss: 91.8962\n",
      "Epoch 710/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.5210 - val_loss: 91.7980\n",
      "Epoch 711/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.4749 - val_loss: 91.7005\n",
      "Epoch 712/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.4290 - val_loss: 91.6037\n",
      "Epoch 713/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.3835 - val_loss: 91.5076\n",
      "Epoch 714/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.3384 - val_loss: 91.4121\n",
      "Epoch 715/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.2935 - val_loss: 91.3173\n",
      "Epoch 716/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.2490 - val_loss: 91.2232\n",
      "Epoch 717/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.2048 - val_loss: 91.1297\n",
      "Epoch 718/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.1609 - val_loss: 91.0369\n",
      "Epoch 719/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.1173 - val_loss: 90.9447\n",
      "Epoch 720/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.0741 - val_loss: 90.8531\n",
      "Epoch 721/10000\n",
      "1126/1126 [==============================] - 0s - loss: 58.0311 - val_loss: 90.7621\n",
      "Epoch 722/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.9884 - val_loss: 90.6718\n",
      "Epoch 723/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.9460 - val_loss: 90.5820\n",
      "Epoch 724/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 57.9038 - val_loss: 90.4929\n",
      "Epoch 725/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.8620 - val_loss: 90.4044\n",
      "Epoch 726/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.8204 - val_loss: 90.3164\n",
      "Epoch 727/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.7791 - val_loss: 90.2291\n",
      "Epoch 728/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.7380 - val_loss: 90.1423\n",
      "Epoch 729/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.6973 - val_loss: 90.0561\n",
      "Epoch 730/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.6567 - val_loss: 89.9705\n",
      "Epoch 731/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.6164 - val_loss: 89.8854\n",
      "Epoch 732/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.5764 - val_loss: 89.8009\n",
      "Epoch 733/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.5365 - val_loss: 89.7170\n",
      "Epoch 734/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.4970 - val_loss: 89.6335\n",
      "Epoch 735/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.4576 - val_loss: 89.5507\n",
      "Epoch 736/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.4185 - val_loss: 89.4683\n",
      "Epoch 737/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.3795 - val_loss: 89.3865\n",
      "Epoch 738/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.3409 - val_loss: 89.3052\n",
      "Epoch 739/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.3024 - val_loss: 89.2245\n",
      "Epoch 740/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.2641 - val_loss: 89.1443\n",
      "Epoch 741/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.2260 - val_loss: 89.0646\n",
      "Epoch 742/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.1882 - val_loss: 88.9854\n",
      "Epoch 743/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.1505 - val_loss: 88.9067\n",
      "Epoch 744/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.1130 - val_loss: 88.8286\n",
      "Epoch 745/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.0757 - val_loss: 88.7509\n",
      "Epoch 746/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.0386 - val_loss: 88.6737\n",
      "Epoch 747/10000\n",
      "1126/1126 [==============================] - 0s - loss: 57.0017 - val_loss: 88.5970\n",
      "Epoch 748/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.9650 - val_loss: 88.5208\n",
      "Epoch 749/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.9284 - val_loss: 88.4451\n",
      "Epoch 750/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.8920 - val_loss: 88.3698\n",
      "Epoch 751/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.8557 - val_loss: 88.2950\n",
      "Epoch 752/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.8197 - val_loss: 88.2207\n",
      "Epoch 753/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.7838 - val_loss: 88.1469\n",
      "Epoch 754/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.7480 - val_loss: 88.0735\n",
      "Epoch 755/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.7124 - val_loss: 88.0006\n",
      "Epoch 756/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.6769 - val_loss: 87.9281\n",
      "Epoch 757/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.6416 - val_loss: 87.8561\n",
      "Epoch 758/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.6065 - val_loss: 87.7846\n",
      "Epoch 759/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.5714 - val_loss: 87.7135\n",
      "Epoch 760/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.5365 - val_loss: 87.6428\n",
      "Epoch 761/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.5018 - val_loss: 87.5725\n",
      "Epoch 762/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.4671 - val_loss: 87.5028\n",
      "Epoch 763/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.4326 - val_loss: 87.4334\n",
      "Epoch 764/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.3982 - val_loss: 87.3645\n",
      "Epoch 765/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.3640 - val_loss: 87.2960\n",
      "Epoch 766/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.3298 - val_loss: 87.2279\n",
      "Epoch 767/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.2958 - val_loss: 87.1603\n",
      "Epoch 768/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.2618 - val_loss: 87.0931\n",
      "Epoch 769/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.2280 - val_loss: 87.0263\n",
      "Epoch 770/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.1943 - val_loss: 86.9599\n",
      "Epoch 771/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.1606 - val_loss: 86.8939\n",
      "Epoch 772/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.1271 - val_loss: 86.8283\n",
      "Epoch 773/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.0936 - val_loss: 86.7632\n",
      "Epoch 774/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.0603 - val_loss: 86.6984\n",
      "Epoch 775/10000\n",
      "1126/1126 [==============================] - 0s - loss: 56.0270 - val_loss: 86.6341\n",
      "Epoch 776/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.9938 - val_loss: 86.5702\n",
      "Epoch 777/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.9607 - val_loss: 86.5066\n",
      "Epoch 778/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.9277 - val_loss: 86.4435\n",
      "Epoch 779/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.8947 - val_loss: 86.3808\n",
      "Epoch 780/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.8618 - val_loss: 86.3185\n",
      "Epoch 781/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.8290 - val_loss: 86.2565\n",
      "Epoch 782/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.7962 - val_loss: 86.1950\n",
      "Epoch 783/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.7635 - val_loss: 86.1338\n",
      "Epoch 784/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.7309 - val_loss: 86.0730\n",
      "Epoch 785/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.6983 - val_loss: 86.0127\n",
      "Epoch 786/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.6658 - val_loss: 85.9527\n",
      "Epoch 787/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.6333 - val_loss: 85.8931\n",
      "Epoch 788/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.6009 - val_loss: 85.8340\n",
      "Epoch 789/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.5685 - val_loss: 85.7752\n",
      "Epoch 790/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.5361 - val_loss: 85.7168\n",
      "Epoch 791/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.5038 - val_loss: 85.6588\n",
      "Epoch 792/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.4716 - val_loss: 85.6012\n",
      "Epoch 793/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.4393 - val_loss: 85.5440\n",
      "Epoch 794/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.4071 - val_loss: 85.4872\n",
      "Epoch 795/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.3750 - val_loss: 85.4308\n",
      "Epoch 796/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.3429 - val_loss: 85.3748\n",
      "Epoch 797/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.3108 - val_loss: 85.3192\n",
      "Epoch 798/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.2787 - val_loss: 85.2640\n",
      "Epoch 799/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.2467 - val_loss: 85.2092\n",
      "Epoch 800/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.2147 - val_loss: 85.1548\n",
      "Epoch 801/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.1828 - val_loss: 85.1008\n",
      "Epoch 802/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.1508 - val_loss: 85.0473\n",
      "Epoch 803/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.1189 - val_loss: 84.9941\n",
      "Epoch 804/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.0871 - val_loss: 84.9414\n",
      "Epoch 805/10000\n",
      "1126/1126 [==============================] - 0s - loss: 55.0553 - val_loss: 84.8891\n",
      "Epoch 806/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 55.0235 - val_loss: 84.8372\n",
      "Epoch 807/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.9918 - val_loss: 84.7858\n",
      "Epoch 808/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.9601 - val_loss: 84.7348\n",
      "Epoch 809/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.9284 - val_loss: 84.6843\n",
      "Epoch 810/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.8968 - val_loss: 84.6341\n",
      "Epoch 811/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.8653 - val_loss: 84.5844\n",
      "Epoch 812/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.8338 - val_loss: 84.5352\n",
      "Epoch 813/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.8023 - val_loss: 84.4863\n",
      "Epoch 814/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.7710 - val_loss: 84.4380\n",
      "Epoch 815/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.7397 - val_loss: 84.3901\n",
      "Epoch 816/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.7084 - val_loss: 84.3426\n",
      "Epoch 817/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.6773 - val_loss: 84.2956\n",
      "Epoch 818/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.6462 - val_loss: 84.2490\n",
      "Epoch 819/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.6152 - val_loss: 84.2029\n",
      "Epoch 820/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.5843 - val_loss: 84.1573\n",
      "Epoch 821/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.5535 - val_loss: 84.1121\n",
      "Epoch 822/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.5228 - val_loss: 84.0673\n",
      "Epoch 823/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.4921 - val_loss: 84.0230\n",
      "Epoch 824/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.4616 - val_loss: 83.9792\n",
      "Epoch 825/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.4312 - val_loss: 83.9358\n",
      "Epoch 826/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.4009 - val_loss: 83.8929\n",
      "Epoch 827/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.3707 - val_loss: 83.8504\n",
      "Epoch 828/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.3406 - val_loss: 83.8084\n",
      "Epoch 829/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.3107 - val_loss: 83.7668\n",
      "Epoch 830/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.2808 - val_loss: 83.7257\n",
      "Epoch 831/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.2511 - val_loss: 83.6850\n",
      "Epoch 832/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.2215 - val_loss: 83.6447\n",
      "Epoch 833/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.1920 - val_loss: 83.6049\n",
      "Epoch 834/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.1626 - val_loss: 83.5655\n",
      "Epoch 835/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.1334 - val_loss: 83.5265\n",
      "Epoch 836/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.1043 - val_loss: 83.4879\n",
      "Epoch 837/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.0753 - val_loss: 83.4498\n",
      "Epoch 838/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.0464 - val_loss: 83.4120\n",
      "Epoch 839/10000\n",
      "1126/1126 [==============================] - 0s - loss: 54.0177 - val_loss: 83.3747\n",
      "Epoch 840/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.9890 - val_loss: 83.3377\n",
      "Epoch 841/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.9605 - val_loss: 83.3012\n",
      "Epoch 842/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.9320 - val_loss: 83.2651\n",
      "Epoch 843/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.9037 - val_loss: 83.2293\n",
      "Epoch 844/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.8755 - val_loss: 83.1939\n",
      "Epoch 845/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.8474 - val_loss: 83.1588\n",
      "Epoch 846/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.8194 - val_loss: 83.1242\n",
      "Epoch 847/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.7915 - val_loss: 83.0899\n",
      "Epoch 848/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.7638 - val_loss: 83.0559\n",
      "Epoch 849/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.7361 - val_loss: 83.0223\n",
      "Epoch 850/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.7084 - val_loss: 82.9890: 54.\n",
      "Epoch 851/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.6809 - val_loss: 82.9561\n",
      "Epoch 852/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.6535 - val_loss: 82.9235\n",
      "Epoch 853/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.6262 - val_loss: 82.8912\n",
      "Epoch 854/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.5989 - val_loss: 82.8593\n",
      "Epoch 855/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.5717 - val_loss: 82.8277\n",
      "Epoch 856/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.5446 - val_loss: 82.7963\n",
      "Epoch 857/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.5176 - val_loss: 82.7654\n",
      "Epoch 858/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.4906 - val_loss: 82.7347\n",
      "Epoch 859/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.4637 - val_loss: 82.7044\n",
      "Epoch 860/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.4369 - val_loss: 82.6743\n",
      "Epoch 861/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.4101 - val_loss: 82.6445\n",
      "Epoch 862/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.3834 - val_loss: 82.6150\n",
      "Epoch 863/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.3568 - val_loss: 82.5858\n",
      "Epoch 864/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.3302 - val_loss: 82.5569\n",
      "Epoch 865/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.3037 - val_loss: 82.5283\n",
      "Epoch 866/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.2772 - val_loss: 82.5000\n",
      "Epoch 867/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.2508 - val_loss: 82.4719\n",
      "Epoch 868/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.2245 - val_loss: 82.4441\n",
      "Epoch 869/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.1982 - val_loss: 82.4166\n",
      "Epoch 870/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.1719 - val_loss: 82.3894\n",
      "Epoch 871/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.1457 - val_loss: 82.3624\n",
      "Epoch 872/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.1196 - val_loss: 82.3357\n",
      "Epoch 873/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.0935 - val_loss: 82.3093\n",
      "Epoch 874/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.0675 - val_loss: 82.2831\n",
      "Epoch 875/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.0415 - val_loss: 82.2572\n",
      "Epoch 876/10000\n",
      "1126/1126 [==============================] - 0s - loss: 53.0155 - val_loss: 82.2315\n",
      "Epoch 877/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.9896 - val_loss: 82.2061: 54.13\n",
      "Epoch 878/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.9638 - val_loss: 82.1810\n",
      "Epoch 879/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.9380 - val_loss: 82.1561\n",
      "Epoch 880/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.9122 - val_loss: 82.1314\n",
      "Epoch 881/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.8865 - val_loss: 82.1070\n",
      "Epoch 882/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.8609 - val_loss: 82.0829\n",
      "Epoch 883/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.8353 - val_loss: 82.0590\n",
      "Epoch 884/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.8097 - val_loss: 82.0354\n",
      "Epoch 885/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.7841 - val_loss: 82.0120\n",
      "Epoch 886/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.7587 - val_loss: 81.9888\n",
      "Epoch 887/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 52.7332 - val_loss: 81.9660\n",
      "Epoch 888/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.7078 - val_loss: 81.9433\n",
      "Epoch 889/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.6825 - val_loss: 81.9208\n",
      "Epoch 890/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.6571 - val_loss: 81.8986\n",
      "Epoch 891/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.6319 - val_loss: 81.8766\n",
      "Epoch 892/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.6067 - val_loss: 81.8549\n",
      "Epoch 893/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.5815 - val_loss: 81.8334\n",
      "Epoch 894/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.5563 - val_loss: 81.8122\n",
      "Epoch 895/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.5312 - val_loss: 81.7911\n",
      "Epoch 896/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.5062 - val_loss: 81.7703\n",
      "Epoch 897/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.4812 - val_loss: 81.7497\n",
      "Epoch 898/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.4562 - val_loss: 81.7293\n",
      "Epoch 899/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.4313 - val_loss: 81.7092\n",
      "Epoch 900/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.4064 - val_loss: 81.6893\n",
      "Epoch 901/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.3815 - val_loss: 81.6696\n",
      "Epoch 902/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.3568 - val_loss: 81.6502\n",
      "Epoch 903/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.3320 - val_loss: 81.6309\n",
      "Epoch 904/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.3073 - val_loss: 81.6119: 53.41\n",
      "Epoch 905/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.2826 - val_loss: 81.5931\n",
      "Epoch 906/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.2580 - val_loss: 81.5745\n",
      "Epoch 907/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.2334 - val_loss: 81.5561\n",
      "Epoch 908/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.2089 - val_loss: 81.5379: 54.\n",
      "Epoch 909/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.1843 - val_loss: 81.5200\n",
      "Epoch 910/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.1599 - val_loss: 81.5022\n",
      "Epoch 911/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.1355 - val_loss: 81.4847\n",
      "Epoch 912/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.1111 - val_loss: 81.4674\n",
      "Epoch 913/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.0868 - val_loss: 81.4502\n",
      "Epoch 914/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.0625 - val_loss: 81.4333\n",
      "Epoch 915/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.0382 - val_loss: 81.4166\n",
      "Epoch 916/10000\n",
      "1126/1126 [==============================] - 0s - loss: 52.0140 - val_loss: 81.4001\n",
      "Epoch 917/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.9899 - val_loss: 81.3839\n",
      "Epoch 918/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.9657 - val_loss: 81.3677\n",
      "Epoch 919/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.9417 - val_loss: 81.3519\n",
      "Epoch 920/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.9176 - val_loss: 81.3362\n",
      "Epoch 921/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.8936 - val_loss: 81.3207\n",
      "Epoch 922/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.8697 - val_loss: 81.3054\n",
      "Epoch 923/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.8458 - val_loss: 81.2902\n",
      "Epoch 924/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.8219 - val_loss: 81.2753\n",
      "Epoch 925/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.7981 - val_loss: 81.2606\n",
      "Epoch 926/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.7743 - val_loss: 81.2461\n",
      "Epoch 927/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.7506 - val_loss: 81.2317\n",
      "Epoch 928/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.7269 - val_loss: 81.2176\n",
      "Epoch 929/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.7032 - val_loss: 81.2036\n",
      "Epoch 930/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.6796 - val_loss: 81.1899\n",
      "Epoch 931/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.6561 - val_loss: 81.1763\n",
      "Epoch 932/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.6326 - val_loss: 81.1628\n",
      "Epoch 933/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.6091 - val_loss: 81.1496\n",
      "Epoch 934/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.5857 - val_loss: 81.1366\n",
      "Epoch 935/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.5623 - val_loss: 81.1238\n",
      "Epoch 936/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.5389 - val_loss: 81.1112\n",
      "Epoch 937/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.5156 - val_loss: 81.0987\n",
      "Epoch 938/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.4923 - val_loss: 81.0864\n",
      "Epoch 939/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.4691 - val_loss: 81.0743\n",
      "Epoch 940/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.4459 - val_loss: 81.0623\n",
      "Epoch 941/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.4228 - val_loss: 81.0505\n",
      "Epoch 942/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.3997 - val_loss: 81.0389\n",
      "Epoch 943/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.3767 - val_loss: 81.0275\n",
      "Epoch 944/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.3537 - val_loss: 81.0163\n",
      "Epoch 945/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.3307 - val_loss: 81.0052\n",
      "Epoch 946/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.3078 - val_loss: 80.9943\n",
      "Epoch 947/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.2849 - val_loss: 80.9836\n",
      "Epoch 948/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.2621 - val_loss: 80.9730\n",
      "Epoch 949/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.2393 - val_loss: 80.9626\n",
      "Epoch 950/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.2165 - val_loss: 80.9524\n",
      "Epoch 951/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.1938 - val_loss: 80.9424\n",
      "Epoch 952/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.1711 - val_loss: 80.9325\n",
      "Epoch 953/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.1485 - val_loss: 80.9228\n",
      "Epoch 954/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.1259 - val_loss: 80.9132\n",
      "Epoch 955/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.1034 - val_loss: 80.9038\n",
      "Epoch 956/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.0808 - val_loss: 80.8946\n",
      "Epoch 957/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.0584 - val_loss: 80.8855\n",
      "Epoch 958/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.0360 - val_loss: 80.8766\n",
      "Epoch 959/10000\n",
      "1126/1126 [==============================] - 0s - loss: 51.0136 - val_loss: 80.8679\n",
      "Epoch 960/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.9912 - val_loss: 80.8593\n",
      "Epoch 961/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.9689 - val_loss: 80.8510\n",
      "Epoch 962/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.9466 - val_loss: 80.8427\n",
      "Epoch 963/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.9244 - val_loss: 80.8346\n",
      "Epoch 964/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.9022 - val_loss: 80.8267\n",
      "Epoch 965/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.8800 - val_loss: 80.8190\n",
      "Epoch 966/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.8579 - val_loss: 80.8114\n",
      "Epoch 967/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.8358 - val_loss: 80.8039\n",
      "Epoch 968/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s - loss: 50.8138 - val_loss: 80.7966\n",
      "Epoch 969/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.7917 - val_loss: 80.7895\n",
      "Epoch 970/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.7698 - val_loss: 80.7825\n",
      "Epoch 971/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.7478 - val_loss: 80.7757\n",
      "Epoch 972/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.7259 - val_loss: 80.7690\n",
      "Epoch 973/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.7040 - val_loss: 80.7625\n",
      "Epoch 974/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.6822 - val_loss: 80.7561\n",
      "Epoch 975/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.6604 - val_loss: 80.7499\n",
      "Epoch 976/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.6386 - val_loss: 80.7439\n",
      "Epoch 977/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.6168 - val_loss: 80.7380\n",
      "Epoch 978/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.5951 - val_loss: 80.7322\n",
      "Epoch 979/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.5734 - val_loss: 80.7267\n",
      "Epoch 980/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.5518 - val_loss: 80.7213\n",
      "Epoch 981/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.5301 - val_loss: 80.7160\n",
      "Epoch 982/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.5085 - val_loss: 80.7108\n",
      "Epoch 983/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.4869 - val_loss: 80.7058\n",
      "Epoch 984/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.4654 - val_loss: 80.7010\n",
      "Epoch 985/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.4439 - val_loss: 80.6963\n",
      "Epoch 986/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.4224 - val_loss: 80.6918\n",
      "Epoch 987/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.4009 - val_loss: 80.6874\n",
      "Epoch 988/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.3794 - val_loss: 80.6832\n",
      "Epoch 989/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.3580 - val_loss: 80.6791\n",
      "Epoch 990/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.3366 - val_loss: 80.6752\n",
      "Epoch 991/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.3152 - val_loss: 80.6715\n",
      "Epoch 992/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.2938 - val_loss: 80.6678\n",
      "Epoch 993/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.2724 - val_loss: 80.6643\n",
      "Epoch 994/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.2511 - val_loss: 80.6610\n",
      "Epoch 995/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.2298 - val_loss: 80.6578\n",
      "Epoch 996/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.2084 - val_loss: 80.6548\n",
      "Epoch 997/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.1871 - val_loss: 80.6519\n",
      "Epoch 998/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.1658 - val_loss: 80.6492\n",
      "Epoch 999/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.1445 - val_loss: 80.6465\n",
      "Epoch 1000/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.1233 - val_loss: 80.6441\n",
      "Epoch 1001/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.1020 - val_loss: 80.6418\n",
      "Epoch 1002/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.0807 - val_loss: 80.6396\n",
      "Epoch 1003/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.0595 - val_loss: 80.6376\n",
      "Epoch 1004/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.0382 - val_loss: 80.6357\n",
      "Epoch 1005/10000\n",
      "1126/1126 [==============================] - 0s - loss: 50.0170 - val_loss: 80.6339\n",
      "Epoch 1006/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.9958 - val_loss: 80.6323\n",
      "Epoch 1007/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.9745 - val_loss: 80.6309\n",
      "Epoch 1008/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.9533 - val_loss: 80.6295\n",
      "Epoch 1009/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.9320 - val_loss: 80.6283\n",
      "Epoch 1010/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.9108 - val_loss: 80.6272\n",
      "Epoch 1011/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.8895 - val_loss: 80.6263\n",
      "Epoch 1012/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.8683 - val_loss: 80.6255\n",
      "Epoch 1013/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.8470 - val_loss: 80.6248\n",
      "Epoch 1014/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.8258 - val_loss: 80.6242\n",
      "Epoch 1015/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.8045 - val_loss: 80.6238\n",
      "Epoch 1016/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.7832 - val_loss: 80.6236\n",
      "Epoch 1017/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.7619 - val_loss: 80.6234\n",
      "Epoch 1018/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.7406 - val_loss: 80.6233\n",
      "Epoch 1019/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.7193 - val_loss: 80.6233\n",
      "Epoch 1020/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.6980 - val_loss: 80.6235\n",
      "Epoch 1021/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.6767 - val_loss: 80.6238\n",
      "Epoch 1022/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.6553 - val_loss: 80.6242\n",
      "Epoch 1023/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.6340 - val_loss: 80.6247\n",
      "Epoch 1024/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.6126 - val_loss: 80.6253\n",
      "Epoch 1025/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.5912 - val_loss: 80.6260\n",
      "Epoch 1026/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.5699 - val_loss: 80.6268\n",
      "Epoch 1027/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.5485 - val_loss: 80.6277\n",
      "Epoch 1028/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.5271 - val_loss: 80.6287\n",
      "Epoch 1029/10000\n",
      "1126/1126 [==============================] - 0s - loss: 49.5057 - val_loss: 80.6298\n",
      "Epoch 01028: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nASAikqEUACIiGUoBICKSoRQAIiIZ6v8DxF1XNBoHCPQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a83a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j8BucLAiOeL4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a8791d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.plot(history.history['loss'], 'r-')\n",
    "plt.plot(history.history['val_loss'], 'b-')\n",
    "plt.show()\n",
    "\n",
    "plt.title('y_pred, y_test')\n",
    "\n",
    "plt.plot(y_pred[:], y_test[:], '+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataframe_to_xy_sequences(df_train, 4)\n",
    "X_valid, y_valid = dataframe_to_xy_sequences(df_valid, 4)\n",
    "X_test, y_test = dataframe_to_xy_sequences(df_test, 4)\n",
    "\n",
    "def simple_rnn_model(nb_units, input_dim, loss='mean_squared_error', optimizer='adagrad'):\n",
    "    print(input_dim)\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(nb_units, batch_input_shape=(96, 4, 8)))#input_dim=input_dim[1], input_length=input_dim[0], return_sequences=True))\n",
    "    #model.add(Dense(nb_units))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (96, 16)                  400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (96, 1)                   17        \n",
      "=================================================================\n",
      "Total params: 417\n",
      "Trainable params: 417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = simple_rnn_model(16, X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected simple_rnn_1_input to have shape (None, 1, 8) but got array with shape (281, 4, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-33b02a6df4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected simple_rnn_1_input to have shape (None, 1, 8) but got array with shape (281, 4, 8)"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto', patience=10)\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5000, validation_data=(X_valid, y_valid), callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
